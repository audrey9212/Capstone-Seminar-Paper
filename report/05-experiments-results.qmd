<!--
Chapter 5: Experiments and Results (sections 5.1–5.7)
This chapter presents benchmark results across all model families, from baseline logistic regression
through tree/GBM models, deep learning, unsupervised extensions, and ensemble methods.
Figures are linked to precomputed artifacts under artifacts/figures/ (generated by scripts 03–08).
-->

# Experiments and Results {#sec-experiments-results}

## Overall Benchmark Summary {#sec-benchmark-summary}

All models were evaluated on the held-out test set using threshold-independent ranking metrics (ROC-AUC and PR-AUC) alongside threshold-dependent decision metrics (precision, recall, and F1) at a fixed operating point. @tbl-model-leaderboard reports the complete benchmark results, while @fig-model-comparison summarizes the corresponding ROC and precision–recall (PR) curves. Because the churn rate in the test set is 28.8%, a non-informative baseline achieves a PR-AUC of approximately 0.2880; all trained models exceeded this baseline by a substantial margin.

Across the supervised model families, gradient-boosted decision trees delivered the strongest overall performance. Using the validation-set ROC-AUC criterion defined in @sec-eval-protocol, XGBoost was selected as the champion model (validation ROC-AUC = 0.6583, validation PR-AUC = 0.4344). On the test set, the champion XGBoost model achieved ROC-AUC = 0.6637 and PR-AUC = 0.4454. At the fixed operating threshold of 0.4400, it reached precision = 0.3670, recall = 0.7390, and F1 = 0.4904 (the corresponding confusion matrix is reported in @sec-operating-point). Notably, LightGBM achieved the highest test ROC-AUC among the single supervised models (test ROC-AUC = 0.6700, test PR-AUC = 0.4476), indicating that the two leading GBM approaches were closely matched on this dataset.

The remaining baselines formed clear lower tiers. Random Forest achieved ROC-AUC = 0.6534 and PR-AUC = 0.4271, while the logistic regression baseline yielded ROC-AUC = 0.5940 and PR-AUC = 0.3560. The deep learning models clustered in a narrow band: their test ROC-AUC values ranged from approximately 0.6440 to 0.6620, and test PR-AUC ranged from approximately 0.4080 to 0.4360. The best neural model by ranking metrics was a Wide & Deep variant with focal loss (test ROC-AUC = 0.6615, test PR-AUC = 0.4356). Under the fixed threshold of 0.4400, several focal-loss neural variants produced very few positive predictions, resulting in near-zero recall and F1 despite competitive AUC/PR-AUC; therefore, ranking metrics provide the fairest comparison at this aggregate level, while threshold-based outcomes are interpreted separately in @sec-operating-point.

```{python}
#| label: tbl-model-leaderboard
#| tbl-cap: "Model leaderboard showing validation and test performance across all model families."
#| echo: false
#| warning: false
#| message: false

from pathlib import Path
import pandas as pd

def find_project_root(start: Path) -> Path:
    p = start.resolve()
    for _ in range(12):
        if (p / "_quarto.yml").exists() or (p / ".git").exists():
            return p
        if p.parent == p:
            break
        p = p.parent
    return start.resolve()

root = find_project_root(Path.cwd())
table_dir = root / "artifacts" / "tables"

# Look for model leaderboard
candidates = list(table_dir.glob("08_model_leaderboard*.csv")) if table_dir.exists() else []
if candidates:
    df = pd.read_csv(candidates[0])
    # Display formatted table
    from IPython.display import Markdown
    display(Markdown(df.to_markdown(index=False)))
else:
    print("Model leaderboard table not found. Expected: artifacts/tables/08_model_leaderboard.csv")
```

@fig-model-comparison visually corroborates these findings. The ROC curves for the leading models (LightGBM and XGBoost) lie consistently above the other model families across most false-positive rates, and the PR curves show a clear separation from the prevalence baseline (approximately 0.2880), with the top models maintaining higher precision at comparable recall. In addition to single-model benchmarks, heterogeneous ensemble experiments (@sec-ensemble-results) produced performance comparable to the best GBMs, with the best ensembles reaching approximately 0.6690 test ROC-AUC and 0.4490 test PR-AUC, representing a modest lift relative to the champion XGBoost model.

```{python}
#| label: fig-model-comparison
#| fig-cap: "ROC and Precision–Recall curves comparing all model families on the test set."
#| fig-alt: "Side-by-side ROC and PR curves showing model comparison."
#| echo: false
#| warning: false
#| message: false

from pathlib import Path
import matplotlib.pyplot as plt
import matplotlib.image as mpimg

def find_project_root(start: Path) -> Path:
    p = start.resolve()
    for _ in range(12):
        if (p / "_quarto.yml").exists() or (p / ".git").exists():
            return p
        if p.parent == p:
            break
        p = p.parent
    return start.resolve()

root = find_project_root(Path.cwd())
fig_dir = root / "artifacts" / "figures"

candidates = []
if fig_dir.exists():
    for pat in ["08_model_comparison*.png", "*model_comparison*.png"]:
        candidates.extend(sorted(fig_dir.glob(pat)))

img_path = candidates[0] if candidates else None

plt.figure(figsize=(5.2, 3.4))
plt.axis("off")
if img_path:
    img = mpimg.imread(img_path)
    plt.imshow(img)
else:
    plt.text(0.5000, 0.5000, "Model comparison figure not found.\nExpected: artifacts/figures/08_model_comparison.png",
             ha="center", va="center", fontsize=10)
plt.tight_layout()
```

Note: Models were selected based on Validation ROC-AUC (as shown in the leaderboard in appendix). The plots below display performance on the held-out Test Set to provide an unbiased evaluation of generalization. Small discrepancies between ranking and plotting order are expected.

## Baseline Performance (Logistic Regression) {#sec-logit-baseline}

To provide a simple and interpretable reference point, a regularized logistic regression model was first evaluated as the supervised baseline. On the held-out test set, the baseline achieved a ROC-AUC of 0.5940 and a PR-AUC (average precision) of 0.3560, indicating modest but meaningful discrimination despite the class imbalance (@tbl-logit-metrics). For context, the test-set churn prevalence is 0.2880, so the baseline PR-AUC is clearly above the prevalence level expected from random ranking; this relationship is also visible in the precision–recall curve where the model stays above the horizontal prevalence line across a wide recall range (@sec-appendix-b3).

```{python}
#| label: tbl-logit-metrics
#| tbl-cap: "Logistic regression baseline performance metrics on validation and test sets."
#| echo: false
#| warning: false
#| message: false

from pathlib import Path
import pandas as pd

def find_project_root(start: Path) -> Path:
    p = start.resolve()
    for _ in range(12):
        if (p / "_quarto.yml").exists() or (p / ".git").exists():
            return p
        if p.parent == p:
            break
        p = p.parent
    return start.resolve()

root = find_project_root(Path.cwd())
table_dir = root / "artifacts" / "tables"

candidates = list(table_dir.glob("03_logit_metrics*.csv")) if table_dir.exists() else []
if not candidates:
    candidates = list(table_dir.glob("*logit*metrics*.csv")) if table_dir.exists() else []

if candidates:
    df = pd.read_csv(candidates[0])
    from IPython.display import Markdown
    display(Markdown(df.to_markdown(index=False)))
else:
    print("Logistic regression metrics table not found.")
```

The baseline exhibited stable performance across data splits. On the validation set, the model reached ROC-AUC = 0.5880 and PR-AUC = 0.3480, closely matching test-set results and providing a consistent lower-bound benchmark for subsequent model families. In addition to ranking metrics, decision-level performance is reported at the operating threshold used for downstream comparisons (threshold = 0.4400). At this threshold, the test-set confusion matrix contained 917 true negatives, 2,717 false positives, 234 false negatives, and 1,237 true positives, corresponding to recall = 0.8410, precision = 0.3130, and F1 = 0.4560. Detailed ROC curves, PR curves, and confusion matrices for the logistic regression baseline are provided in @sec-appendix-b3.

Finally, probability accuracy was summarized via the Brier score, which was 0.2440 on the test set; the corresponding reliability diagram provides a visual check of calibration quality (@fig-logit-calibration). Overall, this baseline establishes a conservative performance floor against which the gains from nonlinear models (tree/GBM, neural networks) and ensembling can be quantified.

```{python}
#| label: fig-logit-calibration
#| fig-cap: "Calibration (reliability) diagram for logistic regression with Brier score annotation."
#| fig-alt: "Reliability diagram showing probability calibration quality."
#| echo: false
#| warning: false
#| message: false

from pathlib import Path
import matplotlib.pyplot as plt
import matplotlib.image as mpimg

def find_project_root(start: Path) -> Path:
    p = start.resolve()
    for _ in range(12):
        if (p / "_quarto.yml").exists() or (p / ".git").exists():
            return p
        if p.parent == p:
            break
        p = p.parent
    return start.resolve()

root = find_project_root(Path.cwd())
fig_dir = root / "artifacts" / "figures"

candidates = []
if fig_dir.exists():
    for pat in ["03_logit_calibration*.png", "*logit*calibration*.png", "*calibration*.png"]:
        candidates.extend(sorted(fig_dir.glob(pat)))

img_path = candidates[0] if candidates else None

plt.figure(figsize=(5.2, 3.4))
plt.axis("off")
if img_path:
    img = mpimg.imread(img_path)
    plt.imshow(img)
else:
    plt.text(0.5000, 0.5000, "Calibration diagram not found.\nExpected: artifacts/figures/03_logit_calibration.png",
             ha="center", va="center", fontsize=10)
plt.tight_layout()
```


## Tree and GBM Results {#sec-tree-gbm-results}

As a first step beyond the linear baseline, two tree-based benchmarks were evaluated: a single decision tree and a random forest. @tbl-tree-leaderboard reports ROC-AUC and PR-AUC on both validation and test sets, and @fig-tree-rocpr-test visualizes the corresponding ROC and PR curves on the test set.

```{python}
#| label: tbl-tree-leaderboard
#| tbl-cap: "Performance summary of tree-based models on validation and test sets."
#| echo: false
#| warning: false
#| message: false

from pathlib import Path
import pandas as pd

def find_project_root(start: Path) -> Path:
    p = start.resolve()
    for _ in range(12):
        if (p / "_quarto.yml").exists() or (p / ".git").exists():
            return p
        if p.parent == p:
            break
        p = p.parent
    return start.resolve()

root = find_project_root(Path.cwd())
table_dir = root / "artifacts" / "tables"

candidates = list(table_dir.glob("04_tree_leaderboard*.csv")) if table_dir.exists() else []
if not candidates:
    candidates = list(table_dir.glob("*tree*leaderboard*.csv")) if table_dir.exists() else []

if candidates:
    df = pd.read_csv(candidates[0])
    from IPython.display import Markdown
    display(Markdown(df.to_markdown(index=False)))
else:
    print("Tree leaderboard table not found. Expected: artifacts/tables/04_tree_leaderboard.csv")
```

On the held-out test set, the decision tree achieved ROC-AUC = 0.6250 and PR-AUC = 0.3930 (validation: ROC-AUC = 0.6320, PR-AUC = 0.4040), showing that even shallow, rule-like splits can capture churn-related interactions absent in a linear boundary. However, performance improved noticeably when variance was reduced through bagging. The random forest increased test-set discrimination to ROC-AUC = 0.6530 and PR-AUC = 0.4290 (validation: ROC-AUC = 0.6360, PR-AUC = 0.4120), and its PR curve dominates the single-tree baseline across most recall levels in @fig-tree-rocpr-test. These gains indicate that churn signals are distributed across multiple weak patterns, and an ensemble of diverse trees aggregates them more reliably than a single fitted tree. Importantly, the validation-to-test gaps for both DT and RF were small in magnitude for ROC-AUC and PR-AUC, suggesting that the improvements are not driven by a split-specific artifact. At the default decision threshold of 0.5000, both DT and RF yielded test-set F1 scores in the mid-0.4700 range (DT F1 = 0.4770, RF F1 = 0.4700), reinforcing that these baselines improve ranking but still leave substantial room for improvement in the precision–recall trade-off.

```{python}
#| label: fig-tree-rocpr-test
#| fig-cap: "ROC and Precision–Recall curves for tree/GBM models on the test set."
#| fig-alt: "ROC and PR curves comparing Decision Tree, Random Forest, XGBoost, and LightGBM."
#| echo: false
#| warning: false
#| message: false

from pathlib import Path
import matplotlib.pyplot as plt
import matplotlib.image as mpimg

def find_project_root(start: Path) -> Path:
    p = start.resolve()
    for _ in range(12):
        if (p / "_quarto.yml").exists() or (p / ".git").exists():
            return p
        if p.parent == p:
            break
        p = p.parent
    return start.resolve()

root = find_project_root(Path.cwd())
fig_dir = root / "artifacts" / "figures"

candidates = []
if fig_dir.exists():
    for pat in ["04_all_tree_roc_pr*.png", "*tree*roc*pr*.png", "04_tree*.png"]:
        candidates.extend(sorted(fig_dir.glob(pat)))

img_path = candidates[0] if candidates else None

plt.figure(figsize=(5.2, 3.4))
plt.axis("off")
if img_path:
    img = mpimg.imread(img_path)
    plt.imshow(img)
else:
    plt.text(0.5000, 0.5000, "Tree ROC/PR comparison not found.\nExpected: artifacts/figures/04_all_tree_roc_pr_test.png",
             ha="center", va="center", fontsize=10)
plt.tight_layout()
```

### Gradient-Boosted Decision Trees

Gradient-boosted decision trees delivered the strongest ranking performance among the classical ML families. As summarized in @tbl-tree-leaderboard and visible in @fig-tree-rocpr-test, the XGBoost baseline improved test discrimination to ROC-AUC = 0.6640 and PR-AUC = 0.4450 (validation: ROC-AUC = 0.6510, PR-AUC = 0.4350). Applying Optuna hyperparameter search to XGBoost produced a modest additional lift in ROC-AUC on the test set (0.6690 versus 0.6640) while maintaining a very similar PR-AUC (0.4440 versus 0.4450). LightGBM performed comparably and achieved the best overall ranking on the test set in this experiment (ROC-AUC = 0.6700, PR-AUC = 0.4480; validation PR-AUC = 0.4330). Relative to the logistic regression baseline (test PR-AUC = 0.3560), the best GBM models provide an absolute PR-AUC gain of roughly 0.0900, which is material given the 0.2880 churn prevalence.

When evaluated at a common default operating point (threshold = 0.5000), the Optuna-tuned XGBoost achieved the highest F1 among the tree family (F1 = 0.4900), reflecting a better precision–recall trade-off than the DT/RF baselines under the same decision rule. Because deployment typically requires an explicit operating point, a recall-constrained threshold sweep was performed on the validation set for the tuned XGBoost model, which selected threshold = 0.4400 as the best F1-achieving rule under the recall constraint. @fig-xgb-operating-point highlights this operating point directly on the ROC and PR curves for the tuned model, making the ranking–decision connection explicit. In addition to higher AUC metrics, the boosted models also achieved slightly lower Brier scores (approximately 0.2200 on the test set), suggesting better probability calibration than the single-tree baselines even before explicit calibration methods are applied.

```{python}
#| label: fig-xgb-operating-point
#| fig-cap: "Tuned XGBoost ROC/PR curves with the selected operating threshold (0.4400) highlighted on the test set."
#| fig-alt: "ROC and PR curves with operating point marker."
#| echo: false
#| warning: false
#| message: false

from pathlib import Path
import matplotlib.pyplot as plt
import matplotlib.image as mpimg

def find_project_root(start: Path) -> Path:
    p = start.resolve()
    for _ in range(12):
        if (p / "_quarto.yml").exists() or (p / ".git").exists():
            return p
        if p.parent == p:
            break
        p = p.parent
    return start.resolve()

root = find_project_root(Path.cwd())
fig_dir = root / "artifacts" / "figures"

candidates = []
if fig_dir.exists():
    for pat in ["04_best_roc_pr_point*.png", "*xgb*operating*.png", "*best*roc*pr*.png"]:
        candidates.extend(sorted(fig_dir.glob(pat)))

img_path = candidates[0] if candidates else None

plt.figure(figsize=(5.2, 3.4))
plt.axis("off")
if img_path:
    img = mpimg.imread(img_path)
    plt.imshow(img)
else:
    plt.text(0.5000, 0.5000, "XGBoost operating point figure not found.\nExpected: artifacts/figures/04_best_roc_pr_point_test.png",
             ha="center", va="center", fontsize=10)
plt.tight_layout()
```

### Hyperparameter Optimization Diagnostics

The Optuna diagnostics were inspected to verify that the hyperparameter search behaved sensibly and to summarize which settings drove performance. The optimization history shows rapid improvement in the early trials followed by a clear plateau, consistent with quickly locating a strong region of the hyperparameter space and then encountering diminishing returns. The parallel-coordinate visualization indicates that the better trials concentrate within a relatively narrow band of configurations, rather than appearing as isolated outliers, which reduces the likelihood that the selected model is the result of chance. The hyperparameter-importance summary further suggests that performance is most sensitive to the learning rate, while tree depth and regularization terms play a secondary role. This pattern aligns with the small empirical gap between the untuned and tuned XGBoost models: tuning improves ROC-AUC slightly, but the overall precision–recall ranking remains of similar magnitude. Full Optuna diagnostic plots, including the optimization history, parallel-coordinate visualization, and hyperparameter importance rankings, are provided in @sec-appendix-c3.


## Deep Learning Results (PyTorch) {#sec-nn-results}

This section reports neural network experiments implemented in PyTorch, focusing on (i) architecture comparisons and (ii) loss-function/imbalance handling ablations. Results are summarized in @tbl-nn-leaderboard, with the best model's test curves shown in @fig-nn-best-rocpr and its training dynamics summarized in @fig-nn-curves-best. Additional training curves for alternative architectures (MLP baseline, embedding MLP, and deeper Wide & Deep variants) are provided in @sec-appendix-d2 and @sec-appendix-d3.

```{python}
#| label: tbl-nn-leaderboard
#| tbl-cap: "PyTorch neural network results (test metrics at validation-selected thresholds)."
#| echo: false
#| warning: false
#| message: false

from pathlib import Path
import pandas as pd

def find_project_root(start: Path) -> Path:
    p = start.resolve()
    for _ in range(12):
        if (p / "_quarto.yml").exists() or (p / ".git").exists():
            return p
        if p.parent == p:
            break
        p = p.parent
    return start.resolve()

root = find_project_root(Path.cwd())
table_dir = root / "artifacts" / "tables"

candidates = list(table_dir.glob("05_nn_leaderboard*.csv")) if table_dir.exists() else []
if not candidates:
    candidates = list(table_dir.glob("*nn*leaderboard*.csv")) if table_dir.exists() else []

if candidates:
    df = pd.read_csv(candidates[0])
    from IPython.display import Markdown
    display(Markdown(df.to_markdown(index=False)))
else:
    print("Neural network leaderboard not found. Expected: artifacts/tables/05_nn_leaderboard.csv")
```

### Architecture Comparison

Across the neural network candidates, the top-ranked models in terms of discrimination are the focal-loss variants: wide_deep_focal and embedding_focal_loss. On the test set, wide_deep_focal achieves ROC-AUC = 0.6610 and PR-AUC = 0.4360, narrowly outperforming the other NN configurations on ranking metrics. The embedding-only focal model is extremely close (ROC-AUC = 0.6610, PR-AUC = 0.4350), suggesting that learned categorical representations provide most of the gains, while the additional "wide" pathway mainly stabilizes decision performance rather than dramatically shifting AUC.

In contrast, the purely dense baseline (dense_baseline, without embedding-driven representation learning) trails in ranking quality (ROC-AUC = 0.6550, PR-AUC = 0.4220), and the weighted-BCE Wide & Deep (wide_and_deep) is lower still (ROC-AUC = 0.6500, PR-AUC = 0.4210). Increasing depth (wide_and_deep_deeper) does not help and slightly reduces test ranking metrics (ROC-AUC = 0.6480, PR-AUC = 0.4110), consistent with diminishing returns under the available signal and regularization constraints.

```{python}
#| label: fig-nn-best-rocpr
#| fig-cap: "Best neural model (Wide & Deep with Focal Loss) ROC/PR curves with selected operating threshold on the test set."
#| fig-alt: "ROC and PR curves for the best neural network model."
#| echo: false
#| warning: false
#| message: false

from pathlib import Path
import matplotlib.pyplot as plt
import matplotlib.image as mpimg

def find_project_root(start: Path) -> Path:
    p = start.resolve()
    for _ in range(12):
        if (p / "_quarto.yml").exists() or (p / ".git").exists():
            return p
        if p.parent == p:
            break
        p = p.parent
    return start.resolve()

root = find_project_root(Path.cwd())
fig_dir = root / "artifacts" / "figures"

candidates = []
if fig_dir.exists():
    for pat in ["05_nn_wide_deep_focal_roc_pr*.png", "*nn*wide*deep*focal*roc*.png", "05_nn*best*.png"]:
        candidates.extend(sorted(fig_dir.glob(pat)))

img_path = candidates[0] if candidates else None

plt.figure(figsize=(5.2, 3.4))
plt.axis("off")
if img_path:
    img = mpimg.imread(img_path)
    plt.imshow(img)
else:
    plt.text(0.5000, 0.5000, "Best NN ROC/PR figure not found.\nExpected: artifacts/figures/05_nn_wide_deep_focal_roc_pr_test.png",
             ha="center", va="center", fontsize=10)
plt.tight_layout()
```

Decision-level metrics (computed at the best validation-selected threshold per model) are broadly similar across the stronger NN candidates, with F1 concentrated around 0.4800–0.4900. For the best-ranking model wide_deep_focal, the chosen threshold is 0.3100, producing precision = 0.3640, recall = 0.7180, and F1 = 0.4830 on test. The embedding-only focal model selects a similar threshold (0.3000) but shifts toward higher sensitivity (recall = 0.8120) at lower precision (0.3430), yielding F1 = 0.4820. Interestingly, the dense baseline attains the highest F1 among NN variants (F1 = 0.4890) but with weaker AUC/PR-AUC, illustrating that "best F1" can be achieved via a favorable threshold even when the underlying ranking quality is lower. Probability accuracy further differentiates the models: focal-loss networks show a markedly improved Brier score of approximately 0.1970, compared with 0.2330–0.2360 for weighted-BCE variants, indicating better-calibrated probabilities in addition to stronger discrimination.

### Loss/Imbalance Ablation

Focal Loss provides the clearest performance lift in this NN setup. Holding the general architecture fixed, switching Wide & Deep from weighted BCE (wide_and_deep) to focal (wide_deep_focal) improves ROC-AUC from 0.6500 to 0.6610 and PR-AUC from 0.4210 to 0.4360, while substantially lowering the Brier score (0.2360 to 0.1970). A similar pattern holds for embedding-only models: ROC-AUC improves from 0.6490 to 0.6610 and PR-AUC from 0.4190 to 0.4350 when moving from embedding_baseline to embedding_focal_loss. In contrast, aggressively increasing the positive-class weight (embedding_strong_weight) under weighted BCE reduces ranking quality (ROC-AUC = 0.6440, PR-AUC = 0.4080), suggesting that extreme reweighting can over-correct and degrade generalization. Overall, focal loss appears to provide a more effective imbalance strategy by focusing learning on harder examples rather than uniformly amplifying the minority class.

The focal loss formulation down-weights easy examples and focuses training on hard-to-classify instances:

$$
\mathcal{L}_{\text{focal}} = -\alpha_t (1 - p_t)^\gamma \log(p_t)
$$

where $p_t$ is the model's estimated probability for the true class, $\alpha_t$ is a class-balancing weight, and $\gamma \geq 0$ is the focusing parameter. When $\gamma = 0$, focal loss reduces to standard cross-entropy. Higher values of $\gamma$ increase the relative loss for hard examples (where $p_t$ is small), encouraging the model to focus on difficult cases.

### Training Dynamics

```{python}
#| label: fig-nn-curves-best
#| fig-cap: "Training versus validation loss and AUC curves for the best neural network model (Wide & Deep with Focal Loss)."
#| fig-alt: "Learning curves showing training dynamics."
#| echo: false
#| warning: false
#| message: false

from pathlib import Path
import matplotlib.pyplot as plt
import matplotlib.image as mpimg

def find_project_root(start: Path) -> Path:
    p = start.resolve()
    for _ in range(12):
        if (p / "_quarto.yml").exists() or (p / ".git").exists():
            return p
        if p.parent == p:
            break
        p = p.parent
    return start.resolve()

root = find_project_root(Path.cwd())
fig_dir = root / "artifacts" / "figures"

candidates = []
if fig_dir.exists():
    for pat in ["05_nn_wide_deep_focal_curves*.png", "*nn*wide*deep*focal*curves*.png"]:
        candidates.extend(sorted(fig_dir.glob(pat)))

img_path = candidates[0] if candidates else None

plt.figure(figsize=(5.2, 3.4))
plt.axis("off")
if img_path:
    img = mpimg.imread(img_path)
    plt.imshow(img)
else:
    plt.text(0.5000, 0.5000, "Best NN training curves not found.\nExpected: artifacts/figures/05_nn_wide_deep_focal_curves.png",
             ha="center", va="center", fontsize=10)
plt.tight_layout()
```

Training curves (@fig-nn-curves-best) show that the best focal model continues to reduce training loss while validation loss plateaus, and validation AUC stabilizes in the mid-0.6400 range after roughly the mid-training period; early stopping selects epoch 62 for wide_deep_focal. Across variants, deeper/wider configurations tend to widen the train–validation gap without improving validation AUC, consistent with the small benefit observed from the deeper Wide & Deep model. Additional learning curves for alternative architectures and imbalance settings are provided in @sec-appendix-d2 (best model training dynamics) and @sec-appendix-d3 (alternative architecture comparisons).


## Unsupervised/Semi-Supervised Extension (Autoencoder + Pseudo-Label) {#sec-autoencoder-results}

To explore whether unlabeled data can improve churn prediction, a denoising autoencoder (DAE) was trained on the feature space and evaluated for two downstream uses: (i) using the learned latent representation as features, and (ii) using an Optuna-tuned XGBoost teacher to generate pseudo-labels on a real unlabeled holdout set.

### DAE Convergence and Reconstruction

The DAE showed stable optimization behavior with early stopping at epoch 16 (@fig-dae-loss). Training reconstruction loss dropped quickly in the first few epochs and then plateaued, while the validation curve remained flat after the best epoch, indicating the encoder learned a consistent low-dimensional structure without late-epoch divergence. Note that validation loss is lower than training loss, which is expected here because noise is injected during training (denoising objective), making the training reconstruction task harder than the clean validation reconstruction.

```{python}
#| label: fig-dae-loss
#| fig-cap: "DAE reconstruction loss curves with early-stopping epoch marked."
#| fig-alt: "Training and validation reconstruction loss over epochs."
#| echo: false
#| warning: false
#| message: false

from pathlib import Path
import matplotlib.pyplot as plt
import matplotlib.image as mpimg

def find_project_root(start: Path) -> Path:
    p = start.resolve()
    for _ in range(12):
        if (p / "_quarto.yml").exists() or (p / ".git").exists():
            return p
        if p.parent == p:
            break
        p = p.parent
    return start.resolve()

root = find_project_root(Path.cwd())
fig_dir = root / "artifacts" / "figures"

candidates = []
if fig_dir.exists():
    for pat in ["06_dae_loss*.png", "*dae*loss*.png", "06_autoencoder*loss*.png"]:
        candidates.extend(sorted(fig_dir.glob(pat)))

img_path = candidates[0] if candidates else None

plt.figure(figsize=(5.2, 3.4))
plt.axis("off")
if img_path:
    img = mpimg.imread(img_path)
    plt.imshow(img)
else:
    plt.text(0.5000, 0.5000, "DAE loss curve not found.\nExpected: artifacts/figures/06_dae_loss_curve.png",
             ha="center", va="center", fontsize=10)
plt.tight_layout()
```

### Pseudo-Label Confidence and Coverage

Using the XGBoost teacher trained on labeled data, the real unlabeled holdout was scored and a conservative high-confidence rule was applied (assign churn if $p \geq 0.9500$, non-churn if $p \leq 0.0500$). @fig-pseudolabel-confidence shows the teacher's probability distribution concentrates in the mid-range (roughly $p \approx 0.1500$–$0.4500$), producing almost no extreme probabilities. As a result, pseudo-label coverage was effectively zero: only 2 samples met the low-confidence (non-churn) criterion and 0 samples met the high-confidence (churn) criterion, with 19,998/20,000 (approximately 100%) remaining "uncertain." This is a key practical finding: with strict thresholds designed to minimize label noise, the teacher did not provide enough pseudo-labeled examples to form a meaningful semi-supervised training signal on this dataset split.

```{python}
#| label: fig-pseudolabel-confidence
#| fig-cap: "Teacher probability distribution on the unlabeled holdout and pseudo-label coverage at thresholds 0.0500/0.9500."
#| fig-alt: "Histogram of predicted probabilities showing pseudo-label coverage."
#| echo: false
#| warning: false
#| message: false

from pathlib import Path
import matplotlib.pyplot as plt
import matplotlib.image as mpimg

def find_project_root(start: Path) -> Path:
    p = start.resolve()
    for _ in range(12):
        if (p / "_quarto.yml").exists() or (p / ".git").exists():
            return p
        if p.parent == p:
            break
        p = p.parent
    return start.resolve()

root = find_project_root(Path.cwd())
fig_dir = root / "artifacts" / "figures"

candidates = []
if fig_dir.exists():
    for pat in ["06_pseudo_confidence*.png", "*pseudo*confidence*.png", "*pseudolabel*.png"]:
        candidates.extend(sorted(fig_dir.glob(pat)))

img_path = candidates[0] if candidates else None

plt.figure(figsize=(5.2, 3.4))
plt.axis("off")
if img_path:
    img = mpimg.imread(img_path)
    plt.imshow(img)
else:
    plt.text(0.5000, 0.5000, "Pseudo-label confidence figure not found.\nExpected: artifacts/figures/06_pseudo_confidence_dist.png",
             ha="center", va="center", fontsize=10)
plt.tight_layout()
```

### Downstream Impact (Ablation Summary)

@tbl-ae-ablation reports the predictive utility of the learned latent features via a controlled ablation. Using the same evaluation protocol, the baseline (X only) achieved ROC-AUC = 0.6465 and PR-AUC = 0.4254. Using latent-only (Z only) reduced performance to ROC-AUC = 0.6252 and PR-AUC = 0.3872, implying the compressed representation retains substantial but not complete churn-relevant information. Finally, augmenting features (X+Z) essentially matched the baseline ranking metrics (ROC-AUC = 0.6451, PR-AUC = 0.4271), yielding only a marginal PR-AUC increase. Under tuned thresholds (selected on validation), baseline and augmented variants also delivered very similar operating-point metrics (F1 around 0.4800 with recall around 0.7300), consistent with the conclusion that latent features provide, at best, a small incremental signal in this configuration.

```{python}
#| label: tbl-ae-ablation
#| tbl-cap: "Downstream impact of DAE latent features (baseline vs. latent-only vs. augmented)."
#| echo: false
#| warning: false
#| message: false

from pathlib import Path
import pandas as pd

def find_project_root(start: Path) -> Path:
    p = start.resolve()
    for _ in range(12):
        if (p / "_quarto.yml").exists() or (p / ".git").exists():
            return p
        if p.parent == p:
            break
        p = p.parent
    return start.resolve()

root = find_project_root(Path.cwd())
table_dir = root / "artifacts" / "tables"

candidates = list(table_dir.glob("06_autoencoder_ablation*.csv")) if table_dir.exists() else []
if not candidates:
    candidates = list(table_dir.glob("*ae*ablation*.csv")) if table_dir.exists() else []

if candidates:
    df = pd.read_csv(candidates[0])
    from IPython.display import Markdown
    display(Markdown(df.to_markdown(index=False)))
else:
    print("Autoencoder ablation table not found. Expected: artifacts/tables/06_autoencoder_ablation.csv")
```

Additional diagnostics, including latent t-SNE visualization (@fig-latent-tsne in @sec-appendix-e2) and reconstruction-error distribution plots (@fig-recon-error in @sec-appendix-e1), are provided to support the qualitative assessment of representation structure and error overlap between churn/non-churn groups.


## Ensemble Results (Blending vs. OOF Stacking) {#sec-ensemble-results}

To test whether combining heterogeneous learners could yield incremental gains beyond the best single model, several ensemble strategies were evaluated built on the same base predictors (Logistic Regression, XGBoost, LightGBM, Random Forest). @tbl-ensemble-leaderboard summarizes validation and test ranking performance.

```{python}
#| label: tbl-ensemble-leaderboard
#| tbl-cap: "Ensemble performance comparison on validation and test sets (ROC-AUC and PR-AUC)."
#| echo: false
#| warning: false
#| message: false

from pathlib import Path
import pandas as pd

def find_project_root(start: Path) -> Path:
    p = start.resolve()
    for _ in range(12):
        if (p / "_quarto.yml").exists() or (p / ".git").exists():
            return p
        if p.parent == p:
            break
        p = p.parent
    return start.resolve()

root = find_project_root(Path.cwd())
table_dir = root / "artifacts" / "tables"

candidates = list(table_dir.glob("07_ensemble_leaderboard*.csv")) if table_dir.exists() else []
if not candidates:
    candidates = list(table_dir.glob("*ensemble*leaderboard*.csv")) if table_dir.exists() else []

if candidates:
    df = pd.read_csv(candidates[0])
    from IPython.display import Markdown
    display(Markdown(df.to_markdown(index=False)))
else:
    print("Ensemble leaderboard not found. Expected: artifacts/tables/07_ensemble_leaderboard.csv")
```

### Blending Results

Overall, blending delivered the strongest and most stable results, with Blend_WeightedAUC achieving the highest test ROC-AUC (0.6688) and a test PR-AUC of 0.4488. A simple unweighted average (Blend_SimpleAvg) performed nearly identically (test ROC-AUC 0.6683; PR-AUC 0.4472), suggesting that most of the ensemble benefit comes from variance reduction rather than finely tuned weights. The NNLS-constrained blend (Blend_NNLS) also matched this band (test ROC-AUC 0.6672; PR-AUC 0.4488).

Compared with the best single tree/GBM model from @sec-tree-gbm-results, these ensembles largely preserve discrimination while providing a small but consistent PR-AUC lift, which is most relevant under class imbalance. For operating-point comparability across all model families, the fixed threshold = 0.4400 is maintained for the main cross-section comparisons; tuned thresholds and blending weights for each ensemble are detailed in @sec-appendix-f4 (@tbl-ensemble-comparison).

### OOF Stacking Results

OOF stacking (Stacking_OOF) performed competitively but not decisively better than blending: it reached test ROC-AUC 0.6679 and the highest test PR-AUC 0.4490 among the listed ensembles. Notably, its optimal threshold is much lower (approximately 0.2260) than the blending variants (approximately 0.4200), which is consistent with the meta-learner producing probabilities on a different scale (or less calibrated) even when ranking quality is similar. This makes stacking attractive for ranking, but it requires explicit threshold re-selection or calibration if deployed as a decision rule (see @sec-appendix-h4 and @tbl-optimal-thresholds for threshold sensitivity analysis). In this dataset, the near-tie between stacking and blending suggests that the relationship between base model scores is close to linear, so a learned meta-combination provides limited additional upside over robust averaging.

The stacking meta-learner combines base model predictions $z(x)$ using logistic regression:

$$
z(x) = \big[\hat{p}_{1}(x), \hat{p}_{2}(x), \ldots, \hat{p}_{M}(x)\big]
$$

$$
\hat{p}_{\text{stack}}(x) = \sigma(\beta^\top z(x) + b)
$$

where $\sigma$ is the sigmoid function and the coefficients $\beta$ are learned from out-of-fold predictions to prevent leakage between base and meta levels.

### Ensemble Diversity Analysis

The effectiveness of ensembling is supported by diversity evidence from the validation-set prediction correlations (@fig-ensemble-corr). Tree-based learners are highly correlated with each other (e.g., XGBoost–LightGBM = 0.9310, XGBoost–RF = 0.8790, LightGBM–RF = 0.8690), indicating they capture similar nonlinear structure. In contrast, the Logistic Regression predictions correlate much less with the tree models (approximately 0.4700 with each), implying complementary error patterns. This explains why including a weaker linear baseline can still improve an ensemble: it contributes orthogonal signal that slightly improves precision–recall ranking. Finally, learned blend weights show XGBoost dominates the NNLS solution, while LightGBM and Random Forest contribute meaningful secondary weight and Logistic Regression remains small but non-zero, consistent with the correlation-based diversity story.

```{python}
#| label: fig-ensemble-corr
#| fig-cap: "Correlation matrix of base model predictions on the validation set (lower correlation indicates greater diversity)."
#| fig-alt: "Heatmap showing prediction correlations between base models."
#| out-width: "60%"
#| fig-width: 3.8
#| fig-height: 2.6
#| echo: false
#| warning: false
#| message: false

from pathlib import Path
import matplotlib.pyplot as plt
import matplotlib.image as mpimg
import pandas as pd
import numpy as np

def find_project_root(start: Path) -> Path:
    p = start.resolve()
    for _ in range(12):
        if (p / "_quarto.yml").exists() or (p / ".git").exists():
            return p
        if p.parent == p:
            break
        p = p.parent
    return start.resolve()

root = find_project_root(Path.cwd())
fig_dir = root / "artifacts" / "figures"
table_dir = root / "artifacts" / "tables"

# Try to find pre-generated heatmap first
candidates = []
if fig_dir.exists():
    for pat in ["07_diversity_corr*.png", "*diversity*corr*.png", "*ensemble*corr*.png"]:
        candidates.extend(sorted(fig_dir.glob(pat)))

if candidates:
    plt.figure(figsize=(3.8, 2.6))
    plt.axis("off")
    img = mpimg.imread(candidates[0])
    plt.imshow(img)
    plt.tight_layout()
else:
    # Try to generate from CSV
    csv_candidates = list(table_dir.glob("07_diversity_corr*.csv")) if table_dir.exists() else []
    if csv_candidates:
        df = pd.read_csv(csv_candidates[0], index_col=0)
        fig, ax = plt.subplots(figsize=(4, 3))
        im = ax.imshow(df.values, cmap='RdYlBu_r', vmin=0.4000, vmax=1.0)
        ax.set_xticks(range(len(df.columns)))
        ax.set_yticks(range(len(df.index)))
        ax.set_xticklabels(df.columns, rotation=45, ha='right')
        ax.set_yticklabels(df.index)
        plt.colorbar(im, ax=ax, label='Correlation')
        ax.set_title('Base Model Prediction Correlations')
        for i in range(len(df.index)):
            for j in range(len(df.columns)):
                ax.text(j, i, f'{df.iloc[i, j]:.3f}', ha='center', va='center', fontsize=8)
        plt.tight_layout()
    else:
        plt.figure(figsize=(3.8, 2.6))
        plt.axis("off")
        plt.text(0.5000, 0.5000, "Correlation matrix not found.\nExpected: artifacts/tables/07_diversity_corr_val.csv",
                 ha="center", va="center", fontsize=10)
```


## Operating Point Analysis (Threshold Policy in Practice) {#sec-operating-point}

To turn probabilistic churn scores into an actionable retention list, this study adopts a single, fixed operating threshold across all candidate models. Concretely, $\tau = 0.4400$ (chosen on the validation set during the main tree-based model selection) serves as the production policy: any customer with predicted churn probability $\hat{p}(\text{churn}) \geq \tau$ is flagged for intervention. This fixed-threshold policy is intentionally conservative from a reporting standpoint: it avoids per-model "best threshold" cherry-picking and reflects how real teams often deploy one stable rule tied to capacity and budget. Full threshold sweeps showing precision–recall trade-offs across $\tau$ are provided in @sec-appendix-h3 (@tbl-xgb-threshold-full).

Under $\tau = 0.4400$, the best balance of precision and recall is achieved by LightGBM with Precision = 0.3760, Recall = 0.7310, and F1 = 0.4970 (@tbl-operating-point). XGBoost is extremely close (0.3670/0.7390/0.4900), indicating that the two GBMs behave similarly under the same decision rule. The dense NN baseline is more aggressive at this threshold, reaching higher recall (0.8060) but lower precision (0.3520), while Logistic Regression pushes recall even higher (0.8410) at the cost of precision (0.3130). These patterns match the expected operational trade-off: higher recall reduces missed churners but increases the number of false positives competing for retention resources.

```{python}
#| label: tbl-operating-point
#| tbl-cap: "Operating point comparison across models at fixed threshold τ = 0.4400."
#| echo: false
#| warning: false
#| message: false

from pathlib import Path
import pandas as pd

def find_project_root(start: Path) -> Path:
    p = start.resolve()
    for _ in range(12):
        if (p / "_quarto.yml").exists() or (p / ".git").exists():
            return p
        if p.parent == p:
            break
        p = p.parent
    return start.resolve()

root = find_project_root(Path.cwd())
table_dir = root / "artifacts" / "tables"

candidates = list(table_dir.glob("08_model_leaderboard*.csv")) if table_dir.exists() else []
if not candidates:
    candidates = list(table_dir.glob("*operating*point*.csv")) if table_dir.exists() else []

if candidates:
    df = pd.read_csv(candidates[0])
    from IPython.display import Markdown
    display(Markdown(df.to_markdown(index=False)))
else:
    print("Operating point table not found. Expected: artifacts/tables/08_model_leaderboard.csv")
```

### Confusion Matrix Analysis

The confusion matrix for XGBoost at $\tau = 0.4400$ illustrates the scale of this trade-off on the test set (n = 5,105): TP = 1,087, FP = 1,875, FN = 384, TN = 1,759 (@fig-confmat-xgb). This means the model correctly identifies 1,087 churners while flagging 1,875 non-churners as potential churners. In a retention campaign context, this translates to contacting approximately 2,962 customers (TP + FP) to capture 73.9% of actual churners, with a precision of 36.7%.

```{python}
#| label: fig-confmat-xgb
#| fig-cap: "Confusion matrix for XGBoost at the operating threshold τ = 0.4400 on the test set."
#| fig-alt: "Confusion matrix showing TP, FP, TN, FN counts."
#| out-width: "70%"
#| fig-width: 4.5
#| fig-height: 3.5
#| echo: false
#| warning: false
#| message: false

from pathlib import Path
import matplotlib.pyplot as plt
import matplotlib.image as mpimg

def find_project_root(start: Path) -> Path:
    p = start.resolve()
    for _ in range(12):
        if (p / "_quarto.yml").exists() or (p / ".git").exists():
            return p
        if p.parent == p:
            break
        p = p.parent
    return start.resolve()

root = find_project_root(Path.cwd())
fig_dir = root / "artifacts" / "figures"

candidates = []
if fig_dir.exists():
    for pat in ["08_confusion_matrix*.png", "*confusion*matrix*.png", "*confmat*.png"]:
        candidates.extend(sorted(fig_dir.glob(pat)))

img_path = candidates[0] if candidates else None

plt.figure(figsize=(5.2, 3.4))
plt.axis("off")
if img_path:
    img = mpimg.imread(img_path)
    plt.imshow(img)
else:
    plt.text(0.5000, 0.5000, "Confusion matrix figure not found.\nExpected: artifacts/figures/08_confusion_matrix.png",
             ha="center", va="center", fontsize=10)
plt.tight_layout()
```

Finally, models whose probability scales differ substantially (e.g., the DAE teacher and some focal-loss networks) can become overly conservative under a shared $\tau$ (high precision but very low recall), suggesting that calibration or model-specific thresholds are important alternatives. Detailed calibration metrics and model-specific optimal thresholds are provided in @sec-appendix-h4 (@tbl-optimal-thresholds) and @sec-appendix-h4 for calibration details.


## Summary of Key Findings {#sec-results-summary}

The experimental results across all model families can be summarized as follows:

1. **Gradient-boosted trees outperform other single-model families.** XGBoost and LightGBM achieved the highest test ROC-AUC (0.6640–0.6700) and PR-AUC (0.4440–0.4480), providing meaningful improvements over the logistic regression baseline (ROC-AUC = 0.5940, PR-AUC = 0.3560).

2. **Neural networks with focal loss are competitive but do not surpass GBMs.** The best Wide & Deep model with focal loss achieved test ROC-AUC = 0.6610 and PR-AUC = 0.4360, demonstrating that deep learning approaches can capture churn patterns effectively on tabular data. Focal loss consistently outperformed weighted BCE for handling class imbalance.

3. **Autoencoder-based representation learning provides limited incremental value.** Latent features from a denoising autoencoder did not meaningfully improve downstream classification beyond the original feature set. Pseudo-labeling with conservative confidence thresholds yielded insufficient coverage for semi-supervised learning.

4. **Ensemble methods provide modest but consistent gains.** Blending and OOF stacking reached approximately 0.6690 test ROC-AUC and 0.4490 test PR-AUC, representing a small lift over the best single model. The benefits derive primarily from diversity between linear (logistic regression) and nonlinear (tree-based) base learners.

5. **Operating threshold selection is critical for deployment.** At the fixed threshold $\tau = 0.4400$, the champion models achieve recall around 73–74% with precision around 37%, suitable for targeted retention campaigns. Different models require different optimal thresholds, highlighting the importance of calibration in production.

These findings inform the discussion of model interpretation, business implications, and deployment considerations in the following chapter.
