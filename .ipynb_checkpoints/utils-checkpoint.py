# ---
# jupyter:
#   jupytext:
#     text_representation:
#       extension: .py
#       format_name: percent
#       format_version: '1.3'
#       jupytext_version: 1.17.1
#   kernelspec:
#     display_name: Python 3 (ACTL3143)
#     language: python
#     name: actl3143
# ---

# %%
"""
Minimal project utilities for Capstone-Seminar-Paper.

Scope (intentionally minimal):
- Project root detection (env override supported)
- Directory helpers:
  data/raw, data/processed, artifacts/{figures,tables,meta}, models, report, notebooks, tuner_dir
- Save helpers: CSV (DataFrame), JSON, Pickle, text, bytes, matplotlib figures

No logging, no timestamps, no data cleaning or grouping tools.
"""

from __future__ import annotations
import os
import json
import pickle
from dataclasses import dataclass
from pathlib import Path
from typing import Optional
from sklearn.base import BaseEstimator, TransformerMixin
import numpy as np
import pandas as pd



# =========================
# Root detection
# =========================

def _detect_project_root() -> Path:
    """
    Detect project root. Priority:
    1) env CAPSTONE_ROOT
    2) /Users/audreychang/Projects/Capstone-Seminar-Paper (Audrey's default)
    3) search upwards from cwd for a folder containing 'data' and 'notebooks'
    4) fallback to cwd
    """
    env = os.environ.get("CAPSTONE_ROOT")
    if env:
        p = Path(env).expanduser().resolve()
        if p.exists():
            return p

    default = Path("/Users/audreychang/Projects/Capstone-Seminar-Paper")
    if default.exists():
        return default.resolve()

    cur = Path.cwd().resolve()
    for parent in [cur, *cur.parents]:
        if (parent / "data").exists() and (parent / "notebooks").exists():
            return parent

    return cur

ROOT: Path = _detect_project_root()

# =========================
# Directory schema
# =========================

@dataclass(frozen=True)
class Dirs:
    root: Path = ROOT
    data: Path = ROOT / "data"
    raw: Path = ROOT / "data" / "raw"
    processed: Path = ROOT / "data" / "processed"
    artifacts: Path = ROOT / "artifacts"
    figures: Path = ROOT / "artifacts" / "figures"
    tables: Path = ROOT / "artifacts" / "tables"
    meta: Path = ROOT / "artifacts" / "meta"
    models: Path = ROOT / "models"
    notebooks: Path = ROOT / "notebooks"
    report: Path = ROOT / "report"
    tuner: Path = ROOT / "tuner_dir"

DIRS = Dirs()

def ensure_dirs() -> None:
    """Create all common project folders if missing."""
    for p in [
        DIRS.data, DIRS.raw, DIRS.processed,
        DIRS.artifacts, DIRS.figures, DIRS.tables, DIRS.meta,
        DIRS.models, DIRS.notebooks, DIRS.report, DIRS.tuner,
    ]:
        p.mkdir(parents=True, exist_ok=True)

ensure_dirs()

# =========================
# Path helpers (return Path)
# =========================

def path_in(*parts: str | Path) -> Path:
    return DIRS.root.joinpath(*parts)

def raw_path(name: str) -> Path:
    return DIRS.raw / name

def processed_path(name: str) -> Path:
    return DIRS.processed / name

def figure_path(name: str) -> Path:
    """Return artifacts/figures/<name>. If no extension, default to .png."""
    if "." not in name:
        name = f"{name}.png"
    return DIRS.figures / name

def table_path(name: str) -> Path:
    """Return artifacts/tables/<name>.csv if no extension given."""
    if not name.lower().endswith(".csv"):
        name = f"{name}.csv"
    return DIRS.tables / name

def meta_path(name: str) -> Path:
    return DIRS.meta / name

def model_path(name: str) -> Path:
    return DIRS.models / name

def report_path(name: str) -> Path:
    return DIRS.report / name

def notebook_path(name: str) -> Path:
    return DIRS.notebooks / name

def tuner_path(name: str) -> Path:
    return DIRS.tuner / name

# =========================
# Save helpers
# =========================

def _ensure_parent(p: Path) -> None:
    p.parent.mkdir(parents=True, exist_ok=True)

def save_df(df: pd.DataFrame, name: str, folder: Optional[Path] = None, index: bool = False) -> Path:
    """
    å„²å­˜ DataFrameï¼š
    - è‹¥å‰¯æª”åç‚º .parquet â†’ ä»¥ parquet æ ¼å¼å„²å­˜ï¼ˆä¿ç•™å‹åˆ¥è³‡è¨Šï¼‰
    - å¦å‰‡ä»¥ CSV æ ¼å¼å„²å­˜ã€‚
    é è¨­è³‡æ–™å¤¾ï¼šartifacts/tablesã€‚
    
    âš ï¸ é‡è¦ï¼šç¢ºä¿è³‡æ–™å¤¾å­˜åœ¨
    """
    folder = folder or DIRS.tables
    folder.mkdir(parents=True, exist_ok=True)  # âœ… é€™è¡Œæœ€é‡è¦ï¼

    # çµ„å®Œæ•´è·¯å¾‘
    p = folder / name

    # è‹¥æ²’æŒ‡å®šå‰¯æª”åï¼Œé è¨­åŠ ä¸Š .csv
    if p.suffix == "":
        p = p.with_suffix(".csv")

    # æ ¹æ“šå‰¯æª”åæ±ºå®šæ ¼å¼
    if p.suffix.lower() == ".parquet":
        df.to_parquet(p, index=index)
    else:
        df.to_csv(p, index=index)

    print(f"[save_df] å·²å„²å­˜ï¼š{p}")
    return p


def save_json(obj, name: str, folder: Optional[Path] = None, ensure_ascii: bool = False, indent: int = 2) -> Path:
    """
    Save JSON with UTF-8 encoding.
    Default folder: artifacts/meta.
    """
    folder = folder or DIRS.meta
    p = folder / (name if name.lower().endswith(".json") else f"{name}.json")
    _ensure_parent(p)
    with open(p, "w", encoding="utf-8") as f:
        json.dump(obj, f, indent=indent, ensure_ascii=ensure_ascii)
    return p

def save_pickle(obj, name: str, folder: Optional[Path] = None) -> Path:
    """
    Save Pickle binary.
    Default folder: models (ä¿®æ­£ï¼šåŸæœ¬æ˜¯ meta)
    
    âš ï¸ é‡è¦ä¿®æ­£ï¼šæ¨¡å‹æ‡‰å­˜æ”¾åœ¨ models/ è€Œé meta/
    """
    # ğŸ”§ ä¿®æ­£ï¼šé è¨­è³‡æ–™å¤¾å¾ DIRS.meta æ”¹ç‚º DIRS.models
    folder = folder or DIRS.models  # âœ… ä¿®æ­£å¾Œ
    
    p = folder / (name if name.lower().endswith(".pkl") else f"{name}.pkl")
    _ensure_parent(p)
    with open(p, "wb") as f:
        pickle.dump(obj, f)
    print(f"[save_pickle] å·²å„²å­˜ï¼š{p}")  # ğŸ”§ åŠ å…¥æ—¥èªŒ
    return p

def save_text(text: str, name: str, folder: Optional[Path] = None, encoding: str = "utf-8") -> Path:
    """
    Save plain text file.
    Default folder: artifacts/meta.
    """
    folder = folder or DIRS.meta
    p = folder / (name if name.lower().endswith(".txt") else f"{name}.txt")
    _ensure_parent(p)
    with open(p, "w", encoding=encoding) as f:
        f.write(text)
    return p

def save_bytes(data: bytes, name: str, folder: Optional[Path] = None) -> Path:
    """
    Save raw bytes (e.g., images already in bytes).
    Default folder: artifacts/meta.
    """
    folder = folder or DIRS.meta
    p = folder / name
    _ensure_parent(p)
    with open(p, "wb") as f:
        f.write(data)
    return p

def save_fig(fig, name: str, dpi: int = 200, tight: bool = True) -> Path:
    """
    Save a matplotlib figure under artifacts/figures.
    If no extension in `name`, PNG is used.
    """
    p = figure_path(name)
    _ensure_parent(p)
    if tight:
        fig.savefig(p, dpi=dpi, bbox_inches="tight")
    else:
        fig.savefig(p, dpi=dpi)
    return p

from typing import Optional, Union
from pathlib import Path
import json

# =========================
# Load helpers
# =========================

def load_json(name: Union[str, Path], folder: Optional[Path] = None):
    """
    è®€å– JSON æª”ã€‚

    ç”¨æ³• 1ï¼šçµ¦æª”åï¼ˆå»ºè­°ï¼‰
        load_json("preprocess_params")
        load_json("preprocess_params.json")

    ç”¨æ³• 2ï¼šçµ¦å®Œæ•´ Pathï¼ˆé€²éšï¼‰
        load_json(meta_path("preprocess_params.json"))

    folder:
        - è‹¥ name æ˜¯å­—ä¸²ï¼šç”¨ folder æˆ– DIRS.meta ç•¶è³‡æ–™å¤¾
        - è‹¥ name æ˜¯ Pathï¼šæœƒå¿½ç•¥ folderï¼Œç›´æ¥ç”¨é€™å€‹ Path
    """
    # å¦‚æœç›´æ¥çµ¦ Pathï¼Œå°±ç›´æ¥ç”¨å®ƒ
    if isinstance(name, Path):
        path = name
    else:
        folder = folder or DIRS.meta
        filename = name if name.lower().endswith(".json") else f"{name}.json"
        path = folder / filename

    if not path.exists():
        raise FileNotFoundError(f"æ‰¾ä¸åˆ° JSON æª”ï¼š{path}")

    with open(path, "r", encoding="utf-8") as f:
        return json.load(f)


# =========================
# Model result logging helpers
# =========================
def log_model_result(
    model_name: str,
    params: Optional[dict] = None,
    metrics: Optional[dict] = None,
    notes: Optional[str] = None,
    filename: str = "model_results.csv",
):
    """
    æŠŠæ¯æ¬¡å¯¦é©—çµæœ append åˆ° artifacts/meta/model_results.csvã€‚

    Parameters
    ----------
    model_name : str
        æ¨¡å‹æˆ–å¯¦é©—çš„åç¨±ï¼Œä¾‹å¦‚ "logit_baseline_v1"ã€‚
    params : dict, optional
        è¶…åƒæ•¸ï¼Œä¾‹å¦‚ GridSearchCV çš„ best_params_ã€‚
    metrics : dict, optional
        å·²ç¶“æ‰å¹³åŒ–å¥½çš„æŒ‡æ¨™ï¼Œä¾‹å¦‚
        {"auc_train": 0.61, "auc_val": 0.60, "brier_val": 0.24}ã€‚
    notes : str, optional
        å‚™è¨»æ–‡å­—ï¼Œä¾‹å¦‚ "with class_weight=balanced"ã€‚
    filename : str, optional
        å­˜æˆçš„æª”æ¡ˆåç¨±ï¼Œé è¨­ "model_results.csv"ã€‚
    """
    import datetime
    import pandas as pd

    params = params or {}
    metrics = metrics or {}

    # 1. çµ„ä¸€åˆ— row è³‡æ–™
    row = {
        "timestamp": datetime.datetime.now().isoformat(timespec="seconds"),
        "model_name": model_name,
    }

    # è¶…åƒæ•¸ä¸€å¾‹åŠ  param__ å‰ç¶´
    for k, v in params.items():
        row[f"param__{k}"] = v

    # æŒ‡æ¨™ä¸€å¾‹åŠ  metric__ å‰ç¶´
    for k, v in metrics.items():
        row[f"metric__{k}"] = v

    if notes is not None:
        row["notes"] = notes

    df = pd.DataFrame([row])

    # 2. ç›®æ¨™æª”æ¡ˆè·¯å¾‘ï¼šartifacts/meta/model_results.csv
    p = meta_path(filename)
    _ensure_parent(p)  # âœ… ç¢ºä¿è³‡æ–™å¤¾å­˜åœ¨

    # 3. è‹¥æª”æ¡ˆå·²å­˜åœ¨ â†’ append ä¸å¯« headerï¼›å¦å‰‡å»ºç«‹æ–°æª”
    if p.exists():
        df.to_csv(p, mode="a", header=False, index=False)
    else:
        df.to_csv(p, mode="w", header=True, index=False)

    print(f"[log_model_result] å·²è¿½åŠ  1 ç­†çµæœåˆ° {p}")
    return p




# =========================
# sklearn custom transformers 
# =========================

class MedianFromConfigImputer(BaseEstimator, TransformerMixin):
    """
    ä½¿ç”¨ preprocess_params.json ä¸­çš„ numeric_impute_values ä¾†åšå¡«è£œï¼Œ
    ä¸å†å¾è³‡æ–™æœ¬èº«é‡æ–°ä¼°è¨ˆ medianã€‚

    impute_values: dictï¼Œkey æ˜¯æ¬„ä½åï¼Œvalue æ˜¯å°æ‡‰çš„ median
    feature_names: listï¼Œå¯é¸ï¼ŒæŒ‡å®šé€™å€‹ transformer é æœŸè™•ç†çš„æ¬„ä½é †åº
    """
    def __init__(self, impute_values: dict, feature_names=None):
        self.impute_values = impute_values
        self.feature_names = feature_names

    def fit(self, X, y=None):
        # è¨˜ä½æ¬„ä½é †åºï¼Œè®“ transform / get_feature_names_out ç”¨
        if hasattr(X, "columns"):
            self.feature_names_in_ = list(X.columns)
        elif self.feature_names is not None:
            self.feature_names_in_ = list(self.feature_names)
        else:
            self.feature_names_in_ = [f"col_{i}" for i in range(X.shape[1])]
        return self

    def transform(self, X):
        # çµ±ä¸€è½‰æˆ DataFrame è™•ç†
        if hasattr(X, "columns"):
            X_df = X.copy()
        else:
            X_df = pd.DataFrame(X, columns=self.feature_names_in_)

        for col in self.feature_names_in_:
            if col in self.impute_values:
                X_df[col] = X_df[col].fillna(self.impute_values[col])

        # å†è½‰å›åŸä¾†å‹æ…‹
        if hasattr(X, "columns"):
            return X_df
        else:
            return X_df.values

    def get_feature_names_out(self, input_features=None):
        if input_features is None:
            input_features = getattr(self, "feature_names_in_", None)
        return np.asarray(input_features, dtype=object)


class ToFloatTransformer(BaseEstimator, TransformerMixin):
    """
    æŠŠè¼¸å…¥çµ±ä¸€è½‰æˆ floatï¼Œç”¨åœ¨ 0/1 flag é¡æ¬„ä½ï¼Œ
    ä¸¦ä¿æŒ feature_names_out ä¸è®Šã€‚
    """
    def fit(self, X, y=None):
        self.feature_names_in_ = getattr(X, "columns", None)
        return self

    def transform(self, X):
        return X.astype(float)

    def get_feature_names_out(self, input_features=None):
        if input_features is None:
            input_features = getattr(self, "feature_names_in_", None)
        return np.asarray(input_features, dtype=object)

# =========================
# ğŸ§ª æ¸¬è©¦ç¨‹å¼ç¢¼ï¼ˆå¯é¸ï¼‰
# =========================

if __name__ == "__main__":
    """
    å¿«é€Ÿæ¸¬è©¦ utils åŠŸèƒ½
    """
    print("="*80)
    print("Testing utils.py")
    print("="*80)
    
    # æ¸¬è©¦è·¯å¾‘
    print("\n1. æ¸¬è©¦è·¯å¾‘åŠŸèƒ½:")
    print(f"  ROOT: {ROOT}")
    print(f"  DIRS.models: {DIRS.models}")
    print(f"  DIRS.meta: {DIRS.meta}")
    
    # æ¸¬è©¦ save/load JSON
    print("\n2. æ¸¬è©¦ JSON åŠŸèƒ½:")
    test_data = {"test": "data", "value": 123}
    json_path = save_json(test_data, "test_utils", folder=DIRS.meta)
    print(f"  å·²å„²å­˜: {json_path}")
    
    loaded_data = load_json("test_utils", folder=DIRS.meta)
    print(f"  å·²è¼‰å…¥: {loaded_data}")
    assert loaded_data == test_data, "JSON æ¸¬è©¦å¤±æ•—ï¼"
    print("  âœ“ JSON æ¸¬è©¦é€šé")
    
    # æ¸¬è©¦ save_df
    print("\n3. æ¸¬è©¦ DataFrame åŠŸèƒ½:")
    import pandas as pd
    test_df = pd.DataFrame({"A": [1, 2, 3], "B": [4, 5, 6]})
    df_path = save_df(test_df, "test_utils.csv", folder=DIRS.tables)
    print(f"  å·²å„²å­˜: {df_path}")
    print("  âœ“ DataFrame æ¸¬è©¦é€šé")
    
    # æ¸¬è©¦ log_model_result
    print("\n4. æ¸¬è©¦ log_model_result:")
    log_model_result(
        model_name="test_model",
        params={"param1": 0.1, "param2": 100},
        metrics={"auc_val": 0.85, "brier_val": 0.15},
        notes="æ¸¬è©¦ç”¨æ¨¡å‹"
    )
    print("  âœ“ log_model_result æ¸¬è©¦é€šé")
    
    print("\n" + "="*80)
    print("âœ… æ‰€æœ‰æ¸¬è©¦é€šéï¼")
    print("="*80)

# =========================
# Public API
# =========================
__all__ = [
    "ROOT", "DIRS", "ensure_dirs",
    "path_in", "raw_path", "processed_path",
    "figure_path", "table_path", "meta_path", "model_path", "report_path", "notebook_path", "tuner_path",
    "save_df", "save_json", "save_pickle", "save_text", "save_bytes", "save_fig",
    "load_json",  # âœ… ç¢ºä¿é€™è¡Œå­˜åœ¨
    "MedianFromConfigImputer", "ToFloatTransformer",
    "log_model_result",
]

