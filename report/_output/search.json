[
  {
    "objectID": "index.html",
    "href": "index.html",
    "title": "Predicting Telecom Customer Churn with Explainable Machine Learning",
    "section": "",
    "text": "Abstract\nAbstract Customer churn prediction is a critical objective for telecom operators because retention efforts are typically more cost-effective than acquiring new customers. This thesis develops a leakage-safe and reproducible machine learning pipeline for churn prediction and benchmarks interpretable and high-performing models under realistic deployment constraints. The workflow covers feature cleaning and engineering, stratified holdout evaluation, and comparison across logistic regression, gradient-boosted tree models, deep neural networks (including focal-loss variants), and ensemble methods (weighted blending and out-of-fold stacking) to address class imbalance and improve ranking performance. Models are assessed using threshold-free metrics (ROC-AUC and PR-AUC) alongside a fixed operating point that reflects limited retention outreach capacity. The logistic regression baseline achieved a test ROC-AUC of 0.594 and PR-AUC of 0.356. Using validation ROC-AUC as the selection criterion, XGBoost was chosen as the champion model (val ROC-AUC ≈ 0.657, val PR-AUC ≈ 0.436), achieving test ROC-AUC of 0.664 and PR-AUC of 0.445. At the chosen operating threshold (τ = 0.44), the XGBoost model reached 0.367 precision, 0.739 recall, and 0.490 F1, prioritizing churn capture while maintaining actionable precision. While LightGBM achieved the highest test ROC-AUC (≈ 0.670), ensemble methods provided a modest additional lift, reaching approximately 0.669 ROC-AUC and 0.449 PR-AUC on the test set. Finally, SHAP-based interpretability is used to validate model behavior and translate predictions into retention insights by highlighting actionable patterns in customer tenure, usage dynamics, and service quality signals. Overall, the results show that explainable gradient-boosting models can deliver robust predictive performance while remaining operationally useful for targeted retention strategies.\n Telecom churn; supervised learning; gradient boosting; stacking ensemble; imbalanced classification; explainable machine learning; SHAP",
    "crumbs": [
      "<span class='chapter-number'>1</span>  <span class='chapter-title'>Abstract</span>"
    ]
  },
  {
    "objectID": "report/01-introduction.html",
    "href": "report/01-introduction.html",
    "title": "2  Introduction",
    "section": "",
    "text": "2.1 Background & Motivation\nCustomer churn, defined as customers canceling service or switching to a competing provider, is a major driver of revenue loss in the telecommunications industry. In saturated mobile markets, where growth through new subscriptions is constrained, retaining existing customers becomes strategically as important as acquiring new ones. Prior telecom studies document substantial churn levels and the operational difficulty of stabilizing subscriber bases over time (Ahn, Han, and Lee 2006; Wei and Chiu 2002).\nChurn directly erodes recurring revenue and increases operating costs because replacement typically requires additional spending on marketing, onboarding, and incentives. Retention-oriented marketing research further suggests that acquiring a new customer may cost multiple times more than retaining an existing customer, reinforcing why churn prevention is frequently treated as a high-impact managerial priority (Šonková and Grabowska 2015; Rosenberg and Czepiel 1984). Customer churn can also weaken longer-term customer equity by reducing lifetime value and creating a compounding revenue gap in subscription settings (Geiler, Affeldt, and Nadif 2022).\nThese economic and strategic pressures explain why churn prediction has become a standard analytics application in customer relationship management. By identifying high-risk customers in advance, telecom operators can prioritize proactive interventions and allocate retention budgets more efficiently. However, in operational contexts, predictive performance alone is insufficient. Retention teams typically require explanations of why a customer is flagged as high risk, because different churn drivers imply different actions (e.g., service quality remediation vs. price-based incentives). As a result, the churn modeling task is inherently dual-purpose: producing accurate risk scores while enabling actionable interpretation that supports decision-making.",
    "crumbs": [
      "<span class='chapter-number'>2</span>  <span class='chapter-title'>Introduction</span>"
    ]
  },
  {
    "objectID": "report/01-introduction.html#research-gap",
    "href": "report/01-introduction.html#research-gap",
    "title": "2  Introduction",
    "section": "2.2 Research Gap",
    "text": "2.2 Research Gap\nTwo persistent gaps motivate the present study.\nFirst, although many churn prediction studies have progressively adopted higher-capacity machine learning models, transparency and managerial actionability are not consistently addressed. Early work frequently relied on interpretable models such as logistic regression and decision trees, which can provide direct insight into drivers but may underperform when churn patterns are nonlinear or driven by complex feature interactions (Lemmens and Croux 2006; Bogaert and Delaere 2023). Recent work has moved beyond linear baselines and highlights ensemble and other flexible nonlinear models, including random forests, gradient boosting, support vector machines, and neural networks, which have been shown to improve predictive performance in churn prediction tasks (Vafeiadis et al. 2015; Bogaert and Delaere 2023). At the same time, this shift can reduce transparency, making it harder to convert model outputs into targeted retention actions.\nSecond, evaluation practices and “real-world” data constraints remain unevenly handled across the literature. Telecom churn data are commonly imbalanced, and naive reporting of accuracy can be misleading because a model can appear strong while failing to identify churners effectively (Burez and Van Den Poel 2009; Akosa 2017). Empirical work comparing imbalance-handling strategies highlights that model selection and thresholding decisions materially affect minority-class performance, yet such considerations are not always reflected in standard reporting practices (Zhu et al. 2023). Recent reviews further emphasize that churn prediction research continues to face challenges related to robustness and practical deployment considerations, including mismatches between experimental assumptions and production settings (Imani 2024).\nTogether, these gaps indicate a need for churn research that simultaneously (i) benchmarks strong predictive model families under a consistent experimental design and (ii) treats interpretability and decision-oriented evaluation as core requirements, not as secondary considerations.",
    "crumbs": [
      "<span class='chapter-number'>2</span>  <span class='chapter-title'>Introduction</span>"
    ]
  },
  {
    "objectID": "report/01-introduction.html#research-questions",
    "href": "report/01-introduction.html#research-questions",
    "title": "2  Introduction",
    "section": "2.3 Research Questions",
    "text": "2.3 Research Questions\nTo address the above gaps, this study is organized around three research questions:\n\nRQ1 (Model Effectiveness): Under a consistent telecom churn data setting, which model family provides the strongest overall performance among linear baselines, tree-based models, gradient boosting methods, tabular neural networks, and heterogeneous ensembles?\nRQ2 (Imbalance Robustness): Under class imbalance, which modeling and decision strategies yield more stable churner detection, particularly when using class weighting, alternative loss functions (e.g., focal loss), and decision-threshold adjustment, rather than relying on default accuracy-driven evaluation?\nRQ3 (Interpretability to Action): Can model interpretation outputs be transformed into actionable churn drivers that support customer profiling, risk-factor discovery, and retention-oriented grouping, while remaining consistent with the predictive pipeline used for final evaluation?",
    "crumbs": [
      "<span class='chapter-number'>2</span>  <span class='chapter-title'>Introduction</span>"
    ]
  },
  {
    "objectID": "report/01-introduction.html#contributions",
    "href": "report/01-introduction.html#contributions",
    "title": "2  Introduction",
    "section": "2.4 Contributions",
    "text": "2.4 Contributions\nThis study makes the following contributions, emphasizing a unified pipeline that balances predictive performance and interpretability:\n\nComparative modeling under consistent controls: Multiple model families are implemented and evaluated in a standardized setting, spanning interpretable baselines and higher-capacity learners reported as strong performers in churn prediction research (Lemmens and Croux 2006; Bogaert and Delaere 2023; Lalwani et al. 2022).\nImbalance-aware evaluation emphasis: The study foregrounds pitfalls of accuracy-dominated reporting and motivates decision-relevant evaluation under imbalance, consistent with prior findings in churn prediction methodology research (Burez and Van Den Poel 2009; Akosa 2017; Zhu et al. 2023).\nInterpretability aligned with deployment needs: The research treats interpretability as an operational requirement: model outputs are framed as inputs to retention actions (not solely as predictive scores), with the objective of producing explanations that support practical intervention design.\nPlanned extension beyond supervised learning: In addition to supervised modeling, a semi-supervised direction is articulated as future work in response to practical constraints highlighted in recent reviews, acknowledging that real-world churn labeling can be incomplete or delayed (Imani 2024).",
    "crumbs": [
      "<span class='chapter-number'>2</span>  <span class='chapter-title'>Introduction</span>"
    ]
  },
  {
    "objectID": "report/01-introduction.html#paper-roadmap",
    "href": "report/01-introduction.html#paper-roadmap",
    "title": "2  Introduction",
    "section": "2.5 Paper Roadmap",
    "text": "2.5 Paper Roadmap\nThe remainder of this thesis is structured as follows. Section 2 reviews related work on churn mechanisms, telecom data characteristics, modeling approaches, evaluation under class imbalance, and emerging research trends. Section 3 defines the dataset and prediction task, describes the experimental setup, and details preprocessing choices designed to support fair model comparison. Subsequent sections present the implemented modeling approaches, report experimental results, and provide interpretation-driven analysis aimed at connecting churn prediction to actionable retention insights.  \n\n\n\n\nAhn, Jae-Hyeon, Sang Pil Han, and Yung-Seop Lee. 2006. “Customer Churn Analysis: Churn Determinants and Mediation Effects of Partial Defection in the Korean Mobile Telecommunications Service Industry.” Telecommunications Policy 30 (November): 552–68. https://doi.org/10.1016/j.telpol.2006.09.006.\n\n\nAkosa, J. 2017. “Predictive Accuracy : A Misleading Performance Measure for Highly Imbalanced Data.” In.\n\n\nBogaert, Matthias, and Lex Delaere. 2023. “Ensemble Methods in Customer Churn Prediction: A Comparative Analysis of the State-of-the-Art.” Mathematics 11 (February): 1137. https://doi.org/10.3390/math11051137.\n\n\nBurez, J., and D. Van Den Poel. 2009. “Handling Class Imbalance in Customer Churn Prediction.” Expert Systems with Applications 36 (3): 4626–36. https://doi.org/10.1016/j.eswa.2008.05.027.\n\n\nGeiler, Louis, Séverine Affeldt, and Mohamed Nadif. 2022. “An Effective Strategy for Churn Prediction and Customer Profiling.” Data & Knowledge Engineering 142 (November): 102100. https://doi.org/10.1016/j.datak.2022.102100.\n\n\nImani, Mehdi. 2024. “Evaluating Classification and Sampling Methods for Customer Churn Prediction Under Varying Imbalance Levels.”\n\n\nLalwani, Praveen, Manas Kumar Mishra, Jasroop Singh Chadha, and Pratyush Sethi. 2022. “Customer Churn Prediction System: A Machine Learning Approach.” Computing 104 (2): 271–94. https://doi.org/10.1007/s00607-021-00908-y.\n\n\nLemmens, Aurélie, and Christophe Croux. 2006. “Bagging and Boosting Classification Trees to Predict Churn.” Journal of Marketing Research 43 (May). https://doi.org/10.1509/jmkr.43.2.276.\n\n\nRosenberg, Larry J., and John A. Czepiel. 1984. “A MARKETING APPROACH FOR CUSTOMER RETENTION.” Journal of Consumer Marketing 1 (2): 45–51. https://doi.org/10.1108/eb008094.\n\n\nŠonková, Tereza, and Monika Grabowska. 2015. “Customer Engagement: Transactional Vs. Relationship Marketing.” Journal of International Studies 8 (May): 196–207. https://doi.org/10.14254/2071-8330.2015/8-1/17.\n\n\nVafeiadis, T., K. I. Diamantaras, G. Sarigiannidis, and K.Ch. Chatzisavvas. 2015. “A Comparison of Machine Learning Techniques for Customer Churn Prediction.” Simulation Modelling Practice and Theory 55 (June): 1–9. https://doi.org/10.1016/j.simpat.2015.03.003.\n\n\nWei, Chih-Ping, and I-Tang Chiu. 2002. “Turning Telecommunications Call Details to Churn Prediction: A Data Mining Approach.” Expert Systems with Applications 23 (2): 103–12. https://doi.org/10.1016/S0957-4174(02)00030-1.\n\n\nZhu, Bing, Cheng Qian, Seppe vanden Broucke, Jin Xiao, and Yuanyuan Li. 2023. “A Bagging-Based Selective Ensemble Model for Churn Prediction on Imbalanced Data.” Expert Systems with Applications 227 (October): 120223. https://doi.org/10.1016/j.eswa.2023.120223.",
    "crumbs": [
      "<span class='chapter-number'>2</span>  <span class='chapter-title'>Introduction</span>"
    ]
  },
  {
    "objectID": "report/02-literature-review.html",
    "href": "report/02-literature-review.html",
    "title": "3  Literature Review",
    "section": "",
    "text": "3.1 Churn Theory and Behavioral Mechanisms\nTelecom Churn Prediction with Interpretable Machine Learning: From Behavioral Mechanisms to Modern Ensembles\nTelecom churn is often framed as a predictive classification problem, but the underlying phenomenon is a behavioral decision shaped by service experience, perceived value, and switching considerations. Service marketing research shows that customers frequently switch after adverse service encounters, dissatisfaction, or unmet expectations, while the dominant “reason for switching” can vary across customers and contexts (Keaveney 1995). Work on regain management further characterizes churn as a process of customer defection and potential recovery, where churn prevention and win-back should be evaluated jointly as part of relationship management rather than as isolated outcomes (“Regaining Service Customers - Bernd Stauss, Christian Friege, 1999” n.d.).\nA related theme is that loyalty and churn are influenced by both attitudinal factors (e.g., satisfaction and preference) and behavioral constraints (e.g., switching costs and barriers), implying that churn drivers can be nonlinear and interactive (Dick and Basu 1994). In mobile telecommunications, empirical studies highlight the roles of satisfaction, perceived service quality, and switching barriers in shaping loyalty and churn outcomes (Gerpott, Rams, and Schindler 2001; H.-S. Kim and Yoon 2004; M.-K. Kim, Park, and Jeong 2004). These findings motivate a practical view for churn modeling: features such as usage and engagement variables, billing and pricing proxies, tenure or contract indicators, and service-quality-related signals can be interpreted as measurable correlates of satisfaction and switching frictions rather than as purely random predictors. Accordingly, later sections interpret model explanations not only as statistical associations but also as evidence consistent with churn mechanisms discussed in prior retention research.",
    "crumbs": [
      "<span class='chapter-number'>3</span>  <span class='chapter-title'>Literature Review</span>"
    ]
  },
  {
    "objectID": "report/02-literature-review.html#telecom-churn-data-characteristics-and-practical-constraints",
    "href": "report/02-literature-review.html#telecom-churn-data-characteristics-and-practical-constraints",
    "title": "3  Literature Review",
    "section": "3.2 Telecom Churn Data Characteristics and Practical Constraints",
    "text": "3.2 Telecom Churn Data Characteristics and Practical Constraints\nTelecom churn datasets typically combine heterogeneous information sources, including numerical usage measures, customer demographics, billing variables, and high-cardinality categorical descriptors (e.g., plan identifiers or geographic codes). These properties create recurring modeling challenges that shape churn pipelines and motivate robust, reproducible experimentation (Lalwani et al. 2022; Vafeiadis et al. 2015). In applied churn research, the most emphasized constraints include:\n\nClass imbalance: Churn is often a minority event, so naïve training and default thresholding can bias models toward the majority class. Prior work proposes a range of imbalance-handling strategies, including reweighting, resampling (over- or under-sampling), and imbalance-aware ensemble methods, each of which can influence both measured performance and the set of customers prioritized for intervention (Beeharry and Tsokizep Fokone 2022; Sikri et al. 2024).\nMissingness and measurement noise: Telecom records are operational logs, so missing values may reflect business processes rather than random absence, and churn labels depend on operational definitions that may introduce label ambiguity.\nLeakage risk: Features created after the churn window (or strong proxies of post-churn outcomes) can inflate performance if not controlled. Leakage prevention is therefore part of credible benchmarking.\nTemporal instability: Behavior, product offerings, and marketing policies evolve. Drift-focused work emphasizes that churn models can degrade as the data-generating process shifts, motivating monitoring, retraining, and robustness checks (Bugajev, Kriauzienė, and Chadyšas 2025). These constraints motivate end-to-end workflows where preprocessing, splitting, and evaluation are standardized across model families. Consistent data handling is especially important when comparing heterogeneous algorithms because small preprocessing differences can confound conclusions about which model family truly performs best.",
    "crumbs": [
      "<span class='chapter-number'>3</span>  <span class='chapter-title'>Literature Review</span>"
    ]
  },
  {
    "objectID": "report/02-literature-review.html#modeling-approaches-for-telecom-churn-prediction",
    "href": "report/02-literature-review.html#modeling-approaches-for-telecom-churn-prediction",
    "title": "3  Literature Review",
    "section": "3.3 Modeling Approaches for Telecom Churn Prediction",
    "text": "3.3 Modeling Approaches for Telecom Churn Prediction\nThe churn literature spans interpretable baselines, high-capacity machine learning, and heterogeneous ensembles. These families differ in inductive bias, data requirements, and interpretability properties, which motivates systematic comparison within a unified pipeline.\n\n3.3.1 Interpretable Baselines and Classical Models\nLogistic regression and tree-based baselines remain widely used because they are transparent, stable, and straightforward to deploy in CRM environments. Benchmarking studies frequently treat such baselines as essential anchors even when more complex methods improve discrimination (Vafeiadis et al. 2015). Their limitations are also well recognized: linear decision boundaries may underfit nonlinear churn mechanisms, and shallow trees may sacrifice accuracy to maintain interpretability. For this reason, modern churn studies often implement baseline models not as end goals but as reference points for measuring incremental benefit from more complex approaches.\n\n\n3.3.2 Ensemble Learning and Tree-based Methods\nEnsemble learning is central in churn modeling because it can improve generalization by aggregating multiple learners and capturing nonlinear effects without extensive manual feature specification. Ensemble approaches have demonstrated benefits in churn prediction across domains (Coussement and De Bock 2013), and telecom-focused work similarly reports strong performance from tree ensembles on heterogeneous tabular data (Lalwani et al. 2022; Vafeiadis et al. 2015). Practical advantages include handling mixed feature types, automatically modeling interaction effects, and achieving high discrimination without deep representation learning.\n\n\n3.3.3 Neural and Hybrid Approaches for Tabular Churn Prediction\nDeep learning has also been explored for churn prediction, particularly when representation learning is expected to capture complex feature relationships or mitigate sparsity from categorical inputs. Empirical comparisons of supervised techniques have evaluated neural models alongside traditional methods, with results often depending on preprocessing choices, regularization, and dataset structure (Khodabandehlou and Rahman 2017). Neural-network-based churn prediction continues to appear in more recent surveys and applied studies, suggesting sustained interest in deep architectures for customer analytics (Thangeda, Kumar, and Majhi 2024). However, deep learning in tabular settings is commonly viewed as sensitive to feature encoding, hyperparameters, and imbalance-handling choices. Consequently, neural methods are most informative when evaluated within controlled benchmarking frameworks rather than assumed to universally outperform tree ensembles.\nA recurring theme is that neural methods are most informative when evaluated within a controlled benchmarking framework. Therefore, in this thesis deep learning is treated as one model family among several, compared under identical data splits and preprocessing constraints. This design enables a fair assessment of whether added capacity improves discrimination and stability on telecom-style tabular churn data, and it supports subsequent interpretability analyses.\n\n\n3.3.4 Stacking and Heterogeneous Ensembles\nBeyond bagging and boosting, stacking aims to exploit model diversity by combining base learners through a meta-learner trained on out-of-fold predictions. Telecom churn research suggests that stacking can improve performance when base models have complementary error profiles (Haddadi and Hamidi 2025). Related work further indicates that sampling-aware heterogeneous ensembles can create diversity via both algorithm choice and data views, which can be beneficial in imbalanced churn settings (Sikri et al. 2024). These findings support the use of heterogeneous stacking as a practical strategy when multiple strong models exist but no single method dominates across all evaluation dimensions.",
    "crumbs": [
      "<span class='chapter-number'>3</span>  <span class='chapter-title'>Literature Review</span>"
    ]
  },
  {
    "objectID": "report/02-literature-review.html#evaluation-class-imbalance-and-explainability",
    "href": "report/02-literature-review.html#evaluation-class-imbalance-and-explainability",
    "title": "3  Literature Review",
    "section": "3.4 Evaluation, Class Imbalance, and Explainability",
    "text": "3.4 Evaluation, Class Imbalance, and Explainability\nBecause churn prediction is operationalized as a ranking task for targeting retention actions, evaluation must go beyond overall accuracy. The churn literature emphasizes that imbalanced settings require metrics sensitive to minority-class performance and practical decision trade-offs (Beeharry and Tsokizep Fokone 2022; Jiao and Xu 2021; Sikri et al. 2024). In practice, this motivates reporting multiple complementary views of performance, including:\n\nDiscrimination metrics (e.g., ROC-AUC) to assess ranking ability across thresholds\nPrecision–recall trade-offs and related measures when churn is rare and false positives are costly\nThreshold- or top-k analyses to align model outputs with limited retention capacity Systematic reviews further emphasize that metric selection should reflect deployment goals and dataset imbalance rather than relying on a single score (Zhu et al. 2023). Accordingly, later sections report a metric set designed to support realistic retention decision-making and to compare models fairly under imbalance.\n\nBecause churn models are often used as probability scores to prioritize intervention, calibration and threshold selection are also practically important: a model can rank customers well but still produce probabilities that are systematically over- or under-confident. As a result, later sections complement discrimination metrics with calibration diagnostics and threshold-based analyses aligned to realistic retention capacity constraints.\nBeyond predictive discrimination, several studies emphasize linking churn scores to customer profiling and segmentation so that retention actions can be tailored. In this view, churn prediction is coupled with “who” and “why” analyses that identify groups of customers with distinct churn drivers, supporting more targeted marketing and service interventions (Geiler, Affeldt, and Nadif 2022). This motivates treating modeling and interpretability as a joint design problem: stronger models are valuable insofar as their outputs remain explainable and deployable for retention strategy design.\nExplainability is increasingly treated as essential for making churn models actionable. While baselines provide transparent coefficients or decision paths, ensembles and neural networks often require post-hoc methods. Recent churn-related studies demonstrate SHAP-based explanations as a practical approach to produce both global summaries (portfolio-level drivers) and local explanations (customer-level reasons for high risk) (Asif, Arif, and Mukheimer 2025; Poudel, Pokharel, and Timilsina 2024). Complementary tools for visualizing nonlinear feature effects are also commonly discussed, supporting interpretation in cases where churn drivers interact or exhibit threshold effects. This perspective motivates integrating explainability directly into the modeling pipeline. In this thesis, explainability is not treated as an afterthought but as a parallel analysis layer that supports error analysis, segment comparisons, and calibration/threshold discussions, thereby connecting predictive gains to interpretable churn mechanisms.",
    "crumbs": [
      "<span class='chapter-number'>3</span>  <span class='chapter-title'>Literature Review</span>"
    ]
  },
  {
    "objectID": "report/02-literature-review.html#open-issues-and-emerging-trends",
    "href": "report/02-literature-review.html#open-issues-and-emerging-trends",
    "title": "3  Literature Review",
    "section": "3.5 Open Issues and Emerging Trends",
    "text": "3.5 Open Issues and Emerging Trends\nSeveral open issues remain. Concept drift and temporal instability are persistent challenges in telecom churn because customer behavior and market conditions evolve; drift analyses reinforce the need for monitoring and periodic retraining if churn models are deployed in production (Bugajev, Kriauzienė, and Chadyšas 2025). Another direction is moving from “who will churn” to “who should be targeted,” where retention research argues that targeting customers solely by predicted churn risk can be suboptimal when high-risk customers are not always influenceable or profitable to retain. These issues motivate broader frameworks that integrate predictive performance with business value and intervention design.\nWithin this landscape, the empirical contribution of the present thesis is positioned as an engineering-grade, end-to-end churn prediction pipeline that systematically compares model families (interpretable baseline, tree ensembles, deep learning, and stacking) under realistic constraints such as imbalance and leakage control, while using SHAP-centered explainability to connect predictive outputs to plausible churn mechanisms and actionable retention insights. \n\n\n\n\nAsif, Daniyal, Muhammad Shoaib Arif, and Aiman Mukheimer. 2025. “A Data-Driven Approach with Explainable Artificial Intelligence for Customer Churn Prediction in the Telecommunications Industry.” Results in Engineering 26 (June): 104629. https://doi.org/10.1016/j.rineng.2025.104629.\n\n\nBeeharry, Yogesh, and Ristin Tsokizep Fokone. 2022. “Hybrid Approach Using Machine Learning Algorithms for Customers’ Churn Prediction in the Telecommunications Industry.” Concurrency and Computation: Practice and Experience 34 (4): e6627. https://doi.org/10.1002/cpe.6627.\n\n\nBugajev, Andrej, Rima Kriauzienė, and Viktoras Chadyšas. 2025. “Realistic Data Delays and Alternative Inactivity Definitions in Telecom Churn: Investigating Concept Drift Using a Sliding-Window Approach.” Applied Sciences 15 (3): 1599. https://doi.org/10.3390/app15031599.\n\n\nCoussement, Kristof, and Koen W. De Bock. 2013. “Customer Churn Prediction in the Online Gambling Industry: The Beneficial Effect of Ensemble Learning.” Journal of Business Research, Advancing Research Methods in Marketing, 66 (9): 1629–36. https://doi.org/10.1016/j.jbusres.2012.12.008.\n\n\nDick, A. S., and K. Basu. 1994. “Customer Loyalty: Toward an Integrated Conceptual Framework.” Journal of the Academy of Marketing Science 22 (2): 99–113. https://doi.org/10.1177/0092070394222001.\n\n\nGeiler, Louis, Séverine Affeldt, and Mohamed Nadif. 2022. “An Effective Strategy for Churn Prediction and Customer Profiling.” Data & Knowledge Engineering 142 (November): 102100. https://doi.org/10.1016/j.datak.2022.102100.\n\n\nGerpott, Torsten J, Wolfgang Rams, and Andreas Schindler. 2001. “Customer Retention, Loyalty, and Satisfaction in the German Mobile Cellular Telecommunications Market.” Telecommunications Policy 25 (4): 249–69. https://doi.org/10.1016/S0308-5961(00)00097-5.\n\n\nHaddadi, Amir Mohammad, and Hodjat Hamidi. 2025. “A Hybrid Model for Improving Customer Lifetime Value Prediction Using Stacking Ensemble Learning Algorithm.” Computers in Human Behavior Reports 18 (May): 100616. https://doi.org/10.1016/j.chbr.2025.100616.\n\n\nJiao, Gui’e, and Hong Xu. 2021. “Analysis and Comparison of Forecasting Algorithms for Telecom Customer Churn.” Journal of Physics: Conference Series 1881 (3): 032061. https://doi.org/10.1088/1742-6596/1881/3/032061.\n\n\nKeaveney, Susan M. 1995. “Customer Switching Behavior in Service Industries: An Exploratory Study.” Journal of Marketing 59 (2): 71–82.\n\n\nKhodabandehlou, Samira, and Mahmoud Rahman. 2017. “Comparison of Supervised Machine Learning Techniques for Customer Churn Prediction Based on Analysis of Customer Behavior.” Journal of Systems and Information Technology 19 (August): 00–00. https://doi.org/10.1108/JSIT-10-2016-0061.\n\n\nKim, Hee-Su, and Choong-Han Yoon. 2004. “Determinants of Subscriber Churn and Customer Loyalty in the Korean Mobile Telephony Market.” Telecommunications Policy 28 (9-10): 751–65. https://doi.org/10.1016/j.telpol.2004.05.013.\n\n\nKim, Moon-Koo, Myeong-Cheol Park, and Dong-Heon Jeong. 2004. “The Effects of Customer Satisfaction and Switching Barrier on Customer Loyalty in Korean Mobile Telecommunication Services.” Telecommunications Policy, Growth in mobile communications, 28 (2): 145–59. https://doi.org/10.1016/j.telpol.2003.12.003.\n\n\nLalwani, Praveen, Manas Kumar Mishra, Jasroop Singh Chadha, and Pratyush Sethi. 2022. “Customer Churn Prediction System: A Machine Learning Approach.” Computing 104 (2): 271–94. https://doi.org/10.1007/s00607-021-00908-y.\n\n\nPoudel, Sumana Sharma, Suresh Pokharel, and Mohan Timilsina. 2024. “Explaining Customer Churn Prediction in Telecom Industry Using Tabular Machine Learning Models.” Machine Learning with Applications 17 (September): 100567. https://doi.org/10.1016/j.mlwa.2024.100567.\n\n\n“Regaining Service Customers - Bernd Stauss, Christian Friege, 1999.” n.d. https://journals-sagepub-com.wwwproxy1.library.unsw.edu.au/doi/abs/10.1177/109467059914006. Accessed October 15, 2025.\n\n\nSikri, Alisha, Roshan Jameel, Sheikh Mohammad Idrees, and Harleen Kaur. 2024. “Enhancing Customer Retention in Telecom Industry with Machine Learning Driven Churn Prediction.” Scientific Reports 14 (1): 13097. https://doi.org/10.1038/s41598-024-63750-0.\n\n\nThangeda, Rahul, Niraj Kumar, and Ritanjali Majhi. 2024. “A Neural Network-Based Predictive Decision Model for Customer Retention in the Telecommunication Sector.” Technological Forecasting and Social Change 202 (May): 123250. https://doi.org/10.1016/j.techfore.2024.123250.\n\n\nVafeiadis, T., K. I. Diamantaras, G. Sarigiannidis, and K.Ch. Chatzisavvas. 2015. “A Comparison of Machine Learning Techniques for Customer Churn Prediction.” Simulation Modelling Practice and Theory 55 (June): 1–9. https://doi.org/10.1016/j.simpat.2015.03.003.\n\n\nZhu, Bing, Cheng Qian, Seppe vanden Broucke, Jin Xiao, and Yuanyuan Li. 2023. “A Bagging-Based Selective Ensemble Model for Churn Prediction on Imbalanced Data.” Expert Systems with Applications 227 (October): 120223. https://doi.org/10.1016/j.eswa.2023.120223.",
    "crumbs": [
      "<span class='chapter-number'>3</span>  <span class='chapter-title'>Literature Review</span>"
    ]
  },
  {
    "objectID": "report/03-data-and-problem.html",
    "href": "report/03-data-and-problem.html",
    "title": "4  Data and Problem Setup",
    "section": "",
    "text": "4.1 Data sources and study scope\nThis study utilizes a single primary dataset, cell2celltrain.csv, a customer-level telecom churn dataset consisting of 51,047 customer records and 58 original columns (including the identifier and raw target label). To facilitate binary classification modeling, the original string-based churn label was encoded into a binary numerical format (1/0) to serve as the final prediction target. Each row corresponds to one customer, with CustomerID serving as a unique identifier; no duplicate CustomerID values were detected.\nTo make feature handling auditable and reproducible across notebooks, the project also maintains a configuration-driven feature registry (built from feature_config_clean.csv) that specifies feature types (numeric/binary/ordinal/nominal), semantic groups, and risk/processing tags (e.g., “high_card”, “sparse_zero”, “zero_as_missing”). This design supports consistent “data scope” decisions (e.g., dropping identifiers, excluding leakage-prone fields) without hard-coding per-notebook logic.\nIn addition, a Detailed Feature Decision Table and a Data Dictionary were created as formal documentation of the dataset’s columns and the feature inclusion/exclusion rationale. The decision table is treated as a “single source of truth” for feature-level decisions and is provided in Section A.2.1 (Table A.1).",
    "crumbs": [
      "<span class='chapter-number'>4</span>  <span class='chapter-title'>Data and Problem Setup</span>"
    ]
  },
  {
    "objectID": "report/03-data-and-problem.html#prediction-target-and-observation-window",
    "href": "report/03-data-and-problem.html#prediction-target-and-observation-window",
    "title": "4  Data and Problem Setup",
    "section": "4.2 Prediction target and observation window",
    "text": "4.2 Prediction target and observation window\nThe modeling task is a binary classification problem: predicting whether a customer will churn. The dataset’s churn definition is event-based: “customers who churned 31- 60 days later” are labeled as churners. Many behavioral variables are constructed as historical summaries (e.g., four-month means), which aligns with a practical churn prediction setting where features represent pre-outcome customer state rather than post-churn operational actions.\nFrom a business perspective, this framing supports proactive retention: the goal is to rank customers by churn risk so that limited retention resources can be targeted toward those most likely to leave.",
    "crumbs": [
      "<span class='chapter-number'>4</span>  <span class='chapter-title'>Data and Problem Setup</span>"
    ]
  },
  {
    "objectID": "report/03-data-and-problem.html#data-documentation-and-feature-organization",
    "href": "report/03-data-and-problem.html#data-documentation-and-feature-organization",
    "title": "4  Data and Problem Setup",
    "section": "4.3 Data documentation and feature organization",
    "text": "4.3 Data documentation and feature organization\nBecause the dataset contains heterogeneous feature families (usage, billing, tenure, device, demographics, and operational indicators), EDA was used not only to summarize distributions but also to organize features into consistent groups for later modeling and interpretation. The project produced reusable documentation outputs, including a feature registry table (Section A.2.1) and a semantic grouping table (Section A.2.2).\nAt a high level, the raw columns include a mixture of numeric, integer, and object-typed variables (29 float64, 7 int64, 22 object in the raw load). A structured type and tag system was maintained for each feature (e.g., some features are tagged as high-cardinality, some as zero-as-missing, some as sparse-zero), enabling later preprocessing and model design to follow a documented specification rather than ad hoc decisions.\n\n\n\n\nTable 4.1: Feature registry excerpt (top 5 rows).\n\n\n\n\n\n\n\n\n\n\nfeature\norigin\nsemantic_group\ntype\ntransform_encoding\nkeep_glm\nkeep_tree\nkeep_nn\ndecision_type_short\n\n\n\n\n0\nCustomerID\nOriginal\nid_target\nIdentifier (ID)\n–\nFalse\nFalse\nFalse\ndrop\n\n\n1\nChurn\nOriginal\nid_target\nBinary target\nEncoded to 0/1 (Churn01)\nFalse\nFalse\nFalse\ndescriptive_only\n\n\n2\nMonthlyRevenue\nOriginal\nbilling_economics\nNumeric (continuous)\nNo transform (highly skewed) – consider log or...\nFalse\nTrue\nFalse\nreplace\n\n\n3\nMonthlyMinutes\nOriginal\nusage_activity\nNumeric (continuous)\nNo transform (consider log for GLM if heavy-ta...\nTrue\nTrue\nTrue\nkeep\n\n\n4\nTotalRecurringCharge\nOriginal\nbilling_economics\nNumeric (continuous)\n–\nFalse\nFalse\nFalse\ndrop\n\n\n\n\n\n\n\n\n\n\nThe complete registry is provided in Appendix A (see Section A.2.1, Complete Feature Registry).",
    "crumbs": [
      "<span class='chapter-number'>4</span>  <span class='chapter-title'>Data and Problem Setup</span>"
    ]
  },
  {
    "objectID": "report/03-data-and-problem.html#sec-eda",
    "href": "report/03-data-and-problem.html#sec-eda",
    "title": "4  Data and Problem Setup",
    "section": "4.4 Exploratory data analysis: what the data “looks like” and why it matters",
    "text": "4.4 Exploratory data analysis: what the data “looks like” and why it matters\nEDA in this study was designed to (i) quantify the dataset’s major modeling constraints (imbalance, sparsity, nonlinearity), (ii) detect leakage risks, and (iii) create reproducible tables/figures that guide later methodological choices.\n\n4.4.1 Class balance and baseline difficulty\nChurn is a minority class but not extremely rare: 14,711 churners (28.82%) versus 36,336 non-churners (71.18%). A naïve classifier that always predicts “non-churn” would achieve 0.7118 accuracy, demonstrating that accuracy alone is not a sufficient measure for this task.\n\n\n\n\n\n\nFigure 4.1: Churn class distribution in the Cell2Cell training set.\n\n\n\nThis imbalance motivated evaluation choices later in the thesis (e.g., ranking metrics and precision/recall-focused diagnostics), but the formal metric definitions are deferred to the Methodology section.\n\n\n4.4.2 Missingness, sparsity, and “zero inflation”\nA key constraint in this telecom dataset is that many variables are structurally sparse, meaning they are missing for most customers or recorded as zeros that effectively encode “not applicable” or “not observed”. The EDA therefore tracked missing values and zero inflation together (a “missing+zero” sparsity score) and saved a ranked summary.\nThe top-ranked features by combined sparsity were mainly zero-inflated retention and low-frequency call variables (e.g., CallForwardingCalls, RetentionOffersAccepted, RetentionCalls), followed by sparse call-behavior and customer-contact measures (e.g., ThreewayCalls, RoamingCalls, CustomerCareCalls) and high-missingness demographic/pricing proxies such as IncomeGroup, AgeHH1/AgeHH2, and HandsetPrice.\n\n\n\n\n\n\nFigure 4.2: Missingness and zero-rate diagnostics for input features.\n\n\n\n\n\n4.4.3 Ordinal, categorical, and continuous patterns\nEDA included several diagnostic plots for ordinal and categorical features, examining the relationship between variable categories and churn rate. For example, Figure 4.3 visualizes how churn risk varies across ordinal feature levels.\n\n\n\n\n\n\nFigure 4.3: Churn rates across ordinal variable levels.\n\n\n\nAn important categorical analysis tool was Cramér’s V, which measures association strength between categorical predictors and the binary churn outcome. Features with strong associations are potentially informative but require care to distinguish from leakage. Figure 4.4 presents the Cramér’s V ranking for the top categorical features.\n\n\n\n\n\n\nFigure 4.4: Cramér’s V association between top categorical features and churn label.\n\n\n\n\n\n4.4.4 Leakage risk and variable exclusion\nA central rigor requirement in churn modeling is avoiding predictors that implicitly contain post-outcome information. This study therefore treated leakage control as a data scoping decision: features that would not be available at prediction time (or that are only triggered once churn intent is revealed) were excluded from the modeling dataset.\nA dedicated leakage scan (saved as T5j_leakage_risk_rank.csv) ranked features by a composite leakage risk score (see Section A.2.3 for the complete ranking). In the scan, the highest-risk features were retention intervention variables such as MadeCallToRetentionTeam, RetentionCalls, and RetentionOffersAccepted, which were flagged as essentially post-churn indicators. The EDA further visualized these risks using leakage-flag churn comparisons and saved the summary plot.\n\n\n\n\n\n\nFigure 4.5: Leakage risk visualization: churn rates by retention-related flag values.\n\n\n\nBased on both statistical evidence and business-process logic, these retention-related variables were treated as leakage and excluded from all predictive models. This choice preserves realism: a churn model should predict before retention actions occur, not infer churn because an intervention has already been triggered.",
    "crumbs": [
      "<span class='chapter-number'>4</span>  <span class='chapter-title'>Data and Problem Setup</span>"
    ]
  },
  {
    "objectID": "report/03-data-and-problem.html#artifacts-and-documentation-outputs",
    "href": "report/03-data-and-problem.html#artifacts-and-documentation-outputs",
    "title": "4  Data and Problem Setup",
    "section": "4.5 Artifacts and documentation outputs",
    "text": "4.5 Artifacts and documentation outputs\nFeature documentation artifacts (T0_feature_registry.csv, T0_semantic_groups_table.csv) were generated and reused across notebooks (see Section A.2.1 and Section A.2.2 for complete tables).\nThe EDA stage produced:\n\nSparsity diagnostics (T1_sparsity_overview.csv; see Figure 4.2).\nOrdinal trend analysis (T4_ordinal_trends.csv; see Figure 4.3).\nCategorical association analysis (T4_cramers_v_vs_churn.csv; see Figure 4.4).\nLeakage risk ranking (T5j_leakage_risk_rank.csv; see Figure 4.5 and Section A.2.3 for the full ranking).\n\nThe complete registry is provided in Appendix A (see Section A.2.1, Complete Feature Registry).",
    "crumbs": [
      "<span class='chapter-number'>4</span>  <span class='chapter-title'>Data and Problem Setup</span>"
    ]
  },
  {
    "objectID": "report/03-data-and-problem.html#summary-of-data-constraints",
    "href": "report/03-data-and-problem.html#summary-of-data-constraints",
    "title": "4  Data and Problem Setup",
    "section": "4.6 Summary of data constraints",
    "text": "4.6 Summary of data constraints\n\nmoderate class imbalance (28.82% churn), (2) substantial sparsity and zero inflation across multiple feature families, (3) nonlinear churn patterns (e.g., a U-shaped CreditRating risk curve), and (4) high-stakes leakage risks in retention-related variables requiring explicit exclusion. These observations establish the constraints and motivations for the modeling comparisons presented in subsequent sections.",
    "crumbs": [
      "<span class='chapter-number'>4</span>  <span class='chapter-title'>Data and Problem Setup</span>"
    ]
  },
  {
    "objectID": "report/04-methodology.html",
    "href": "report/04-methodology.html",
    "title": "5  Methodology",
    "section": "",
    "text": "5.1 Data Preprocessing Pipeline\nThe preprocessing pipeline transforms the raw Cell2Cell features into a modeling-ready format while strictly preventing data leakage between training, validation, and test splits. All pipeline parameters are estimated on the training set only and then applied unchanged to validation and test sets.\nFeature scoping and leakage control. Feature inclusion is governed by a feature registry that records which columns are retained and how they should be treated for different model families. This registry-driven approach avoids per-notebook, ad hoc column selection and makes the modeling dataset auditable. As part of scoping, identifier fields and target fields are excluded from the feature set. In addition, retention-intervention variables are removed because they plausibly reflect post-outcome actions (for example, contact with a retention team or accepted retention offers) and can introduce leakage if used as predictors.\nTraining-only fitting of preprocessing parameters. To avoid leakage through preprocessing, all transformation parameters are estimated using the training split only, and then reused unchanged to transform validation and test sets (details of the splitting protocol are described in Section 5.2). For numeric variables, the pipeline computes training-set medians for imputation and training-set means and standard deviations for standardization. For categorical variables, it constructs a training-derived mapping from category strings to integer codes, reserving a default code for missing or unseen categories. The fitted parameters are saved for reproducibility and reusability across subsequent experiments.\nTwo output representations. Because different model families have different input requirements, the pipeline outputs two feature representations:\nOverall, this preprocessing design functions as a stable “data contract” for the rest of the thesis: later sections can focus on modeling and evaluation choices while relying on a consistent, leakage-aware, and reproducible transformation of raw telecom records into model inputs.\nFigure 5.1: Preprocessing pipeline overview showing data flow from raw features to model-ready representations.",
    "crumbs": [
      "<span class='chapter-number'>5</span>  <span class='chapter-title'>Methodology</span>"
    ]
  },
  {
    "objectID": "report/04-methodology.html#sec-preprocess",
    "href": "report/04-methodology.html#sec-preprocess",
    "title": "5  Methodology",
    "section": "",
    "text": "Base representation: A one-hot encoded feature matrix suitable for logistic regression and tree-based models, which require fixed-width numeric input without embedding layers.\nDeep representation: A structured output separating continuous features (as a float tensor), categorical features (as integer indices for embedding lookup), and binary features (as a separate float tensor). This format feeds directly into PyTorch data loaders for neural network training.",
    "crumbs": [
      "<span class='chapter-number'>5</span>  <span class='chapter-title'>Methodology</span>"
    ]
  },
  {
    "objectID": "report/04-methodology.html#sec-eval-protocol",
    "href": "report/04-methodology.html#sec-eval-protocol",
    "title": "5  Methodology",
    "section": "5.2 Experimental Design and Evaluation Protocol",
    "text": "5.2 Experimental Design and Evaluation Protocol\nAll modeling experiments in this thesis follow a unified evaluation protocol to ensure fair comparison across model families. The core design choices are described below.\nData splitting strategy. The labeled dataset was divided into three non-overlapping subsets: a training set (70%), a validation set (15%), and a test set (15%). Stratified sampling was applied to maintain consistent churn prevalence (approximately 28.8%) across splits. The training set is used for model fitting and hyperparameter search; the validation set guides early stopping, threshold selection, and model selection decisions; the test set is reserved for final, unbiased performance reporting. No model selection or tuning decisions were made based on test-set results.\nPrimary evaluation metrics. Model ranking and selection were based primarily on two threshold-independent metrics:\n\nROC-AUC (Area Under the Receiver Operating Characteristic Curve): Measures the probability that a randomly chosen positive (churner) is ranked higher than a randomly chosen negative (non-churner). This metric is standard for binary classification and is insensitive to class imbalance when used for ranking purposes.\nPR-AUC (Area Under the Precision–Recall Curve): Also known as Average Precision, this metric is more sensitive to performance on the minority class (churners) and is particularly relevant when class imbalance is present.\n\nFixed operating threshold for decision-level metrics. To enable consistent comparison of decision-level performance (precision, recall, F1), a single operating threshold (\\(\\tau = 0.4400\\)) was selected on the validation set during the main tree-based model selection and then applied to all models uniformly. This fixed-threshold policy avoids per-model “best threshold” cherry-picking and reflects realistic deployment scenarios where a single, stable decision rule is applied across all customers.\nCross-validation for hyperparameter search. Where applicable (logistic regression, XGBoost with Optuna), hyperparameters were tuned using stratified 5-fold cross-validation on the training set, with ROC-AUC as the optimization objective. The best hyperparameters were then used to refit a final model on the full training set before evaluation on validation and test sets.\nReproducibility controls. All experiments used fixed random seeds (42) for data splitting, model initialization, and stochastic optimization. Model artifacts, including fitted parameters, training curves, and prediction outputs, were saved to enable exact reproduction of results.",
    "crumbs": [
      "<span class='chapter-number'>5</span>  <span class='chapter-title'>Methodology</span>"
    ]
  },
  {
    "objectID": "report/04-methodology.html#sec-logit",
    "href": "report/04-methodology.html#sec-logit",
    "title": "5  Methodology",
    "section": "5.3 Baseline Model: Regularized Logistic Regression",
    "text": "5.3 Baseline Model: Regularized Logistic Regression\nA regularized logistic regression model was developed as the supervised baseline for this churn prediction task. Logistic regression offers a transparent, interpretable starting point: its linear decision boundary and directly interpretable coefficients make it straightforward to identify which features contribute most to churn prediction before introducing more complex, non-linear methods.\nThe logistic regression model was trained on the same cleaned, leakage-controlled feature scope defined in the project’s feature registry, using the subset of predictors designated for the GLM family (i.e., features marked as retained for the linear baseline). Because categorical variables had already been expanded into a consistent one-hot feature space in the “base” representation created during preprocessing, the logistic regression stage operated directly on this fixed design matrix. Feature inclusion followed the registry logic: when a raw feature corresponded to a categorical variable, its associated one-hot columns were included as a group (via prefix-based matching), ensuring that the linear baseline used the complete encoded information for each retained categorical predictor while excluding disallowed or leaky fields.\nModel fitting used L1/L2-regularized logistic regression with a controlled hyperparameter search restricted to the most impactful regularization choices. Specifically, the inverse regularization strength and the penalty type (L1 vs. L2) were tuned using a grid search. The search was evaluated via stratified 5-fold cross-validation on the training split, using ROC-AUC as the selection criterion, consistent with the overall evaluation protocol described in Section 5.2. The hyperparameter search space was intentionally constrained to regularization strength (\\(C\\)) and penalty type. This restricted scope ensures the baseline remains interpretable and stable, avoiding the risk of over-engineering.\nAfter selecting the best hyperparameters under cross-validation, the final baseline model was refitted on the full training split and then evaluated under the common experimental protocol (validation used for operating-point selection; test reserved for final reporting). Threshold selection and any constraint-based operating-point choice were handled outside the training objective and followed the unified decision rule described in Section 5.2, so that the baseline and later models were compared under the same deployment-style evaluation assumptions.",
    "crumbs": [
      "<span class='chapter-number'>5</span>  <span class='chapter-title'>Methodology</span>"
    ]
  },
  {
    "objectID": "report/04-methodology.html#sec-trees",
    "href": "report/04-methodology.html#sec-trees",
    "title": "5  Methodology",
    "section": "5.4 Tree and Gradient-Boosted Tree Models",
    "text": "5.4 Tree and Gradient-Boosted Tree Models\nTree-based methods were the primary non-linear modeling family for this structured telecom dataset. The modeling strategy progressed from a simple, interpretable tree to ensemble methods that improve generalization, and finally to gradient-boosted frameworks that typically deliver the strongest ranking performance on tabular data. Data splitting, metric definitions, and the shared threshold policy follow the experimental protocol in Section 5.2.\nAll tree models were trained on the same base, one-hot encoded representation produced by the preprocessing pipeline (Section 5.1). To keep the tree family comparable and leakage-safe, input columns were selected using the project’s feature registry (features explicitly marked as allowed for tree models). Categorical predictors were represented by their one-hot expansions in a shared column space. When a raw feature was retained, all corresponding dummy columns were included together (via prefix matching), ensuring consistent inclusion of encoded categorical information across the tree family.\n\n5.4.1 Decision Tree Baseline (CART)\nA single classification tree was used as a simple non-linear reference point. Because unpruned trees can overfit on tabular data, this baseline used explicit structural constraints, including a bounded maximum depth and minimum sample requirements for splits and leaves. These constraints provide controlled flexibility while preserving interpretability at the segment and rule level. Class imbalance was handled through balanced class weights so that splits were not dominated by the majority (non-churn) class. This model serves as a minimum viable non-linear benchmark and tests whether a small set of hierarchical rules can capture churn signals beyond a linear baseline.\n\n\n5.4.2 Random Forest Ensemble\nTo improve stability and generalization beyond a single tree, a random forest classifier was trained as a bagging-based ensemble. Random forests reduce variance by averaging many decorrelated trees trained on bootstrap samples, and they typically provide stronger ranking quality than a single CART without requiring sensitive tuning. The implementation used a fixed configuration with a substantial number of trees and moderate depth control, along with feature subsampling to encourage diversity between trees. Class imbalance was addressed through balanced class weights, mirroring the single-tree setup. The forest is treated as a strong classical ensemble baseline rather than the primary optimized model.\n\n\n5.4.3 Gradient-Boosted Frameworks: XGBoost and LightGBM\nGradient boosting was implemented with XGBoost as the main boosted framework. The baseline XGBoost configuration used moderate tree depth, a standard learning rate, and row and column subsampling to provide competitive performance without heavy search. Because boosting optimizes a differentiable loss over all observations, imbalance was handled through positive-class reweighting using a prevalence-based weight derived from the training set. This keeps the dataset intact while aligning the objective with churn detection asymmetry.\nLightGBM was trained as a complementary boosted reference model. LightGBM uses a histogram-based algorithm and leaf-wise growth strategy, offering a different bias profile and computational behavior from XGBoost. In this study it was configured as a compact model with a fixed parameter set (rather than a full Bayesian search), including a moderate learning rate, a controlled number of estimators, and explicit regularization. Imbalance handling again used prevalence-based positive-class weighting.\n\n\n5.4.4 Bayesian Hyperparameter Optimization\nTo obtain a high-quality boosted model under a reproducible and auditable search process, XGBoost was tuned using Bayesian optimization with Optuna. The tuning objective was mean ROC-AUC under stratified cross-validation on the training split, consistent with Section 5.2. A Tree-structured Parzen Estimator sampler proposed hyperparameters, and a median-based pruning rule stopped unpromising trials early to improve compute efficiency without changing the evaluation criterion.\nThe search space covered key capacity and regularization controls, including number of estimators, tree depth, learning rate (sampled on a log scale), row subsampling, column subsampling, minimum child weight, split regularization via gamma, and L1/L2 regularization. The best hyperparameters selected by cross-validated ROC-AUC were used to refit a final XGBoost model on the full training split before entering the shared validation and test evaluation pipeline.\n\n\n5.4.5 Reproducibility and Model Artifacts\nAcross all tree and boosted models, the implementation was designed for deterministic reruns under the same data splits and feature registry constraints. Tuned parameters, fitted models, and evaluation summaries were saved as artifacts (serialized models and JSON metadata) to ensure that the methodology is auditable and that later stages such as ensembling and interpretation can reuse the same fitted estimators rather than retraining them implicitly.",
    "crumbs": [
      "<span class='chapter-number'>5</span>  <span class='chapter-title'>Methodology</span>"
    ]
  },
  {
    "objectID": "report/04-methodology.html#sec-deep-learning",
    "href": "report/04-methodology.html#sec-deep-learning",
    "title": "5  Methodology",
    "section": "5.5 Deep Learning Architecture",
    "text": "5.5 Deep Learning Architecture\nNeural network models were developed to test whether learned representations and higher-order non-linear interactions could provide incremental gains beyond tree-based methods on customer churn prediction. Unlike GBDTs, which learn interactions through recursive partitioning, neural models can combine continuous covariates and categorical embeddings in a unified latent space and potentially capture smooth interaction effects. All neural models were implemented in PyTorch and trained/evaluated under the common experimental protocol in Section 5.2.\n\n5.5.1 Input representation and dataset construction\nNeural models used the “deep” feature representation produced by the preprocessing stage, which separates variables into three semantic types to match common tabular deep learning practice:\n\n5.5.1.1 Continuous features (float tensors).\nA fixed set of numeric variables was provided as continuous inputs. These features were preprocessed in a leakage-safe manner using training-derived parameters (imputation and standardization), resulting in approximately zero-mean, unit-variance scaled inputs. Standardization improves optimization stability for gradient-based training and reduces sensitivity to feature scale differences.\n\n\n5.5.1.2 Categorical features (integer-index tensors).\nHigh-cardinality and ordinal variables were mapped to integer indices suitable for embedding lookup. Each categorical column was encoded using a training-derived vocabulary so that unseen categories at inference time can be mapped to a reserved index. This representation allows the network to learn dense, low-dimensional representations for each category rather than relying on sparse one-hot vectors.\n\n\n5.5.1.3 Binary features (float tensors).\nBinary indicators (e.g., yes/no flags, converted to 0/1) were passed directly as float inputs, preserving their semantic status while avoiding unnecessary embedding overhead for two-level variables.\nThis three-part input structure is assembled by a custom PyTorch Dataset class that returns aligned tensors for each batch, enabling modular experimentation with different network heads while keeping the data pipeline consistent.\n\n\n\n5.5.2 Embedding MLP model\nThe first neural architecture is an Embedding MLP that learns dense representations for categorical features and concatenates them with continuous and binary inputs before passing through a multi-layer perceptron. This design follows common practice for tabular deep learning, where categorical embeddings can capture richer interactions than one-hot encoding while keeping the model end-to-end differentiable.\n\n5.5.2.1 Embedding layer.\nEach categorical feature is mapped to a learnable embedding vector. The embedding dimension for each feature is set heuristically (e.g., \\(\\min(50, \\text{cardinality} // 2)\\)) to balance expressiveness and regularization. All embeddings are concatenated into a single vector per sample.\n\n\n5.5.2.2 MLP trunk.\nThe concatenated representation (embeddings + continuous + binary) is passed through a feedforward network with ReLU activations and dropout regularization between layers. A typical configuration uses three hidden layers with decreasing width (e.g., 256 → 128 → 64), though the exact architecture can be varied experimentally.\n\n\n5.5.2.3 Output head.\nA final linear layer with a single output unit produces a logit, which is passed through a sigmoid activation to yield a churn probability. For training, the loss is computed directly on the logit (using BCEWithLogitsLoss or focal loss variants), which is numerically more stable than applying sigmoid before loss computation.\n\n\n\n\n\n\n\n\nFigure 5.2: Embedding MLP architecture: categorical embeddings are concatenated with continuous/binary features and passed through a multi-layer perceptron.\n\n\n\n\n\n\n\n\n5.5.3 Wide & Deep model\nThe second architecture is a Wide & Deep network inspired by the approach introduced by Google for recommendation systems. The key idea is to combine a “wide” linear component (which memorizes feature interactions explicitly) with a “deep” neural component (which generalizes through learned representations). This hybrid structure can capture both low-order, interpretable effects and high-order, non-linear interactions.\n\n5.5.3.1 Wide component.\nA linear layer directly connects the concatenated input features (continuous + binary + flattened embeddings or one-hot encoded categoricals) to the output logit. This component acts like a logistic regression and can memorize specific feature combinations that are predictive of churn.\n\n\n5.5.3.2 Deep component.\nThe same input representation is passed through an MLP trunk (similar to the Embedding MLP described above) to produce a learned representation that captures non-linear interactions. The final hidden layer output is concatenated with the wide component’s linear contribution before the output layer.\n\n\n5.5.3.3 Output combination.\nThe wide and deep contributions are summed (or concatenated and passed through a final linear layer) to produce the final logit. This architecture allows the model to benefit from both memorization (wide) and generalization (deep), which can be particularly useful for tabular data with both dense and sparse predictive patterns.\n\n\n\n\n\n\n\n\nFigure 5.3: Wide & Deep architecture combining a linear ‘wide’ path with a non-linear ‘deep’ path for churn prediction.\n\n\n\n\n\n\n\n\n5.5.4 Training objective and handling class imbalance\nClass imbalance (approximately 29% churn vs. 71% non-churn) can bias neural network training toward the majority class if not addressed. Two approaches were implemented and compared:\n\n5.5.4.1 Weighted Binary Cross-Entropy (BCE).\nThe standard BCE loss is modified by assigning a higher weight to positive (churn) samples. If \\(w_{\\text{pos}}\\) denotes the positive-class weight, the loss for a single sample becomes:\n\\[\n\\mathcal{L}_{\\text{weighted-BCE}} = -w_{\\text{pos}} \\cdot y \\log(\\hat{p}) - (1 - y) \\log(1 - \\hat{p})\n\\]\nThe weight \\(w_{\\text{pos}}\\) is typically set to the inverse class frequency ratio (e.g., \\(w_{\\text{pos}} = \\frac{N_{\\text{neg}}}{N_{\\text{pos}}}\\)) so that the total contribution of positive and negative samples is balanced. This approach directly addresses the numerical dominance of majority-class gradients during training.\n\n\n5.5.4.2 Focal Loss.\nAn alternative is focal loss, which down-weights easy examples (those classified correctly with high confidence) and focuses training on hard examples:\n\\[\n\\mathcal{L}_{\\text{focal}} = -\\alpha_t (1 - p_t)^\\gamma \\log(p_t)\n\\]\nwhere \\(p_t = \\hat{p}\\) if \\(y = 1\\) else \\(1 - \\hat{p}\\), \\(\\alpha_t\\) is a class-balancing weight, and \\(\\gamma &gt; 0\\) is the focusing parameter. Higher \\(\\gamma\\) increases the focus on hard-to-classify examples. Focal loss was originally proposed for object detection but has shown benefits for imbalanced tabular classification as well.\nImportantly, both approaches preserve the original data distribution (no oversampling) and fit naturally into the probabilistic scoring framework required by Section 5.2 (ROC/PR evaluation and validation-based thresholding).\n\n\n\n5.5.5 Optimization, regularization, and model selection\nAll neural models were trained using the Adam optimizer with a learning rate selected from a small grid (e.g., \\(\\{10^{-3}, 10^{-4}\\}\\)). Training proceeded for a maximum number of epochs with early stopping based on validation ROC-AUC: if the validation metric did not improve for a specified patience window, training was halted and the best checkpoint was restored.\nRegularization was applied through dropout (applied after each hidden layer) and optional weight decay. Batch normalization was not used in the main experiments to keep the architecture simple and to avoid potential issues with small batch sizes during inference.\nModel selection among neural architectures (Embedding MLP vs. Wide & Deep) and loss functions (weighted BCE vs. focal loss) was performed by comparing validation ROC-AUC. The best-performing configuration was then evaluated on the held-out test set for final reporting.\nArtifacts and reproducibility. All models output churn risk scores that were evaluated using the unified metrics and thresholding procedure in Section 5.2. Artifacts, including training curves, saved model weights, and prediction files, were recorded to enable reproducibility and to support downstream interpretation and ensemble construction in later sections.",
    "crumbs": [
      "<span class='chapter-number'>5</span>  <span class='chapter-title'>Methodology</span>"
    ]
  },
  {
    "objectID": "report/04-methodology.html#sec-uae",
    "href": "report/04-methodology.html#sec-uae",
    "title": "5  Methodology",
    "section": "5.6 Unsupervised and Semi-supervised Extension",
    "text": "5.6 Unsupervised and Semi-supervised Extension\nBeyond purely supervised learning, I investigated whether unlabeled customer records could be exploited to improve churn prediction through (i) unsupervised representation learning and (ii) confidence-based pseudo-labeling. The core idea is that, even without labels, the marginal distribution of customer behavior may contain useful structure (e.g., common usage profiles and atypical patterns) that can be distilled into a compact latent representation and transferred to the downstream churn model. Consistent with Section 5.1, this extension operates on the same cleaned feature schema, while Section 5.2 defines the evaluation protocol used to compare all variants.\n\n5.6.1 Denoising Autoencoder (DAE)\nA denoising autoencoder was trained on the numeric feature subset (all continuous columns) without using churn labels. The DAE consists of:\n\nEncoder: A feedforward network that maps the (corrupted) input features to a lower-dimensional latent representation \\(\\mathbf{z}\\).\nDecoder: A symmetric feedforward network that reconstructs the original (uncorrupted) input from \\(\\mathbf{z}\\).\nNoise injection: During training, input features are corrupted by additive Gaussian noise or masking dropout. The model is trained to reconstruct the clean input, which encourages the encoder to learn robust, denoised representations.\n\nThe reconstruction loss (mean squared error between input and output) is minimized using Adam optimizer with early stopping based on validation reconstruction loss.\nLabel-Free Training. The DAE is trained without using churn labels, and can therefore incorporate both labeled and unlabeled records in a leakage-safe manner (labels are never accessed). Model selection is performed using a held-out validation split and early stopping based on validation reconstruction loss, selecting the epoch that best balances fit and generalization of the learned representation. Once trained, the encoder is treated as a deterministic feature extractor that produces a latent vector for each customer.\n\n\n5.6.2 Downstream Usage: Latent Feature Augmentation\nAfter training, the encoder is frozen and used to extract latent representations for all samples (train, validation, test, and unlabeled holdout). These latent features are then concatenated with the original feature set to form an augmented input representation for the downstream supervised model (e.g., XGBoost). The hypothesis is that the learned latent space may capture useful structure (e.g., customer segments, anomaly patterns) that improves discrimination when combined with the original features.\nBoth variants are evaluated against the supervised baseline using the identical split and metric definitions in Section 5.2, ensuring that any observed change can be attributed to representation learning rather than differences in preprocessing or evaluation.\n\n\n5.6.3 Pseudo-Labeling Extension\nTo test whether unlabeled data can provide additional training signal, a pseudo-labeling strategy was implemented:\n\nTeacher model: An Optuna-tuned XGBoost model trained on labeled data serves as the teacher.\nConfidence thresholding: The teacher scores the unlabeled holdout set. Samples with predicted probability \\(p \\geq 0.95\\) are pseudo-labeled as churners; samples with \\(p \\leq 0.05\\) are pseudo-labeled as non-churners. Samples in between are discarded as uncertain.\nStudent training: A new model is trained on the union of labeled data and high-confidence pseudo-labeled data.\n\nThis conservative threshold strategy prioritizes label quality over quantity, accepting only pseudo-labels where the teacher is highly confident.",
    "crumbs": [
      "<span class='chapter-number'>5</span>  <span class='chapter-title'>Methodology</span>"
    ]
  },
  {
    "objectID": "report/04-methodology.html#sec-stacking",
    "href": "report/04-methodology.html#sec-stacking",
    "title": "5  Methodology",
    "section": "5.7 Heterogeneous Stacking Ensemble",
    "text": "5.7 Heterogeneous Stacking Ensemble\nTo improve robustness beyond any single model family, I implemented a heterogeneous ensemble that combines complementary inductive biases through probability-level aggregation. Rather than relying on a single “best” learner, the ensemble treats each trained model as a noisy but informative estimator of churn risk, and then learns (or prescribes) a principled rule for combining their probability outputs. This section describes the ensemble design and training protocol; performance comparisons are reported later under the common evaluation policy defined in Section 5.2.\n\n5.7.1 Base Learners\nThe ensemble uses the following base learners, all trained on the same feature representation and data splits:\n\nLogistic Regression (linear baseline)\nXGBoost (Optuna-tuned gradient boosting)\nLightGBM (histogram-based gradient boosting)\nRandom Forest (bagging-based ensemble)\nWide & Deep Neural Network (best neural configuration)\n\nThese models span different inductive biases: linear vs. non-linear, tree-based vs. neural, single-model vs. ensemble. The diversity in model families is expected to provide complementary error profiles, which is the key requirement for ensemble benefit.\n\n\n5.7.2 Blending Strategies\nThree blending strategies were evaluated:\n\nSimple Average: Equal-weight averaging of predicted probabilities: \\(\\hat{p}_{\\text{blend}} = \\frac{1}{K} \\sum_{k=1}^{K} \\hat{p}_k\\)\nAUC-Weighted Average: Weights proportional to each model’s validation ROC-AUC: \\(w_k = \\frac{\\text{AUC}_k}{\\sum_j \\text{AUC}_j}\\)\nNNLS-Optimized Weights: Non-negative least squares regression on validation predictions to find optimal weights that minimize squared error relative to true labels.\n\n\n\n5.7.3 Out-of-Fold (OOF) Stacking\nLeakage Control via OOF. The key methodological requirement for stacking is leakage control, as the meta-learner must be trained on base predictions that are out-of-sample with respect to the base learners. I therefore used a \\(K\\)-fold Out-of-Fold (OOF) protocol on the training split. For each fold \\(k\\), each base estimator is fit on the \\(K - 1\\) training folds and used to predict probabilities on the held-out fold. Concatenating held-out predictions across folds yields an OOF prediction matrix \\(Z_{OOF}\\) where every row corresponds to a training instance and every column to a base learner, with the guarantee that each entry was generated by a model that did not train on that instance.\nMeta-Learner Training. A Ridge logistic regression meta-learner is trained on \\(Z_{OOF}\\) to learn optimal combination weights. The meta-learner’s coefficients indicate the relative contribution of each base learner to the final ensemble prediction.\n\n\n\n\n\n\n\n\nFigure 5.4: OOF stacking architecture: base learners produce out-of-fold predictions that are combined by a meta-learner.\n\n\n\n\n\n\n\n5.7.4 Meta-Learner Weights Visualization\nAfter training, the meta-learner coefficients can be visualized to understand the relative contribution of each base model. Positive coefficients indicate models that contribute positively to the ensemble prediction; larger coefficients indicate stronger contributions.\n\n\n\n\n\n\n\n\nFigure 5.5: Meta-learner coefficients showing the contribution of each base model to the stacking ensemble.\n\n\n\n\n\n\n\n5.7.5 Inference Pipeline\nAt inference time, all base models are applied to the test set to produce a prediction matrix \\(Z_{\\text{test}}\\). The meta-learner then combines these predictions to produce final ensemble probabilities. The operating threshold (\\(\\tau = 0.4400\\)) is applied to these ensemble probabilities to generate binary churn predictions for evaluation.",
    "crumbs": [
      "<span class='chapter-number'>5</span>  <span class='chapter-title'>Methodology</span>"
    ]
  },
  {
    "objectID": "report/05-experiments-results.html",
    "href": "report/05-experiments-results.html",
    "title": "6  Experiments and Results",
    "section": "",
    "text": "6.1 Overall Benchmark Summary\nAll models were evaluated on the held-out test set using threshold-independent ranking metrics (ROC-AUC and PR-AUC) alongside threshold-dependent decision metrics (precision, recall, and F1) at a fixed operating point. Table 6.1 reports the complete benchmark results, while Figure 6.1 summarizes the corresponding ROC and precision–recall (PR) curves. Because the churn rate in the test set is 28.8%, a non-informative baseline achieves a PR-AUC of approximately 0.2880; all trained models exceeded this baseline by a substantial margin.\nAcross the supervised model families, gradient-boosted decision trees delivered the strongest overall performance. Using the validation-set ROC-AUC criterion defined in Section 5.2, XGBoost was selected as the champion model (validation ROC-AUC = 0.6571, validation PR-AUC = 0.4359). On the test set, the champion XGBoost model achieved ROC-AUC = 0.6637 and PR-AUC = 0.4454. At the fixed operating threshold of 0.4400, it reached precision = 0.3670, recall = 0.7390, and F1 = 0.4904 (the corresponding confusion matrix is reported in Section 6.7). Notably, LightGBM achieved the highest test ROC-AUC among the single supervised models (test ROC-AUC = 0.6700, test PR-AUC = 0.4476), indicating that the two leading GBM approaches were closely matched on this dataset.\nThe remaining baselines formed clear lower tiers. Random Forest achieved ROC-AUC = 0.6534 and PR-AUC = 0.4271, while the logistic regression baseline yielded ROC-AUC = 0.5940 and PR-AUC = 0.3560. The deep learning models clustered in a narrow band: their test ROC-AUC values ranged from approximately 0.6440 to 0.6620, and test PR-AUC ranged from approximately 0.4080 to 0.4360. The best neural model by ranking metrics was a Wide & Deep variant with focal loss (test ROC-AUC = 0.6615, test PR-AUC = 0.4356). Under the fixed threshold of 0.4400, several focal-loss neural variants produced very few positive predictions, resulting in near-zero recall and F1 despite competitive AUC/PR-AUC; therefore, ranking metrics provide the fairest comparison at this aggregate level, while threshold-based outcomes are interpreted separately in Section 6.7.\nTable 6.1: Model leaderboard showing validation and test performance across all model families.\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nmodel\nval_roc_auc\nval_pr_auc\ntest_roc_auc\ntest_pr_auc\ntest_brier\ntest_f1\ntest_recall\ntest_precision\nthreshold\n\n\n\n\nXGBoost\n0.657146\n0.435915\n0.663681\n0.445441\n0.223314\n0.490413\n0.738953\n0.366982\n0.44\n\n\n05_nn_wide_deep_focal\n0.653063\n0.429338\n0.661457\n0.435612\n0.197007\n0.0174614\n0.00883753\n0.722222\n0.44\n\n\nLightGBM\n0.650239\n0.4311\n0.669973\n0.447632\n0.219983\n0.49688\n0.730795\n0.376401\n0.44\n\n\n05_nn_wide_and_deep\n0.649779\n0.418712\n0.649605\n0.42076\n0.2364\n0.482441\n0.7845\n0.348325\n0.44\n\n\nRandomForest\n0.649016\n0.421517\n0.653428\n0.428959\n0.228971\n0.480699\n0.804215\n0.342799\n0.44\n\n\n05_nn_embedding_focal_loss\n0.648666\n0.421622\n0.661197\n0.434519\n0.197579\n0\n0\n0\n0.44\n\n\n05_nn_dense_baseline\n0.646737\n0.425746\n0.65528\n0.421941\n0.234346\n0.489779\n0.806254\n0.35172\n0.44\n\n\n05_nn_embedding_baseline\n0.642545\n0.408729\n0.648944\n0.419172\n0.232875\n0.487391\n0.781781\n0.354064\n0.44\n\n\n05_nn_wide_and_deep_deeper\n0.642242\n0.414346\n0.647519\n0.411018\n0.232793\n0.482227\n0.765466\n0.351985\n0.44\n\n\n05_nn_embedding_strong_weight\n0.64173\n0.411761\n0.644192\n0.408117\n0.271837\n0.478787\n0.901428\n0.325959\n0.44\n\n\n06_teacher\n0.63778\n0.418322\n0.645056\n0.427104\n0.193299\n0.262616\n0.173351\n0.541401\n0.44\n\n\nLogistic\n0.588286\n0.347494\n0.593987\n0.355974\n0.24378\n0.456037\n0.840925\n0.312848\n0.44\nFigure 6.1 visually corroborates these findings. The ROC curves for the leading models (LightGBM and XGBoost) lie consistently above the other model families across most false-positive rates, and the PR curves show a clear separation from the prevalence baseline (approximately 0.2880), with the top models maintaining higher precision at comparable recall. In addition to single-model benchmarks, heterogeneous ensemble experiments (Section 6.6) produced performance comparable to the best GBMs, with the best ensembles reaching approximately 0.6690 test ROC-AUC and 0.4490 test PR-AUC, representing a modest lift relative to the champion XGBoost model.\nFigure 6.1: ROC and Precision–Recall curves comparing all model families on the test set.\nNote: Models were selected based on Validation ROC-AUC (as shown in the leaderboard in appendix). The plots below display performance on the held-out Test Set to provide an unbiased evaluation of generalization. Small discrepancies between ranking and plotting order are expected.",
    "crumbs": [
      "<span class='chapter-number'>6</span>  <span class='chapter-title'>Experiments and Results</span>"
    ]
  },
  {
    "objectID": "report/05-experiments-results.html#sec-logit-baseline",
    "href": "report/05-experiments-results.html#sec-logit-baseline",
    "title": "6  Experiments and Results",
    "section": "6.2 Baseline Performance (Logistic Regression)",
    "text": "6.2 Baseline Performance (Logistic Regression)\nTo provide a simple and interpretable reference point, a regularized logistic regression model was first evaluated as the supervised baseline. On the held-out test set, the baseline achieved a ROC-AUC of 0.5940 and a PR-AUC (average precision) of 0.3560, indicating modest but meaningful discrimination despite the class imbalance (Table 6.2). For context, the test-set churn prevalence is 0.2880, so the baseline PR-AUC is clearly above the prevalence level expected from random ranking; this relationship is also visible in the precision–recall curve where the model stays above the horizontal prevalence line across a wide recall range (Section A.3.3).\n\n\n\n\nTable 6.2: Logistic regression baseline performance metrics on validation and test sets.\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nsplit\nthreshold\nroc_auc\npr_auc\nbrier_score\nprecision\nrecall\nf1_score\ntrue_negatives\nfalse_positives\nfalse_negatives\ntrue_positives\n\n\n\n\nTrain\n0.44\n0.593797\n0.354992\n0.24399\n0.314526\n0.847396\n0.458771\n7333\n21735\n1796\n9973\n\n\nValidation\n0.44\n0.588286\n0.347494\n0.244917\n0.311646\n0.836846\n0.45416\n915\n2719\n240\n1231\n\n\nTest\n0.44\n0.593987\n0.355974\n0.24378\n0.312848\n0.840925\n0.456037\n917\n2717\n234\n1237\n\n\n\n\n\n\n\n\nThe baseline exhibited stable performance across data splits. On the validation set, the model reached ROC-AUC = 0.5880 and PR-AUC = 0.3480, closely matching test-set results and providing a consistent lower-bound benchmark for subsequent model families. In addition to ranking metrics, decision-level performance is reported at the operating threshold used for downstream comparisons (threshold = 0.4400). At this threshold, the test-set confusion matrix contained 917 true negatives, 2,717 false positives, 234 false negatives, and 1,237 true positives, corresponding to recall = 0.8410, precision = 0.3130, and F1 = 0.4560. Detailed ROC curves, PR curves, and confusion matrices for the logistic regression baseline are provided in Section A.3.3.\nFinally, probability accuracy was summarized via the Brier score, which was 0.2440 on the test set; the corresponding reliability diagram provides a visual check of calibration quality (Figure 6.2). Overall, this baseline establishes a conservative performance floor against which the gains from nonlinear models (tree/GBM, neural networks) and ensembling can be quantified.\n\n\n\n\n\n\n\n\nFigure 6.2: Calibration (reliability) diagram for logistic regression with Brier score annotation.",
    "crumbs": [
      "<span class='chapter-number'>6</span>  <span class='chapter-title'>Experiments and Results</span>"
    ]
  },
  {
    "objectID": "report/05-experiments-results.html#sec-tree-gbm-results",
    "href": "report/05-experiments-results.html#sec-tree-gbm-results",
    "title": "6  Experiments and Results",
    "section": "6.3 Tree and GBM Results",
    "text": "6.3 Tree and GBM Results\nAs a first step beyond the linear baseline, two tree-based benchmarks were evaluated: a single decision tree and a random forest. Table 6.3 reports ROC-AUC and PR-AUC on both validation and test sets, and Figure 6.3 visualizes the corresponding ROC and PR curves on the test set.\n\n\n\n\nTable 6.3: Performance summary of tree-based models on validation and test sets.\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nmodel\nsplit\nroc_auc\npr_auc\nbrier_score\nthreshold\nprecision\nrecall\nf1_score\ntrue_negatives\nfalse_positives\nfalse_negatives\ntrue_positives\n\n\n\n\ndt_baseline\nval\n0.631703\n0.403643\n0.238156\n0.5\n0.370782\n0.590075\n0.455404\n2161\n1473\n603\n868\n\n\ndt_baseline\ntest\n0.625077\n0.393425\n0.238209\n0.5\n0.367982\n0.57036\n0.447347\n2193\n1441\n632\n839\n\n\nrf_baseline\nval\n0.649016\n0.421517\n0.231366\n0.5\n0.365991\n0.662814\n0.471584\n1945\n1689\n496\n975\n\n\nrf_baseline\ntest\n0.653428\n0.428959\n0.228971\n0.5\n0.373933\n0.655337\n0.476167\n2020\n1614\n507\n964\n\n\nxgb_baseline\nval\n0.657146\n0.435915\n0.226199\n0.5\n0.385664\n0.596193\n0.468358\n2237\n1397\n594\n877\n\n\nxgb_baseline\ntest\n0.663681\n0.445441\n0.223314\n0.5\n0.39354\n0.588035\n0.471518\n2301\n1333\n606\n865\n\n\nxgb_optuna\nval\n0.657618\n0.437372\n0.23188\n0.5\n0.382098\n0.658736\n0.483654\n2067\n1567\n502\n969\n\n\nxgb_optuna\ntest\n0.669062\n0.44394\n0.227888\n0.5\n0.390638\n0.658056\n0.490251\n2124\n1510\n503\n968\n\n\nlgbm_small\nval\n0.650239\n0.4311\n0.224585\n0.5\n0.388305\n0.577838\n0.464481\n2295\n1339\n621\n850\n\n\nlgbm_small\ntest\n0.669973\n0.447632\n0.219983\n0.5\n0.401504\n0.580557\n0.474708\n2361\n1273\n617\n854\n\n\n\n\n\n\n\n\nOn the held-out test set, the decision tree achieved ROC-AUC = 0.6250 and PR-AUC = 0.3930 (validation: ROC-AUC = 0.6320, PR-AUC = 0.4040), showing that even shallow, rule-like splits can capture churn-related interactions absent in a linear boundary. However, performance improved noticeably when variance was reduced through bagging. The random forest increased test-set discrimination to ROC-AUC = 0.6530 and PR-AUC = 0.4290 (validation: ROC-AUC = 0.6360, PR-AUC = 0.4120), and its PR curve dominates the single-tree baseline across most recall levels in Figure 6.3. These gains indicate that churn signals are distributed across multiple weak patterns, and an ensemble of diverse trees aggregates them more reliably than a single fitted tree. Importantly, the validation-to-test gaps for both DT and RF were small in magnitude for ROC-AUC and PR-AUC, suggesting that the improvements are not driven by a split-specific artifact. At the default decision threshold of 0.5000, both DT and RF yielded test-set F1 scores in the mid-0.4700 range (DT F1 = 0.4770, RF F1 = 0.4700), reinforcing that these baselines improve ranking but still leave substantial room for improvement in the precision–recall trade-off.\n\n\n\n\n\n\n\n\nFigure 6.3: ROC and Precision–Recall curves for tree/GBM models on the test set.\n\n\n\n\n\n\n6.3.1 Gradient-Boosted Decision Trees\nGradient-boosted decision trees delivered the strongest ranking performance among the classical ML families. As summarized in Table 6.3 and visible in Figure 6.3, the XGBoost baseline improved test discrimination to ROC-AUC = 0.6640 and PR-AUC = 0.4450 (validation: ROC-AUC = 0.6571, PR-AUC = 0.4359). Applying Optuna hyperparameter search to XGBoost produced a modest additional lift in ROC-AUC on the test set (0.6690 versus 0.6640) while maintaining a very similar PR-AUC (0.4440 versus 0.4450). LightGBM performed comparably and achieved the best overall ranking on the test set in this experiment (ROC-AUC = 0.6700, PR-AUC = 0.4480; validation PR-AUC = 0.4310). Relative to the logistic regression baseline (test PR-AUC = 0.3560), the best GBM models provide an absolute PR-AUC gain of roughly 0.0900, which is material given the 0.2880 churn prevalence.\nWhen evaluated at a common default operating point (threshold = 0.5000), the Optuna-tuned XGBoost achieved the highest F1 among the tree family (F1 = 0.4900), reflecting a better precision–recall trade-off than the DT/RF baselines under the same decision rule. Because deployment typically requires an explicit operating point, a recall-constrained threshold sweep was performed on the validation set for the tuned XGBoost model, which selected threshold = 0.4400 as the best F1-achieving rule under the recall constraint. Figure 6.4 highlights this operating point directly on the ROC and PR curves for the tuned model, making the ranking–decision connection explicit. In addition to higher AUC metrics, the boosted models also achieved slightly lower Brier scores (approximately 0.2200 on the test set), suggesting better probability calibration than the single-tree baselines even before explicit calibration methods are applied.\n\n\n\n\n\n\n\n\nFigure 6.4: Tuned XGBoost ROC/PR curves with the selected operating threshold (0.4400) highlighted on the test set.\n\n\n\n\n\n\n\n6.3.2 Hyperparameter Optimization Diagnostics\nThe Optuna diagnostics were inspected to verify that the hyperparameter search behaved sensibly and to summarize which settings drove performance. The optimization history shows rapid improvement in the early trials followed by a clear plateau, consistent with quickly locating a strong region of the hyperparameter space and then encountering diminishing returns. The parallel-coordinate visualization indicates that the better trials concentrate within a relatively narrow band of configurations, rather than appearing as isolated outliers, which reduces the likelihood that the selected model is the result of chance. The hyperparameter-importance summary further suggests that performance is most sensitive to the learning rate, while tree depth and regularization terms play a secondary role. This pattern aligns with the small empirical gap between the untuned and tuned XGBoost models: tuning improves ROC-AUC slightly, but the overall precision–recall ranking remains of similar magnitude. Full Optuna diagnostic plots, including the optimization history, parallel-coordinate visualization, and hyperparameter importance rankings, are provided in Section A.4.5.",
    "crumbs": [
      "<span class='chapter-number'>6</span>  <span class='chapter-title'>Experiments and Results</span>"
    ]
  },
  {
    "objectID": "report/05-experiments-results.html#sec-nn-results",
    "href": "report/05-experiments-results.html#sec-nn-results",
    "title": "6  Experiments and Results",
    "section": "6.4 Deep Learning Results (PyTorch)",
    "text": "6.4 Deep Learning Results (PyTorch)\nThis section reports neural network experiments implemented in PyTorch, focusing on (i) architecture comparisons and (ii) loss-function/imbalance handling ablations. Results are summarized in Table 6.4, with the best model’s test curves shown in Figure 6.5 and its training dynamics summarized in Figure 6.6. Additional training curves for alternative architectures (MLP baseline, embedding MLP, and deeper Wide & Deep variants) are provided in Section A.5.2 and Section A.5.3.\n\n\n\n\nTable 6.4: PyTorch neural network results (test metrics at validation-selected thresholds).\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nexperiment\ntrain_auc\nval_auc\ntest_auc\ntest_pr_auc\ntest_brier\nbest_threshold_val\ntest_precision_at_best_threshold\ntest_recall_at_best_threshold\ntest_f1_at_best_threshold\nbest_epoch\ntrain_time_sec\nloss_type\npos_weight\nfocal_alpha\nfocal_gamma\n\n\n\n\nwide_deep_focal\n0.685802\n0.653063\n0.661457\n0.435612\n0.197007\n0.31\n0.364389\n0.717879\n0.483406\n62\n52.0644\nfocal\n2.46988\n0.25\n2\n\n\nwide_and_deep\n0.698333\n0.649779\n0.649605\n0.42076\n0.2364\n0.46\n0.352218\n0.739633\n0.477193\n44\n41.8901\nbce_weighted\n2.46988\n0\n0\n\n\nembedding_focal_loss\n0.686024\n0.648666\n0.661197\n0.434519\n0.197579\n0.3\n0.342906\n0.811693\n0.482132\n49\n42.9464\nfocal\n2.46988\n0.25\n2\n\n\ndense_baseline\n0.677815\n0.646737\n0.65528\n0.421941\n0.234346\n0.48\n0.369023\n0.723997\n0.488868\n76\n48.8865\nbce_weighted\n2.46988\n0\n0\n\n\nembedding_baseline\n0.683021\n0.642545\n0.648944\n0.419172\n0.232875\n0.42\n0.346409\n0.816451\n0.486432\n33\n30.1891\nbce_weighted\n2.46988\n0\n0\n\n\nwide_and_deep_deeper\n0.695702\n0.642242\n0.647519\n0.411018\n0.232793\n0.46\n0.359181\n0.727396\n0.480899\n27\n35.4544\nbce_weighted\n2.46988\n0\n0\n\n\nembedding_strong_weight\n0.688739\n0.64173\n0.644192\n0.408117\n0.271837\n0.54\n0.354619\n0.743712\n0.480246\n38\n34.5281\nbce_weighted\n3.70482\n0\n0\n\n\n\n\n\n\n\n\n\n6.4.1 Architecture Comparison\nAcross the neural network candidates, the top-ranked models in terms of discrimination are the focal-loss variants: wide_deep_focal and embedding_focal_loss. On the test set, wide_deep_focal achieves ROC-AUC = 0.6610 and PR-AUC = 0.4360, narrowly outperforming the other NN configurations on ranking metrics. The embedding-only focal model is extremely close (ROC-AUC = 0.6610, PR-AUC = 0.4350), suggesting that learned categorical representations provide most of the gains, while the additional “wide” pathway mainly stabilizes decision performance rather than dramatically shifting AUC.\nIn contrast, the purely dense baseline (dense_baseline, without embedding-driven representation learning) trails in ranking quality (ROC-AUC = 0.6550, PR-AUC = 0.4220), and the weighted-BCE Wide & Deep (wide_and_deep) is lower still (ROC-AUC = 0.6500, PR-AUC = 0.4210). Increasing depth (wide_and_deep_deeper) does not help and slightly reduces test ranking metrics (ROC-AUC = 0.6480, PR-AUC = 0.4110), consistent with diminishing returns under the available signal and regularization constraints.\n\n\n\n\n\n\n\n\nFigure 6.5: Best neural model (Wide & Deep with Focal Loss) ROC/PR curves with selected operating threshold on the test set.\n\n\n\n\n\nDecision-level metrics (computed at the best validation-selected threshold per model) are broadly similar across the stronger NN candidates, with F1 concentrated around 0.4800–0.4900. For the best-ranking model wide_deep_focal, the chosen threshold is 0.3100, producing precision = 0.3640, recall = 0.7180, and F1 = 0.4830 on test. The embedding-only focal model selects a similar threshold (0.3000) but shifts toward higher sensitivity (recall = 0.8120) at lower precision (0.3430), yielding F1 = 0.4820. Interestingly, the dense baseline attains the highest F1 among NN variants (F1 = 0.4890) but with weaker AUC/PR-AUC, illustrating that “best F1” can be achieved via a favorable threshold even when the underlying ranking quality is lower. Probability accuracy further differentiates the models: focal-loss networks show a markedly improved Brier score of approximately 0.1970, compared with 0.2330–0.2360 for weighted-BCE variants, indicating better-calibrated probabilities in addition to stronger discrimination.\n\n\n6.4.2 Loss/Imbalance Ablation\nFocal Loss provides the clearest performance lift in this NN setup. Holding the general architecture fixed, switching Wide & Deep from weighted BCE (wide_and_deep) to focal (wide_deep_focal) improves ROC-AUC from 0.6500 to 0.6610 and PR-AUC from 0.4210 to 0.4360, while substantially lowering the Brier score (0.2360 to 0.1970). A similar pattern holds for embedding-only models: ROC-AUC improves from 0.6490 to 0.6610 and PR-AUC from 0.4190 to 0.4350 when moving from embedding_baseline to embedding_focal_loss. In contrast, aggressively increasing the positive-class weight (embedding_strong_weight) under weighted BCE reduces ranking quality (ROC-AUC = 0.6440, PR-AUC = 0.4080), suggesting that extreme reweighting can over-correct and degrade generalization. Overall, focal loss appears to provide a more effective imbalance strategy by focusing learning on harder examples rather than uniformly amplifying the minority class.\nThe focal loss formulation down-weights easy examples and focuses training on hard-to-classify instances:\n\\[\n\\mathcal{L}_{\\text{focal}} = -\\alpha_t (1 - p_t)^\\gamma \\log(p_t)\n\\]\nwhere \\(p_t\\) is the model’s estimated probability for the true class, \\(\\alpha_t\\) is a class-balancing weight, and \\(\\gamma \\geq 0\\) is the focusing parameter. When \\(\\gamma = 0\\), focal loss reduces to standard cross-entropy. Higher values of \\(\\gamma\\) increase the relative loss for hard examples (where \\(p_t\\) is small), encouraging the model to focus on difficult cases.\n\n\n6.4.3 Training Dynamics\n\n\n\n\n\n\n\n\nFigure 6.6: Training versus validation loss and AUC curves for the best neural network model (Wide & Deep with Focal Loss).\n\n\n\n\n\nTraining curves (Figure 6.6) show that the best focal model continues to reduce training loss while validation loss plateaus, and validation AUC stabilizes in the mid-0.6400 range after roughly the mid-training period; early stopping selects epoch 62 for wide_deep_focal. Across variants, deeper/wider configurations tend to widen the train–validation gap without improving validation AUC, consistent with the small benefit observed from the deeper Wide & Deep model. Additional learning curves for alternative architectures and imbalance settings are provided in Section A.5.2 (best model training dynamics) and Section A.5.3 (alternative architecture comparisons).",
    "crumbs": [
      "<span class='chapter-number'>6</span>  <span class='chapter-title'>Experiments and Results</span>"
    ]
  },
  {
    "objectID": "report/05-experiments-results.html#sec-autoencoder-results",
    "href": "report/05-experiments-results.html#sec-autoencoder-results",
    "title": "6  Experiments and Results",
    "section": "6.5 Unsupervised/Semi-Supervised Extension (Autoencoder + Pseudo-Label)",
    "text": "6.5 Unsupervised/Semi-Supervised Extension (Autoencoder + Pseudo-Label)\nTo explore whether unlabeled data can improve churn prediction, a denoising autoencoder (DAE) was trained on the feature space and evaluated for two downstream uses: (i) using the learned latent representation as features, and (ii) using an Optuna-tuned XGBoost teacher to generate pseudo-labels on a real unlabeled holdout set.\n\n6.5.1 DAE Convergence and Reconstruction\nThe DAE showed stable optimization behavior with early stopping at epoch 16 (Figure 6.7). Training reconstruction loss dropped quickly in the first few epochs and then plateaued, while the validation curve remained flat after the best epoch, indicating the encoder learned a consistent low-dimensional structure without late-epoch divergence. Note that validation loss is lower than training loss, which is expected here because noise is injected during training (denoising objective), making the training reconstruction task harder than the clean validation reconstruction.\n\n\n\n\n\n\n\n\nFigure 6.7: DAE reconstruction loss curves with early-stopping epoch marked.\n\n\n\n\n\n\n\n6.5.2 Pseudo-Label Confidence and Coverage\nUsing the XGBoost teacher trained on labeled data, the real unlabeled holdout was scored and a conservative high-confidence rule was applied (assign churn if \\(p \\geq 0.9500\\), non-churn if \\(p \\leq 0.0500\\)). Figure 6.8 shows the teacher’s probability distribution concentrates in the mid-range (roughly \\(p \\approx 0.1500\\)–\\(0.4500\\)), producing almost no extreme probabilities. As a result, pseudo-label coverage was effectively zero: only 2 samples met the low-confidence (non-churn) criterion and 0 samples met the high-confidence (churn) criterion, with 19,998/20,000 (approximately 100%) remaining “uncertain.” This is a key practical finding: with strict thresholds designed to minimize label noise, the teacher did not provide enough pseudo-labeled examples to form a meaningful semi-supervised training signal on this dataset split.\n\n\n\n\n\n\n\n\nFigure 6.8: Teacher probability distribution on the unlabeled holdout and pseudo-label coverage at thresholds 0.0500/0.9500.\n\n\n\n\n\n\n\n6.5.3 Downstream Impact (Ablation Summary)\nTable 6.5 reports the predictive utility of the learned latent features via a controlled ablation. Using the same evaluation protocol, the baseline (X only) achieved ROC-AUC = 0.6465 and PR-AUC = 0.4254. Using latent-only (Z only) reduced performance to ROC-AUC = 0.6252 and PR-AUC = 0.3872, implying the compressed representation retains substantial but not complete churn-relevant information. Finally, augmenting features (X+Z) essentially matched the baseline ranking metrics (ROC-AUC = 0.6451, PR-AUC = 0.4271), yielding only a marginal PR-AUC increase. Under tuned thresholds (selected on validation), baseline and augmented variants also delivered very similar operating-point metrics (F1 around 0.4800 with recall around 0.7300), consistent with the conclusion that latent features provide, at best, a small incremental signal in this configuration.\n\n\n\n\nTable 6.5: Downstream impact of DAE latent features (baseline vs. latent-only vs. augmented).\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nversion\nsplit\nn_features\nroc_auc\npr_auc\nbrier_score\nthreshold_fixed\nprecision_fixed\nrecall_fixed\nf1_fixed\nthreshold_tuned\nprecision_tuned\nrecall_tuned\nf1_tuned\n\n\n\n\nBaseline (X only)\ntest\n11\n0.646541\n0.425392\n0.19332\n0.44\n0.530303\n0.14276\n0.22496\n0.25\n0.353871\n0.730116\n0.476698\n\n\nLatent-only (Z only)\ntest\n32\n0.625202\n0.387228\n0.197288\n0.44\n0.470149\n0.085656\n0.144911\n0.24\n0.339952\n0.768185\n0.471324\n\n\nAugmented (X + Z)\ntest\n43\n0.645056\n0.427104\n0.193299\n0.44\n0.541401\n0.173351\n0.262616\n0.25\n0.35947\n0.719918\n0.479511\n\n\n\n\n\n\n\n\nAdditional diagnostics, including latent t-SNE visualization (Figure A.16 in Section A.6.2) and reconstruction-error distribution plots (Figure A.15 in Section A.6.1), are provided to support the qualitative assessment of representation structure and error overlap between churn/non-churn groups.",
    "crumbs": [
      "<span class='chapter-number'>6</span>  <span class='chapter-title'>Experiments and Results</span>"
    ]
  },
  {
    "objectID": "report/05-experiments-results.html#sec-ensemble-results",
    "href": "report/05-experiments-results.html#sec-ensemble-results",
    "title": "6  Experiments and Results",
    "section": "6.6 Ensemble Results (Blending vs. OOF Stacking)",
    "text": "6.6 Ensemble Results (Blending vs. OOF Stacking)\nTo test whether combining heterogeneous learners could yield incremental gains beyond the best single model, several ensemble strategies were evaluated built on the same base predictors (Logistic Regression, XGBoost, LightGBM, Random Forest). Table 6.6 summarizes validation and test ranking performance.\n\n\n\n\nTable 6.6: Ensemble performance comparison on validation and test sets (ROC-AUC and PR-AUC).\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nmodel\nval_roc_auc\nval_pr_auc\ntest_roc_auc\ntest_pr_auc\nfixed_threshold\nval_best_threshold\ntest_best_threshold\n\n\n\n\nBlend_WeightedAUC\n0.658276\n0.434834\n0.668847\n0.448774\n0.44\n0.435955\n0.423938\n\n\nBlend_SimpleAvg\n0.657885\n0.432159\n0.668322\n0.447171\n0.44\n0.441047\n0.422707\n\n\nStacking_OOF\n0.658582\n0.436697\n0.667918\n0.448953\n0.44\n0.224024\n0.225712\n\n\nBlend_NNLS\n0.658656\n0.437131\n0.66724\n0.448761\n0.44\n0.422218\n0.417435\n\n\nBlend_RankAvg\n0.654898\n0.416895\n0.664951\n0.434002\n0.44\n2029\n1616.25\n\n\n\n\n\n\n\n\n\n6.6.1 Blending Results\nOverall, blending delivered the strongest and most stable results, with Blend_WeightedAUC achieving the highest test ROC-AUC (0.6688) and a test PR-AUC of 0.4488. A simple unweighted average (Blend_SimpleAvg) performed nearly identically (test ROC-AUC 0.6683; PR-AUC 0.4472), suggesting that most of the ensemble benefit comes from variance reduction rather than finely tuned weights. The NNLS-constrained blend (Blend_NNLS) also matched this band (test ROC-AUC 0.6672; PR-AUC 0.4488).\nCompared with the best single tree/GBM model from Section 6.3, these ensembles largely preserve discrimination while providing a small but consistent PR-AUC lift, which is most relevant under class imbalance. For operating-point comparability across all model families, the fixed threshold = 0.4400 is maintained for the main cross-section comparisons; tuned thresholds and blending weights for each ensemble are detailed in Section A.7.4 (Table A.15).\n\n\n6.6.2 OOF Stacking Results\nOOF stacking (Stacking_OOF) performed competitively but not decisively better than blending: it reached test ROC-AUC 0.6679 and the highest test PR-AUC 0.4490 among the listed ensembles. Notably, its optimal threshold is much lower (approximately 0.2260) than the blending variants (approximately 0.4200), which is consistent with the meta-learner producing probabilities on a different scale (or less calibrated) even when ranking quality is similar. This makes stacking attractive for ranking, but it requires explicit threshold re-selection or calibration if deployed as a decision rule (see Section A.9.4 and Table A.18 for threshold sensitivity analysis). In this dataset, the near-tie between stacking and blending suggests that the relationship between base model scores is close to linear, so a learned meta-combination provides limited additional upside over robust averaging.\nThe stacking meta-learner combines base model predictions \\(z(x)\\) using logistic regression:\n\\[\nz(x) = \\big[\\hat{p}_{1}(x), \\hat{p}_{2}(x), \\ldots, \\hat{p}_{M}(x)\\big]\n\\]\n\\[\n\\hat{p}_{\\text{stack}}(x) = \\sigma(\\beta^\\top z(x) + b)\n\\]\nwhere \\(\\sigma\\) is the sigmoid function and the coefficients \\(\\beta\\) are learned from out-of-fold predictions to prevent leakage between base and meta levels.\n\n\n6.6.3 Ensemble Diversity Analysis\nThe effectiveness of ensembling is supported by diversity evidence from the validation-set prediction correlations (Figure 6.9). Tree-based learners are highly correlated with each other (e.g., XGBoost–LightGBM = 0.9310, XGBoost–RF = 0.8790, LightGBM–RF = 0.8690), indicating they capture similar nonlinear structure. In contrast, the Logistic Regression predictions correlate much less with the tree models (approximately 0.4700 with each), implying complementary error patterns. This explains why including a weaker linear baseline can still improve an ensemble: it contributes orthogonal signal that slightly improves precision–recall ranking. Finally, learned blend weights show XGBoost dominates the NNLS solution, while LightGBM and Random Forest contribute meaningful secondary weight and Logistic Regression remains small but non-zero, consistent with the correlation-based diversity story.\n\n\n\n\n\n\n\n\nFigure 6.9: Correlation matrix of base model predictions on the validation set (lower correlation indicates greater diversity).",
    "crumbs": [
      "<span class='chapter-number'>6</span>  <span class='chapter-title'>Experiments and Results</span>"
    ]
  },
  {
    "objectID": "report/05-experiments-results.html#sec-operating-point",
    "href": "report/05-experiments-results.html#sec-operating-point",
    "title": "6  Experiments and Results",
    "section": "6.7 Operating Point Analysis (Threshold Policy in Practice)",
    "text": "6.7 Operating Point Analysis (Threshold Policy in Practice)\nTo turn probabilistic churn scores into an actionable retention list, this study adopts a single, fixed operating threshold across all candidate models. Concretely, \\(\\tau = 0.4400\\) (chosen on the validation set during the main tree-based model selection) serves as the production policy: any customer with predicted churn probability \\(\\hat{p}(\\text{churn}) \\geq \\tau\\) is flagged for intervention. This fixed-threshold policy is intentionally conservative from a reporting standpoint: it avoids per-model “best threshold” cherry-picking and reflects how real teams often deploy one stable rule tied to capacity and budget. Full threshold sweeps showing precision–recall trade-offs across \\(\\tau\\) are provided in Section A.9.3 (Table A.17).\nUnder \\(\\tau = 0.4400\\), the best balance of precision and recall is achieved by LightGBM with Precision = 0.3760, Recall = 0.7310, and F1 = 0.4970 (Table 6.7). XGBoost is extremely close (0.3670/0.7390/0.4900), indicating that the two GBMs behave similarly under the same decision rule. The dense NN baseline is more aggressive at this threshold, reaching higher recall (0.8060) but lower precision (0.3520), while Logistic Regression pushes recall even higher (0.8410) at the cost of precision (0.3130). These patterns match the expected operational trade-off: higher recall reduces missed churners but increases the number of false positives competing for retention resources.\n\n\n\n\nTable 6.7: Operating point comparison across models at fixed threshold τ = 0.4400.\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nmodel\nval_roc_auc\nval_pr_auc\ntest_roc_auc\ntest_pr_auc\ntest_brier\ntest_f1\ntest_recall\ntest_precision\nthreshold\n\n\n\n\nXGBoost\n0.657146\n0.435915\n0.663681\n0.445441\n0.223314\n0.490413\n0.738953\n0.366982\n0.44\n\n\n05_nn_wide_deep_focal\n0.653063\n0.429338\n0.661457\n0.435612\n0.197007\n0.0174614\n0.00883753\n0.722222\n0.44\n\n\nLightGBM\n0.650239\n0.4311\n0.669973\n0.447632\n0.219983\n0.49688\n0.730795\n0.376401\n0.44\n\n\n05_nn_wide_and_deep\n0.649779\n0.418712\n0.649605\n0.42076\n0.2364\n0.482441\n0.7845\n0.348325\n0.44\n\n\nRandomForest\n0.649016\n0.421517\n0.653428\n0.428959\n0.228971\n0.480699\n0.804215\n0.342799\n0.44\n\n\n05_nn_embedding_focal_loss\n0.648666\n0.421622\n0.661197\n0.434519\n0.197579\n0\n0\n0\n0.44\n\n\n05_nn_dense_baseline\n0.646737\n0.425746\n0.65528\n0.421941\n0.234346\n0.489779\n0.806254\n0.35172\n0.44\n\n\n05_nn_embedding_baseline\n0.642545\n0.408729\n0.648944\n0.419172\n0.232875\n0.487391\n0.781781\n0.354064\n0.44\n\n\n05_nn_wide_and_deep_deeper\n0.642242\n0.414346\n0.647519\n0.411018\n0.232793\n0.482227\n0.765466\n0.351985\n0.44\n\n\n05_nn_embedding_strong_weight\n0.64173\n0.411761\n0.644192\n0.408117\n0.271837\n0.478787\n0.901428\n0.325959\n0.44\n\n\n06_teacher\n0.63778\n0.418322\n0.645056\n0.427104\n0.193299\n0.262616\n0.173351\n0.541401\n0.44\n\n\nLogistic\n0.588286\n0.347494\n0.593987\n0.355974\n0.24378\n0.456037\n0.840925\n0.312848\n0.44\n\n\n\n\n\n\n\n\n\n6.7.1 Confusion Matrix Analysis\nThe confusion matrix for XGBoost at \\(\\tau = 0.4400\\) illustrates the scale of this trade-off on the test set (n = 5,105): TP = 1,087, FP = 1,875, FN = 384, TN = 1,759 (Figure 6.10). This means the model correctly identifies 1,087 churners while flagging 1,875 non-churners as potential churners. In a retention campaign context, this translates to contacting approximately 2,962 customers (TP + FP) to capture 73.9% of actual churners, with a precision of 36.7%.\n\n\n\n\n\n\n\n\nFigure 6.10: Confusion matrix for XGBoost at the operating threshold τ = 0.4400 on the test set.\n\n\n\n\n\nFinally, models whose probability scales differ substantially (e.g., the DAE teacher and some focal-loss networks) can become overly conservative under a shared \\(\\tau\\) (high precision but very low recall), suggesting that calibration or model-specific thresholds are important alternatives. Detailed calibration metrics and model-specific optimal thresholds are provided in Section A.9.4 (Table A.18) and Section A.9.4 for calibration details.",
    "crumbs": [
      "<span class='chapter-number'>6</span>  <span class='chapter-title'>Experiments and Results</span>"
    ]
  },
  {
    "objectID": "report/05-experiments-results.html#sec-results-summary",
    "href": "report/05-experiments-results.html#sec-results-summary",
    "title": "6  Experiments and Results",
    "section": "6.8 Summary of Key Findings",
    "text": "6.8 Summary of Key Findings\nThe experimental results across all model families can be summarized as follows:\n\nGradient-boosted trees outperform other single-model families. XGBoost and LightGBM achieved the highest test ROC-AUC (0.6640–0.6700) and PR-AUC (0.4440–0.4480), providing meaningful improvements over the logistic regression baseline (ROC-AUC = 0.5940, PR-AUC = 0.3560).\nNeural networks with focal loss are competitive but do not surpass GBMs. The best Wide & Deep model with focal loss achieved test ROC-AUC = 0.6610 and PR-AUC = 0.4360, demonstrating that deep learning approaches can capture churn patterns effectively on tabular data. Focal loss consistently outperformed weighted BCE for handling class imbalance.\nAutoencoder-based representation learning provides limited incremental value. Latent features from a denoising autoencoder did not meaningfully improve downstream classification beyond the original feature set. Pseudo-labeling with conservative confidence thresholds yielded insufficient coverage for semi-supervised learning.\nEnsemble methods provide modest but consistent gains. Blending and OOF stacking reached approximately 0.6690 test ROC-AUC and 0.4490 test PR-AUC, representing a small lift over the best single model. The benefits derive primarily from diversity between linear (logistic regression) and nonlinear (tree-based) base learners.\nOperating threshold selection is critical for deployment. At the fixed threshold \\(\\tau = 0.4400\\), the champion models achieve recall around 73–74% with precision around 37%, suitable for targeted retention campaigns. Different models require different optimal thresholds, highlighting the importance of calibration in production.\n\nThese findings inform the discussion of model interpretation, business implications, and deployment considerations in the following chapter.",
    "crumbs": [
      "<span class='chapter-number'>6</span>  <span class='chapter-title'>Experiments and Results</span>"
    ]
  },
  {
    "objectID": "report/06-discussion.html",
    "href": "report/06-discussion.html",
    "title": "7  Discussion – Interpretation and Error Analysis",
    "section": "",
    "text": "7.1 Global Interpretability and Feature Insights\nThe champion model’s global interpretability was assessed using SHAP values to quantify which variables most strongly drive churn predictions. The SHAP summary plot (Figure 7.1) shows that churn risk is primarily explained by a mix of device lifecycle timing, engagement, tenure, and service quality signals. The most influential predictors include CurrentEquipmentDays, MonthlyMinutes, MonthsInService, PercChangeMinutes, and DroppedBlockedCalls, followed by features such as OverageMinutes, AgeHH1, and credit-band indicators (e.g., CreditRating_5-Low). Importantly, these drivers align with plausible business mechanisms rather than suspicious identifiers, which provides a sanity check that the model is learning actionable behavioral patterns.\nSeveral of the top features map directly to retention levers. CurrentEquipmentDays captures how long a customer has been on their current handset/equipment; higher values consistently push SHAP contributions upward, indicating higher churn risk. This supports a device-lifecycle interpretation: customers who have gone a long time without an equipment refresh are closer to a decision point and may be more receptive to switching offers. MonthlyMinutes is a direct engagement proxy: low usage contributes positively to churn risk, consistent with disengagement or declining relevance of the service. MonthsInService reflects tenure and loyalty; in general, longer-tenured customers exhibit lower predicted risk, while newer customers are more vulnerable, representing an intuitive pattern for subscription churn. In addition, PercChangeMinutes provides a “trend-like” signal even within snapshot features: large negative usage changes (usage drops) are associated with higher risk, which is consistent with early churn warning signs. Finally, DroppedBlockedCalls and related service-quality variables increase churn propensity, reflecting that persistent call failures or connectivity issues can trigger dissatisfaction and switching behavior. Together, the feature set suggests that the model is prioritizing variables that represent customer experience and engagement, which are exactly the kinds of signals operators can intervene on.",
    "crumbs": [
      "<span class='chapter-number'>7</span>  <span class='chapter-title'>Discussion – Interpretation and Error Analysis</span>"
    ]
  },
  {
    "objectID": "report/06-discussion.html#sec-global-interpretability",
    "href": "report/06-discussion.html#sec-global-interpretability",
    "title": "7  Discussion – Interpretation and Error Analysis",
    "section": "",
    "text": "Figure 7.1: SHAP summary plot (beeswarm) showing top features driving churn predictions in the champion XGBoost model. Feature values are colored by magnitude (high = red, low = blue).\n\n\n\n\n7.1.1 Non-Linear Feature Effects\nBeyond ranking features, SHAP dependence plots reveal non-linear effects that would be difficult to summarize with a single coefficient. For CurrentEquipmentDays (Figure 7.2), churn risk rises as equipment tenure increases, but the pattern is not perfectly linear; the SHAP contributions show clustered regimes where risk increases more sharply after certain ranges, consistent with threshold-like upgrade cycles rather than a smooth monotonic slope. The interaction coloring by HandsetRefurbished_Yes suggests that customers with refurbished handsets (pink points) tend to cluster at lower equipment days but show similar SHAP patterns, while those with non-refurbished devices (blue points) span the full range of equipment tenure.\n\n\n\n\n\n\n\n\nFigure 7.2: SHAP dependence plot for CurrentEquipmentDays showing increasing churn risk with longer equipment tenure.\n\n\n\n\n\nFor MonthlyMinutes (Figure 7.3), the relationship is strongly asymmetric: customers in the very low-usage region receive the largest positive SHAP values (highest churn risk), while higher usage generally reduces risk (negative SHAP contributions). This supports a practical interpretation that “silent disengagement” is a major churn pathway in this dataset, and it also clarifies that the dominant signal is low engagement, not necessarily “very high usage.” The interaction coloring by PercChangeMinutes reveals an additional layer: among low-usage customers, those with declining usage trends (blue/purple points indicating negative percent change) tend to have even higher SHAP values, reinforcing the disengagement narrative.\n\n\n\n\n\n\n\n\nFigure 7.3: SHAP dependence plot for MonthlyMinutes showing asymmetric risk pattern; low usage strongly increases churn risk.\n\n\n\n\n\n\n\n7.1.2 Feature Interaction Insights\nThe SHAP interaction analysis reveals that the top feature pairs with meaningful interaction effects are:\n\nCurrentEquipmentDays × MonthsInService: Customers with both old equipment and short tenure show elevated churn risk beyond what either feature alone would predict, suggesting a “new customer with outdated device” risk profile.\nMonthlyMinutes × PercChangeMinutes: The combination of low current usage and declining trend creates a compounding effect, with the interaction term contributing additional risk beyond the main effects.\nOverageMinutes × MonthlyRevenue: High overage charges relative to base revenue may signal billing dissatisfaction, with the interaction capturing customers who feel “nickel-and-dimed.”\n\nThese interaction patterns align with domain intuition and suggest that the model is capturing meaningful behavioral combinations rather than spurious correlations.\nOverall, the global interpretability results point to a coherent churn story: customers are most at risk when they show aging equipment tenure, low or declining engagement, shorter tenure, and service reliability issues. This set of drivers naturally translates into operational playbooks (e.g., upgrade offers for long equipment tenure, proactive outreach for sharp usage drops, and targeted service remediation for customers experiencing repeated call failures).",
    "crumbs": [
      "<span class='chapter-number'>7</span>  <span class='chapter-title'>Discussion – Interpretation and Error Analysis</span>"
    ]
  },
  {
    "objectID": "report/06-discussion.html#sec-error-analysis",
    "href": "report/06-discussion.html#sec-error-analysis",
    "title": "7  Discussion – Interpretation and Error Analysis",
    "section": "7.2 Error Analysis",
    "text": "7.2 Error Analysis\nWhile overall performance was strong, revisiting the confusion matrix (Figure 6.10) provides deeper insight into the types of residual errors. The chosen operating point (threshold = 0.4400) favored recall (sensitivity to churners) by using a slightly lower decision threshold, which resulted in a higher number of true positives (churners correctly identified) at the expense of some false positives.\nAt this threshold, the model achieves:\n\nTrue Positives (TP): 1,087 churners correctly identified\nFalse Positives (FP): 1,875 non-churners incorrectly flagged as at-risk\nFalse Negatives (FN): 384 churners missed by the model\nTrue Negatives (TN): 1,759 non-churners correctly identified as stable\n\nThis translates to a recall of 73.9% (1,087 / 1,471 actual churners) and a precision of 36.7% (1,087 / 2,962 predicted churners). The model captures nearly three-quarters of actual churners, but inevitably some churners are missed.\n\n7.2.1 False Negatives: The “Silent Churners”\nThese false negatives, specifically churners the model failed to flag, are especially concerning in a churn context because each missed churner is a lost opportunity for intervention. On inspection, many of these missed cases appear to be what might be called “silent churners”: customers who did churn but exhibited no strong warning signs in the available data. For example, some maintained steady usage until the end or had decent engagement metrics, giving the model little indication of their intent to leave.\nOne hypothesis is that these silent churners might be influenced by factors outside the dataset’s scope: perhaps a competitor offered them an attractive deal that cannot be observed internally, or life events (like moving to a non-serviced area or a job change) prompted their departure. Because the model was trained only on internal telecom data (billing, usage, support history, etc.), such external or unobserved triggers would lead to false negatives. Fundamentally, the model is limited to detecting churners exhibiting observable behavioral antecedents within the available feature space. Silent churners highlight the limits of the features, and they may have churned for reasons the data did not capture, remaining inscrutable to the prediction system.\n\n\n7.2.2 False Positives: The Cost of Caution\nOn the other side, the model produces 1,875 false positives, defined as customers flagged as high risk who did not actually churn. False positives have a direct business cost: if retention teams act on these predictions, they might spend retention budget (such as offering a discount or special incentive) on customers who would have stayed anyway.\nHowever, in a churn management context, some level of false positives is usually accepted in exchange for catching more true churners. The trade-off between false positives and false negatives is essentially one of precision vs. recall. The chosen threshold reflects a business decision to prioritize catching churners (higher recall) even if it means a moderate number of false alarms. This is because the cost of a missed churn (lost revenue and customer lifetime value) is typically much greater than the cost of an unnecessary retention offer.\nThat said, false positives must be managed by calibrating the intervention level to customer value (to avoid overspending on low-value customers who were not going to churn). Regular monitoring of the confusion matrix and performance metrics is important post-deployment: if false positives creep too high, the model’s threshold or features may need adjustment. Overall, the error analysis underlines that while the model significantly improves targeting, it is not perfect; understanding the errors allows refinement of strategies and clear communication of expected performance (not 100% recall) to stakeholders.",
    "crumbs": [
      "<span class='chapter-number'>7</span>  <span class='chapter-title'>Discussion – Interpretation and Error Analysis</span>"
    ]
  },
  {
    "objectID": "report/06-discussion.html#sec-autoencoder-interpretation",
    "href": "report/06-discussion.html#sec-autoencoder-interpretation",
    "title": "7  Discussion – Interpretation and Error Analysis",
    "section": "7.3 Interpreting the Autoencoder: Sanity Checks and Limited Gains",
    "text": "7.3 Interpreting the Autoencoder: Sanity Checks and Limited Gains\nBecause autoencoders are inherently unsupervised, sanity checks were added to verify that the model learned meaningful structure rather than noise. First, the latent vectors were visualized using t-SNE and colored by churn labels only for diagnostic purposes (labels were not used in training). Second, reconstruction error distributions were compared by label, revealing a small but consistent difference: churners had slightly higher reconstruction error (0.1639) than non-churners (0.1608), suggesting that churners may exhibit behavioral patterns that are harder to reconstruct with a compact representation.\nDespite these qualitative signals, the DAE did not improve predictive metrics when appended to the supervised feature set. As reported in Section 6.5, using the baseline features alone (X only) achieved ROC-AUC = 0.6465 and PR-AUC = 0.4254, while augmenting with latent features (X+Z) yielded ROC-AUC = 0.6451 and PR-AUC = 0.4271, demonstrating essentially no meaningful improvement. Using latent features alone (Z only) substantially degraded performance (ROC-AUC = 0.6252, PR-AUC = 0.3872), confirming that the compressed representation retains substantial but not complete churn-relevant information.\nA plausible explanation is that the baseline engineered feature set already captures most of the signal that is linearly or semi-linearly recoverable for churn prediction, so the bottleneck representation primarily compresses rather than contributes new predictive information. The autoencoder’s reconstruction objective optimizes for capturing variance in the feature space, but the variance-explaining directions may not align with the churn-discriminative directions. In addition, the pseudo-labeling extension produced only 2 confident pseudo-labels at the strict 0.0500/0.9500 threshold, which is far too few to materially change the decision boundary.\n\n\n\n\n\n\nNotePractical Takeaway for an ML Engineering Audience\n\n\n\nEven when predictive lift is limited, the unsupervised module is still valuable as an engineering capability: it provides a reusable representation learning component, a reproducible artifact set (checkpoint/history/latents), and a clear interface for future semi-supervised variants if more unlabeled data or better confidence calibration becomes available.",
    "crumbs": [
      "<span class='chapter-number'>7</span>  <span class='chapter-title'>Discussion – Interpretation and Error Analysis</span>"
    ]
  },
  {
    "objectID": "report/06-discussion.html#sec-business-impact",
    "href": "report/06-discussion.html#sec-business-impact",
    "title": "7  Discussion – Interpretation and Error Analysis",
    "section": "7.4 Managerial Implications and Business Impact",
    "text": "7.4 Managerial Implications and Business Impact\nAccurately predicting churn is only valuable if it drives better business decisions. The model’s predictions enable targeted retention efforts, which are expected to yield substantial financial benefits. To illustrate the potential impact, a simple ROI simulation was conducted for a model-driven retention campaign with the following realistic business parameters:\n\n\n\nTable 7.1: Business parameters for ROI simulation.\n\n\n\n\n\n\n\n\n\n\nParameter\nValue\nDescription\n\n\n\n\nIntervention Cost\n$10 per customer\nCost of a special offer or support call\n\n\nCustomer LTV\n$200\nRevenue profit if customer is retained long-term\n\n\nIntervention Success Rate\n20%\nProbability that outreach convinces at-risk customer to stay\n\n\n\n\n\n\nUnder these assumptions, two strategies were compared: using the model to target the top-risk customers versus naive random targeting of customers.\n\n7.4.1 Model-Driven Targeting\nImagine a telecom subscriber base where about 10% are truly going to churn in the near term. If the churn model is used to pick a top-decile segment of 1,000 customers most likely to churn, that segment will be enriched with real churners; for example, the model’s precision for the top decile is 30%, meaning about 300 of those 1,000 are actual impending churners (a threefold improvement over the base rate).\nThe retention campaign would contact all 1,000, costing $10,000. Out of the approximately 300 true churners in this group, at a 20% save rate, approximately 60 customers who would have left would be successfully retained. Those 60 saved customers represent an avoided revenue loss of approximately $12,000 (60 × $200 LTV each). Subtracting the $10,000 cost, the net gain is about $2,000 for this intervention.\n\n\n7.4.2 Random Targeting (Baseline)\nCompare this to random targeting of 1,000 customers. With no model, the expected churners in any random group of 1,000 (at 10% base churn) would be about 100. Using the same 20% success rate, only 20 customers would be saved (recovering $4,000 of value) while still spending $10,000, resulting in a net loss of $6,000.\nThis stark difference in outcomes (a positive ROI with model-driven targeting vs. a negative ROI with blind targeting) demonstrates the business value of the churn model. Even if the exact numbers are adjusted for actual churn rates and costs, the direction is clear: focusing retention efforts on the highest-risk customers yields a significantly higher return on investment (ROI).\n\n\n7.4.3 Tiered Retention Playbook\nBeyond the quantitative ROI, the model enables a more nuanced, efficient retention strategy. A tiered retention playbook is proposed that segments customers by both their churn risk and their value to the company, leveraging the model’s risk scores in conjunction with customer LTV or revenue metrics:\n1. High Risk / High Value: These customers (for example, a long-tenured subscriber on an expensive plan who the model flags as likely to churn) should receive premium service interventions. This might entail a personal phone call from a retention specialist, a bespoke loyalty offer, or other high-touch intervention. The cost is higher, but so is the payoff; losing a high-value customer hurts revenue significantly, so extra effort is justified to keep them.\n2. High Risk / Low Value: Customers predicted to churn who have a lower spend or profit profile should still be targeted, but in a more cost-effective way. For this tier, automated outreach is appropriate; for instance, a personalized SMS or email with a special discount, or an in-app notification offering a modest incentive. These interventions are low-cost and can be scaled easily. The attempt to save these customers is still made, but the investment is tailored to be commensurate with their value.\n3. Low Risk (Any Value): Customers with a low predicted churn risk are generally left alone (no proactive retention action). This “do nothing” approach for low-risk segments conserves budget and avoids unnecessary contact. Importantly, refraining from intervening also prevents the “sleeping dog” phenomenon; contacting customers who are not at risk could inadvertently remind them of competitors or issues they had forgotten, possibly creating churn where there would have been none. Thus, the best action for low-risk customers is to continue providing good service and address needs if they arise, but not to invest in special retention efforts preemptively.\nBy implementing this three-tier strategy, management can ensure that retention resources are allocated efficiently: heavy investment goes only to those accounts where it’s most needed and most likely to pay off, while lighter-touch or no intervention is used elsewhere. This targeted approach, powered by the churn prediction model, not only improves ROI as shown above but also aligns with practical constraints of retention teams (which have limited budget and manpower). In summary, the model’s outputs can be directly translated into an action plan that maximizes customer lifetime value saved per dollar spent.",
    "crumbs": [
      "<span class='chapter-number'>7</span>  <span class='chapter-title'>Discussion – Interpretation and Error Analysis</span>"
    ]
  },
  {
    "objectID": "report/06-discussion.html#sec-limitations",
    "href": "report/06-discussion.html#sec-limitations",
    "title": "7  Discussion – Interpretation and Error Analysis",
    "section": "7.5 Limitations",
    "text": "7.5 Limitations\nWhile the results of this churn prediction study and its applications are promising, it is important to acknowledge the study’s limitations.\n\n7.5.1 Static Snapshot Data\nFirst, the modeling was based on relatively static, snapshot data. Each customer is represented by features aggregated up to a certain point in time (e.g., average usage, last month’s bill, whether they have overdue upgrades, etc.). This means the model does not explicitly incorporate time-series patterns or sudden changes in behavior leading up to churn.\nChurn is often preceded by temporal signals; for instance, a customer’s data usage might decline for several months before cancellation, or there could be a spike in dropped calls or customer support tickets. Because the dataset did not include detailed sequential logs or longitudinal data, the model might miss these subtle trend-based indicators. In effect, churn is being predicted from a single-frame picture of the customer, rather than a movie of their behavior over time. This limitation could cause lower sensitivity to churn triggers that are only apparent when looking at how metrics evolve. Any abrupt shifts (say, a recent plunge in satisfaction rating or a sudden change in calling patterns) might be diluted or invisible in the snapshot features.\n\n\n7.5.2 Unobserved Variables and Contextual Factors\nSecondly, there are unobserved variables and contextual factors not captured in the dataset that likely play a role in churn:\n\nCustomer sentiment data: No access to satisfaction survey results, complaint logs, or social media feedback. Negative sentiment or unresolved service issues could be strong churn predictors; their absence means the model might not flag some dissatisfied customers.\nSocial network effects: In telecom, if a customer’s family or close friends leave the service (perhaps all on a shared plan or simply part of a local community), that customer is more likely to churn as well. The data did not reflect these peer effects or word-of-mouth influences.\nCompetitor actions: Price cuts or new offerings from a rival carrier can drive churn, but the model, being trained only on internal data, cannot foresee external enticements.\n\nIn summary, the model might be under-informed about certain churn drivers, which can manifest as the “silent churners” and some level of unexplained churn risk.\n\n\n7.5.3 Performance Ceiling\nFinally, it’s worth noting that the performance ceiling observed (AUC in the high 0.6700 range, with LightGBM achieving 0.6700 and XGBoost achieving 0.6637) indicates that even the best model leaves a substantial portion of churn variance unexplained. This is not a flaw of a particular algorithm but rather a reflection of churn’s inherent complexity and the limitations mentioned above. Thus, there is room for improvement by enhancing data and methodology.\nDespite these limitations, the insights gained are valuable as a foundation. The next chapter discusses how future work can address some of these limitations, incorporating new data sources, methods, and deployment practices to further improve churn prediction and its business utility.",
    "crumbs": [
      "<span class='chapter-number'>7</span>  <span class='chapter-title'>Discussion – Interpretation and Error Analysis</span>"
    ]
  },
  {
    "objectID": "report/07-conclusion.html",
    "href": "report/07-conclusion.html",
    "title": "8  Conclusion and Future Work",
    "section": "",
    "text": "8.1 Conclusion\nThis thesis presented a comprehensive exploration of telecom customer churn prediction, balancing predictive performance with interpretability and practical actionability. A range of modeling approaches was investigated under consistent conditions, class imbalance challenges were tackled, and insights were extracted to guide retention strategy. Here I summarize the findings in relation to the three research questions posed in the Introduction (Chapter 1).",
    "crumbs": [
      "<span class='chapter-number'>8</span>  <span class='chapter-title'>Conclusion and Future Work</span>"
    ]
  },
  {
    "objectID": "report/07-conclusion.html#sec-conclusion-summary",
    "href": "report/07-conclusion.html#sec-conclusion-summary",
    "title": "8  Conclusion and Future Work",
    "section": "",
    "text": "8.1.1 RQ1: Model Effectiveness\nResults indicate that gradient-boosted decision trees achieved the strongest overall performance among all model families tested. The champion single model was XGBoost, achieving a test ROC-AUC of 0.6637 and a test PR-AUC of 0.4454. The best heterogeneous ensemble (Blend_WeightedAUC) achieved a slightly higher test ROC-AUC of 0.6688 and test PR-AUC of 0.4488, but the marginal gain (+0.0050 ROC-AUC) suggests that XGBoost is the most practical choice for deployment. Both substantially outperformed the logistic regression baseline (test ROC-AUC = 0.5940, test PR-AUC = 0.3560).\nHeterogeneous ensemble methods (blending and OOF stacking) provided competitive but not decisively superior results. The best blending approach (Blend_WeightedAUC) reached a test ROC-AUC of 0.6688 and test PR-AUC of 0.4488, while OOF stacking achieved test ROC-AUC of 0.6679 and test PR-AUC of 0.4490. Ensemble methods slightly exceeded XGBoost on ROC-AUC, but the improvement was modest. This suggests that the gains from model combination derive primarily from variance reduction and diversity between linear and nonlinear base learners, rather than capturing fundamentally new predictive signals.\nThe Wide & Deep neural network, a model architecture not commonly used in traditional churn studies, proved feasible on tabular telecom data. The best neural network variant (Wide & Deep with focal loss) achieved test ROC-AUC of 0.6615 and test PR-AUC of 0.4356, demonstrating that deep learning approaches can capture churn patterns effectively, though they did not surpass gradient boosting methods in this dataset. The neural networks clustered in a narrow performance band (test ROC-AUC approximately 0.6440–0.6620), suggesting a performance ceiling for this architecture class on this feature set.\nThese findings address RQ1 by demonstrating that gradient-boosted trees represent the most effective single-model approach for this churn prediction task. Ensemble methods provide modest additional benefits, with the trade-off between performance gain and implementation complexity favoring simpler single-model deployment in many production scenarios.\n\n\n8.1.2 RQ2: Imbalance Robustness\nClass imbalance was a significant challenge, with churners comprising approximately 28.8% of the dataset. Various strategies were implemented and evaluated to ensure the models remained sensitive to the minority class:\n\nClass weighting in training (giving higher weight to churn examples in the loss function)\nExperimenting with focal loss for the neural networks (to focus learning on hard-to-predict churn cases)\nAdjusting the decision threshold on the validation set to optimize recall/precision trade-offs rather than using the default 0.5000\n\nThe findings indicate that these techniques did improve the model’s ability to capture churners. At the fixed operating threshold of τ = 0.4400, LightGBM achieved the best balance with precision = 0.3760, recall = 0.7310, and F1 = 0.4970. XGBoost performed nearly identically with precision = 0.3670, recall = 0.7390, and F1 = 0.4900. This balance is advantageous for retention contexts, facilitating the detection of approximately 73–74% of actual churners while maintaining precision in the high-30s%, a trade-off suitable for targeted retention campaigns where the cost of contacting non-churners is acceptable.\nExperiments with focal loss produced competitive ranking metrics (ROC-AUC, PR-AUC) but exhibited threshold sensitivity: under the fixed threshold of τ = 0.4400, some focal-loss variants produced very few positive predictions, resulting in near-zero recall despite competitive AUC scores. This highlights that loss function selection must be paired with appropriate threshold calibration for deployment. The standard weighted cross-entropy loss proved more robust across operating points in this dataset.\nThus, RQ2 is answered: yes, specific imbalance-aware strategies made the churn predictions more robust and useful, ensuring that the model’s performance on the minority class (churn) was strong enough for practical use. However, threshold selection proved as critical as algorithmic imbalance handling for achieving practical deployment performance.\n\n\n8.1.3 RQ3: Interpretability to Action\nThe best models, despite their complexity, are demonstrated to not be “black boxes” when it comes to drawing insights. Using SHAP values and related interpretability tools, model outputs were translated into actionable churn drivers.\nThe global SHAP analysis (see Section 7.1) highlighted features such as CurrentEquipmentDays, MonthlyMinutes, MonthsInService, PercChangeMinutes, and DroppedBlockedCalls as the primary churn drivers. These features map directly to actionable business levers:\n\nDevice lifecycle timing (CurrentEquipmentDays): Customers with longer equipment tenure show higher churn risk, supporting proactive upgrade offers\nEngagement patterns (MonthlyMinutes): Low usage strongly correlates with churn, identifying disengaged customers for re-engagement campaigns\nTenure effects (MonthsInService): Newer customers exhibit higher vulnerability, informing onboarding and early retention strategies\nService quality signals (DroppedBlockedCalls): Call failures drive dissatisfaction and switching behavior\n\nSHAP dependence plots revealed non-linear effects that would be difficult to summarize with a single coefficient, such as the asymmetric risk pattern for MonthlyMinutes (low usage drives much stronger positive SHAP contributions than high usage provides negative contributions) and threshold-like upgrade cycle effects in CurrentEquipmentDays.\nImportantly, these interpretations were derived from the same model that produces the risk scores, ensuring consistency between what is deployed and what is explained. These insights were further used to craft an action framework (the three-tier retention strategy) described in Section 7.4, exemplifying how interpretability bridges the gap between prediction and decision. Consequently, regarding RQ3, the findings demonstrate that the model interpretation outputs were successfully converted into practical guidance, reinforcing the idea that churn prediction models should be judged not only by their AUC, but also by how well they inform next steps for retention efforts.\n\n\n8.1.4 Engineering Trade-offs\nIn evaluating the above points, it’s worth noting an engineering trade-off that emerged. While ensemble methods achieved comparable performance to single models, they introduce significant operational complexity without proportionate performance gains. The marginal improvements (approximately 0.0010–0.0020 in PR-AUC) must be weighed against deployment and maintenance costs.\n\n\n\nTable 8.1: Comparison of deployment considerations between ensemble methods and single GBM models.\n\n\n\n\n\nConsideration\nEnsemble (Blending/Stacking)\nXGBoost Only\n\n\n\n\nTest ROC-AUC\n0.6679–0.6688\n0.6637\n\n\nTest PR-AUC\n0.4488–0.4490\n0.4454\n\n\nInference Latency\nHigher (multiple models)\nLow\n\n\nDeployment Complexity\nHigh\nLow\n\n\nMaintenance Burden\nMultiple model artifacts\nSingle artifact\n\n\nInterpretability\nRequires aggregation\nDirect SHAP analysis\n\n\nThreshold Calibration\nMore complex\nStraightforward\n\n\n\n\n\n\nFor a real-time deployment where latency and system simplicity are paramount, a single GBM model (XGBoost) represents the pragmatic choice, offering fast inference, straightforward integration, and essentially equivalent predictive performance. The ensemble approach might be justified in batch-scoring scenarios where the slight PR-AUC lift translates to meaningful business value at scale, but the complexity trade-off generally favors single-model deployment.\nThis highlights that the “best” model on paper isn’t always the best in practice; model selection must consider deployment constraints, maintainability, and interpretability requirements as well as raw performance. In this case, XGBoost is the recommended production choice: it delivers strong test performance while maintaining operational simplicity.\nUltimately, this thesis showed that advanced models can be harnessed for churn prediction without making the results opaque. A balance was achieved where performance, interpretability, and practical utility coexist, which is a promising outcome for deploying the model in a real telecom business environment.",
    "crumbs": [
      "<span class='chapter-number'>8</span>  <span class='chapter-title'>Conclusion and Future Work</span>"
    ]
  },
  {
    "objectID": "report/07-conclusion.html#sec-future-work",
    "href": "report/07-conclusion.html#sec-future-work",
    "title": "8  Conclusion and Future Work",
    "section": "8.2 Future Work",
    "text": "8.2 Future Work\nBuilding on this study’s findings and acknowledging its limitations (see Section 7.5), several avenues for future work are identified to further enhance telecom churn prediction and its deployment.\n\n8.2.1 Incorporating Semi-Supervised Learning\nOne promising direction is to leverage unlabeled or partially labeled data through semi-supervised techniques. The current study attempted pseudo-labeling using the XGBoost teacher model on real holdout data, but achieved minimal coverage: at conservative confidence thresholds (0.0500/0.9500), only 2 samples out of 20,000 met the criteria, with approximately 100% remaining “uncertain.” This result highlights that confidence calibration and threshold selection are critical for successful semi-supervised learning.\nFuture work could explore:\n\nRelaxed confidence thresholds with curriculum learning to gradually incorporate pseudo-labels\nSelf-training with consistency regularization (e.g., MixMatch, FixMatch) that does not rely solely on hard pseudo-label thresholds\nContrastive learning approaches that learn representations from unlabeled data before fine-tuning on labeled examples\nTeacher ensemble methods using multiple diverse teachers to provide more calibrated confidence estimates\n\n\n\n\n\n\n\nNoteConnection to Current Work\n\n\n\nThe autoencoder experiments in this thesis (see Section 7.3) showed that learned latent representations did not significantly improve downstream classification beyond the original feature set (augmented features achieved ROC-AUC = 0.6451 vs. baseline 0.6465). However, the autoencoder architecture remains valuable as an engineering capability: it provides a reusable representation learning component and a clear interface for future semi-supervised variants if better confidence calibration or more unlabeled data becomes available.\n\n\n\n\n8.2.2 Sequence Modeling and Temporal Features\nAs identified in the limitations (Section 7.5), the analysis would benefit from true temporal modeling. The current dataset provides snapshot features representing customer state at a single point in time, limiting the model’s ability to capture behavioral trends that precede churn.\nPotential approaches include:\n\nRecurrent Neural Networks (LSTMs): Can learn patterns like “gradual decline in data usage over 6 months” or “a spike in dropped calls last month” that precede churn\nTransformer-based architectures: Could capture complex patterns of seasonality or multi-feature interactions over time for multivariate time series of customer behavior\nFeature engineering for trends: Even without full sequence models, computing rolling statistics (e.g., 3-month moving averages, month-over-month deltas) from historical data could provide temporal signals within the current tabular framework\nEvent-based modeling: Treating customer interactions (calls, complaints, plan changes) as discrete events and using event sequence models\n\nThe PercChangeMinutes feature in the current dataset provides a glimpse of temporal value: it emerged as a top SHAP feature, with declining usage patterns strongly associated with higher churn risk. This suggests that richer temporal features would likely improve predictive performance and enable earlier intervention.\n\n\n8.2.3 Real-time Inference and MLOps Deployment\nFrom an operational standpoint, future efforts should focus on deploying the churn model in a production environment with real-time or near-real-time inference capabilities. The engineering foundation established in this thesis, including MLflow experiment tracking, modular pipeline design, and reproducible preprocessing, provides a starting point for production deployment.\nKey MLOps components for production deployment include:\n\n\n\nTable 8.2: Key components of an MLOps pipeline for churn prediction deployment, prioritized by implementation order.\n\n\n\n\n\n\n\n\n\n\nComponent\nDescription\nPriority\n\n\n\n\nAutomated Data Pipelines\nFeed the model with fresh data continuously\nHigh\n\n\nModel Serving Infrastructure\nAPI or microservice for real-time predictions\nHigh\n\n\nFeature Store\nEnsure training-serving consistency for features\nHigh\n\n\nData Drift Detection\nMonitor input feature distributions\nMedium\n\n\nConcept Drift Detection\nMonitor prediction distribution and model performance\nMedium\n\n\nRetraining Schedules\nRefresh the model periodically (e.g., quarterly)\nMedium\n\n\nA/B Testing Framework\nValidate ROI assumptions with actual campaign results\nMedium\n\n\nShadow Mode Deployment\nRun new models alongside production for comparison\nLow\n\n\n\n\n\n\nThe preprocessing bug discovered during this research, a 1000x scale mismatch between training and holdout data that caused catastrophic model performance degradation (train/validation loss ratios exceeding 285x), underscores the critical importance of training-serving consistency. Production systems must implement:\n\nUnified preprocessing pipelines that share code between training and inference\nFeature signature validation to catch schema mismatches early\nAutomated sanity checks (e.g., feature distribution comparisons) before model scoring\n\nOne critical aspect is drift monitoring. Over time, the profile of churners may change; for instance, if the company launches a new product or a competitor changes their strategy, the factors influencing churn could shift. The SHAP-based interpretability framework developed in this thesis provides a foundation for monitoring: tracking changes in feature importance distributions over time can serve as an early warning system for concept drift.\n\n\n8.2.4 Calibration and Threshold Optimization\nThe experiments revealed that probability calibration and threshold selection significantly impact operational performance, particularly for models trained with focal loss. Future work should explore:\n\nIsotonic regression or Platt scaling for post-hoc calibration\nCost-sensitive threshold optimization incorporating business parameters (intervention cost, customer LTV, save rate)\nDynamic thresholds that adapt to changing business constraints or resource availability\nCalibration monitoring in production to detect when model probabilities become unreliable\n\n\nIn conclusion, the journey of building this churn prediction system has shown both the substantial value such models can deliver and the complexity of making them truly effective in practice. The key findings, indicating that gradient-boosted trees (with XGBoost achieving test ROC-AUC = 0.6637) outperform other model families, that the best ensemble offers only a modest lift (test ROC-AUC = 0.6688), that class imbalance handling requires both algorithmic strategies and threshold tuning, and that SHAP-based interpretability enables actionable business insights, provide a solid foundation for production deployment.\nBy addressing the above future directions, from advanced modeling techniques like semi-supervised learning and sequence modeling to the practicalities of real-time deployment and drift monitoring, the model’s performance and longevity can be further enhanced. Ultimately, the goal is to evolve the churn prediction solution into a long-term, self-improving asset for the telecom provider, one that not only predicts which customers might leave, but continuously learns from new data and helps the business prevent churn in an ever-changing environment.",
    "crumbs": [
      "<span class='chapter-number'>8</span>  <span class='chapter-title'>Conclusion and Future Work</span>"
    ]
  },
  {
    "objectID": "report/97-acknowledgements.html",
    "href": "report/97-acknowledgements.html",
    "title": "Acknowledgments",
    "section": "",
    "text": "I thank Dr. Clinton Watkins, Dr. Eric Yanchenko, and my classmates for guidance and feedback throughout the project.",
    "crumbs": [
      "Acknowledgments"
    ]
  },
  {
    "objectID": "report/98-references.html",
    "href": "report/98-references.html",
    "title": "References",
    "section": "",
    "text": "Ahn, Jae-Hyeon, Sang Pil Han, and Yung-Seop Lee. 2006. “Customer\nChurn Analysis: Churn Determinants and Mediation Effects of\nPartial Defection in the Korean Mobile Telecommunications\nService Industry.” Telecommunications Policy 30\n(November): 552–68. https://doi.org/10.1016/j.telpol.2006.09.006.\n\n\nAkosa, J. 2017. “Predictive Accuracy : A\nMisleading Performance Measure for Highly Imbalanced\nData.” In.\n\n\nAsif, Daniyal, Muhammad Shoaib Arif, and Aiman Mukheimer. 2025. “A\nData-Driven Approach with Explainable Artificial Intelligence for\nCustomer Churn Prediction in the Telecommunications Industry.”\nResults in Engineering 26 (June): 104629. https://doi.org/10.1016/j.rineng.2025.104629.\n\n\nBeeharry, Yogesh, and Ristin Tsokizep Fokone. 2022. “Hybrid\nApproach Using Machine Learning Algorithms for Customers’ Churn\nPrediction in the Telecommunications Industry.” Concurrency\nand Computation: Practice and Experience 34 (4): e6627. https://doi.org/10.1002/cpe.6627.\n\n\nBogaert, Matthias, and Lex Delaere. 2023. “Ensemble\nMethods in Customer Churn Prediction: A\nComparative Analysis of the State-of-the-Art.” Mathematics 11\n(February): 1137. https://doi.org/10.3390/math11051137.\n\n\nBugajev, Andrej, Rima Kriauzienė, and Viktoras Chadyšas. 2025.\n“Realistic Data Delays and Alternative\nInactivity Definitions in Telecom Churn:\nInvestigating Concept Drift Using a Sliding-Window\nApproach.” Applied Sciences 15 (3): 1599. https://doi.org/10.3390/app15031599.\n\n\nBurez, J., and D. Van Den Poel. 2009. “Handling Class Imbalance in\nCustomer Churn Prediction.” Expert Systems with\nApplications 36 (3): 4626–36. https://doi.org/10.1016/j.eswa.2008.05.027.\n\n\nCoussement, Kristof, and Koen W. De Bock. 2013. “Customer Churn\nPrediction in the Online Gambling Industry: The Beneficial\nEffect of Ensemble Learning.” Journal of Business\nResearch, Advancing Research Methods in\nMarketing, 66 (9): 1629–36. https://doi.org/10.1016/j.jbusres.2012.12.008.\n\n\nDick, A. S., and K. Basu. 1994. “Customer Loyalty:\nToward an Integrated Conceptual\nFramework.” Journal of the Academy of Marketing\nScience 22 (2): 99–113. https://doi.org/10.1177/0092070394222001.\n\n\nGeiler, Louis, Séverine Affeldt, and Mohamed Nadif. 2022. “An\nEffective Strategy for Churn Prediction and Customer Profiling.”\nData & Knowledge Engineering 142 (November): 102100. https://doi.org/10.1016/j.datak.2022.102100.\n\n\nGerpott, Torsten J, Wolfgang Rams, and Andreas Schindler. 2001.\n“Customer Retention, Loyalty, and Satisfaction in the\nGerman Mobile Cellular Telecommunications Market.”\nTelecommunications Policy 25 (4): 249–69. https://doi.org/10.1016/S0308-5961(00)00097-5.\n\n\nHaddadi, Amir Mohammad, and Hodjat Hamidi. 2025. “A Hybrid Model\nfor Improving Customer Lifetime Value Prediction Using Stacking Ensemble\nLearning Algorithm.” Computers in Human Behavior Reports\n18 (May): 100616. https://doi.org/10.1016/j.chbr.2025.100616.\n\n\nImani, Mehdi. 2024. “Evaluating Classification and\nSampling Methods for Customer Churn Prediction\nUnder Varying Imbalance Levels.”\n\n\nJiao, Gui’e, and Hong Xu. 2021. “Analysis and\nComparison of Forecasting Algorithms for\nTelecom Customer Churn.” Journal of Physics:\nConference Series 1881 (3): 032061. https://doi.org/10.1088/1742-6596/1881/3/032061.\n\n\nKeaveney, Susan M. 1995. “Customer Switching Behavior\nin Service Industries: An Exploratory\nStudy.” Journal of Marketing 59 (2): 71–82.\n\n\nKhodabandehlou, Samira, and Mahmoud Rahman. 2017. “Comparison of\nSupervised Machine Learning Techniques for Customer Churn Prediction\nBased on Analysis of Customer Behavior.” Journal of Systems\nand Information Technology 19 (August): 00–00. https://doi.org/10.1108/JSIT-10-2016-0061.\n\n\nKim, Hee-Su, and Choong-Han Yoon. 2004. “Determinants of\nSubscriber Churn and Customer Loyalty in the Korean Mobile\nTelephony Market.” Telecommunications Policy 28 (9-10):\n751–65. https://doi.org/10.1016/j.telpol.2004.05.013.\n\n\nKim, Moon-Koo, Myeong-Cheol Park, and Dong-Heon Jeong. 2004. “The\nEffects of Customer Satisfaction and Switching Barrier on Customer\nLoyalty in Korean Mobile Telecommunication\nServices.” Telecommunications Policy, Growth in mobile\ncommunications, 28 (2): 145–59. https://doi.org/10.1016/j.telpol.2003.12.003.\n\n\nLalwani, Praveen, Manas Kumar Mishra, Jasroop Singh Chadha, and Pratyush\nSethi. 2022. “Customer Churn Prediction System: A Machine Learning\nApproach.” Computing 104 (2): 271–94. https://doi.org/10.1007/s00607-021-00908-y.\n\n\nLemmens, Aurélie, and Christophe Croux. 2006. “Bagging and\nBoosting Classification Trees to Predict\nChurn.” Journal of Marketing Research 43 (May).\nhttps://doi.org/10.1509/jmkr.43.2.276.\n\n\nPoudel, Sumana Sharma, Suresh Pokharel, and Mohan Timilsina. 2024.\n“Explaining Customer Churn Prediction in Telecom Industry Using\nTabular Machine Learning Models.” Machine Learning with\nApplications 17 (September): 100567. https://doi.org/10.1016/j.mlwa.2024.100567.\n\n\n“Regaining Service Customers - Bernd\nStauss, Christian Friege, 1999.” n.d.\nhttps://journals-sagepub-com.wwwproxy1.library.unsw.edu.au/doi/abs/10.1177/109467059914006.\nAccessed October 15, 2025.\n\n\nRosenberg, Larry J., and John A. Czepiel. 1984. “A MARKETING\nAPPROACH FOR CUSTOMER RETENTION.” Journal of Consumer\nMarketing 1 (2): 45–51. https://doi.org/10.1108/eb008094.\n\n\nSikri, Alisha, Roshan Jameel, Sheikh Mohammad Idrees, and Harleen Kaur.\n2024. “Enhancing Customer Retention in Telecom Industry with\nMachine Learning Driven Churn Prediction.” Scientific\nReports 14 (1): 13097. https://doi.org/10.1038/s41598-024-63750-0.\n\n\nŠonková, Tereza, and Monika Grabowska. 2015. “Customer Engagement:\nTransactional Vs. Relationship Marketing.” Journal of\nInternational Studies 8 (May): 196–207. https://doi.org/10.14254/2071-8330.2015/8-1/17.\n\n\nThangeda, Rahul, Niraj Kumar, and Ritanjali Majhi. 2024. “A Neural\nNetwork-Based Predictive Decision Model for Customer Retention in the\nTelecommunication Sector.” Technological Forecasting and\nSocial Change 202 (May): 123250. https://doi.org/10.1016/j.techfore.2024.123250.\n\n\nVafeiadis, T., K. I. Diamantaras, G. Sarigiannidis, and K.Ch.\nChatzisavvas. 2015. “A Comparison of Machine Learning Techniques\nfor Customer Churn Prediction.” Simulation Modelling Practice\nand Theory 55 (June): 1–9. https://doi.org/10.1016/j.simpat.2015.03.003.\n\n\nWei, Chih-Ping, and I-Tang Chiu. 2002. “Turning Telecommunications\nCall Details to Churn Prediction: A Data Mining Approach.”\nExpert Systems with Applications 23 (2): 103–12. https://doi.org/10.1016/S0957-4174(02)00030-1.\n\n\nZhu, Bing, Cheng Qian, Seppe vanden Broucke, Jin Xiao, and Yuanyuan Li.\n2023. “A Bagging-Based Selective Ensemble Model for Churn\nPrediction on Imbalanced Data.” Expert Systems with\nApplications 227 (October): 120223. https://doi.org/10.1016/j.eswa.2023.120223.",
    "crumbs": [
      "References"
    ]
  },
  {
    "objectID": "report/99-appendix.html",
    "href": "report/99-appendix.html",
    "title": "Appendix A — Appendix",
    "section": "",
    "text": "A.1 Reproducibility Statement\nThis appendix provides all supplementary materials referenced in the thesis, including reproducibility notes, the complete feature registry, model diagnostic plots, detailed experimental results, and environment configuration information.\nThis project uses a script-based pipeline. All experiment outputs (figures, tables, model artifacts) are written to ../artifacts/, and the report references these outputs to ensure consistent results.",
    "crumbs": [
      "Appendices",
      "<span class='chapter-number'>A</span>  <span class='chapter-title'>Appendix</span>"
    ]
  },
  {
    "objectID": "report/99-appendix.html#sec-repro",
    "href": "report/99-appendix.html#sec-repro",
    "title": "Appendix A — Appendix",
    "section": "",
    "text": "A.1.1 Quick Reproduce (Fast Mode)\nThe commands below complete the full pipeline in about 10-15 minutes (using --fast to skip full hyperparameter search):\n# Set project root\nexport CAPSTONE_ROOT=\"$(pwd)\"\n\n# Run full pipeline\npython scripts/02_preprocess.py           # Data preprocessing and feature engineering\npython scripts/03_logit.py                # Baseline: Logistic Regression\npython scripts/04_trees_gbm.py --fast     # Tree-based models (XGBoost, LightGBM, CatBoost)\npython scripts/05_nn.py --fast            # Deep learning (PyTorch Wide and Deep)\npython scripts/07_stacking.py             # Ensemble stacking\npython scripts/08_interpretation.py --fast # SHAP interpretation\n\n# Render the PDF\nquarto render\n\n\nA.1.2 Full Reproduce (Complete Mode)\nFull run (including Optuna hyperparameter optimization, about 2-4 hours):\nexport CAPSTONE_ROOT=\"$(pwd)\"\n\npython scripts/02_preprocess.py\npython scripts/03_logit.py\npython scripts/04_trees_gbm.py            # Full Optuna tuning (100 trials)\npython scripts/05_nn.py                   # Full NN experiments\npython scripts/06_autoencoder.py          # Semi-supervised learning\npython scripts/07_stacking.py\npython scripts/08_interpretation.py\n\nquarto render\n\n\nA.1.3 Output Directory Structure\n../artifacts/\n├── figures/          # All visualizations (PNG)\n│   └── optuna/       # Optuna diagnostics\n├── tables/           # All result tables (CSV)\n└── data/             # Intermediate outputs",
    "crumbs": [
      "Appendices",
      "<span class='chapter-number'>A</span>  <span class='chapter-title'>Appendix</span>"
    ]
  },
  {
    "objectID": "report/99-appendix.html#sec-appendix-a",
    "href": "report/99-appendix.html#sec-appendix-a",
    "title": "Appendix A — Appendix",
    "section": "A.2 Data Documentation",
    "text": "A.2 Data Documentation\n\nA.2.1 Complete Feature Registry\nThe feature registry serves as the single source of truth for all feature-level decisions throughout the modeling pipeline. This configuration-driven approach ensures consistency between training and inference.\n\n\n\nTable A.1: Complete Feature Registry with preprocessing decisions\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nFeature\nOrigin\nSemantic Group\nType\nTransform\nKeep (GLM/Tree/NN)\nDecision\n\n\n\n\nCustomerID\nOriginal\nid_target\nIdentifier\n–\nN/N/N\nDrop\n\n\nChurn\nOriginal\nid_target\nBinary target\n0/1 encoding\n–\nTarget\n\n\nMonthlyRevenue\nOriginal\nbilling_economics\nContinuous\nlog, winsor\nN/Y/N\nReplace\n\n\nMonthlyMinutes\nOriginal\nusage_activity\nContinuous\nlog, winsor\nY/Y/Y\nKeep\n\n\nTotalRecurringCharge\nOriginal\nbilling_economics\nContinuous\n–\nN/N/N\nDrop\n\n\nDirectorAssistedCalls\nOriginal\nusage_activity\nContinuous\n–\nN/N/N\nDrop\n\n\nOverageMinutes\nOriginal\nbilling_economics\nContinuous\nlog, winsor\nY/Y/Y\nKeep\n\n\nRoamingCalls\nOriginal\nusage_activity\nContinuous\n–\nN/N/N\nDrop\n\n\nPercChangeMinutes\nOriginal\nbilling_economics\nMomentum\nwinsor\nY/Y/Y\nKeep\n\n\nPercChangeRevenues\nOriginal\nbilling_economics\nMomentum\nwinsor\nY/Y/Y\nKeep\n\n\nDroppedCalls\nOriginal\nquality_experience\nCount\n–\nN/N/N\nReplace\n\n\nBlockedCalls\nOriginal\nquality_experience\nCount\n–\nN/N/N\nReplace\n\n\nUnansweredCalls\nOriginal\nquality_experience\nCount\n–\nN/N/N\nDrop\n\n\nCustomerCareCalls\nOriginal\nsupport_retention\nCount\n–\nN/N/N\nReplace\n\n\nThreewayCalls\nOriginal\nusage_activity\nCount\n–\nN/N/N\nDrop\n\n\nReceivedCalls\nOriginal\nusage_activity\nCount\n–\nN/N/N\nDrop\n\n\nOutboundCalls\nOriginal\nusage_activity\nCount\n–\nY/Y/Y\nKeep\n\n\nInboundCalls\nOriginal\nusage_activity\nCount\n–\nN/N/N\nDrop\n\n\nPeakCallsInOut\nOriginal\nusage_activity\nCount\n–\nN/N/N\nDrop\n\n\nOffPeakCallsInOut\nOriginal\nusage_activity\nCount\n–\nN/N/N\nDrop\n\n\nDroppedBlockedCalls\nOriginal\nquality_experience\nCount\nlog, winsor\nY/Y/Y\nKeep\n\n\nCallForwardingCalls\nOriginal\nquality_experience\nCount\n–\nN/N/N\nDrop\n\n\nCallWaitingCalls\nOriginal\nquality_experience\nCount\n–\nN/N/N\nDrop\n\n\nMonthsInService\nOriginal\naccount_tenure\nInteger\n–\nY/Y/Y\nKeep\n\n\nUniqueSubs\nOriginal\naccount_tenure\nCount\n–\nN/N/N\nDrop\n\n\nActiveSubs\nOriginal\naccount_tenure\nCount\n–\nY/Y/Y\nKeep\n\n\nServiceArea\nOriginal\ngeo_segmentation\nNominal (747)\n–\nN/N/N\nDrop\n\n\nHandsets\nOriginal\naccount_tenure\nCount\n–\nN/N/N\nDrop\n\n\nHandsetModels\nOriginal\naccount_tenure\nCount\n–\nN/N/N\nDrop\n\n\nCurrentEquipmentDays\nOriginal\naccount_tenure\nInteger\n–\nY/Y/Y\nKeep\n\n\nAgeHH1\nOriginal\ndemographics\nContinuous\n–\nY/Y/Y\nKeep\n\n\nAgeHH2\nOriginal\ndemographics\nContinuous\n–\nN/N/N\nDrop\n\n\nChildrenInHH\nOriginal\ndemographics\nBinary\nYes/No→1/0\nY/Y/Y\nKeep\n\n\nHandsetRefurbished\nOriginal\nequipment\nBinary\nYes/No→1/0\nY/Y/Y\nKeep\n\n\nHandsetWebCapable\nOriginal\nequipment\nBinary\nYes/No→1/0\nY/Y/Y\nKeep\n\n\nTruckOwner\nOriginal\ndemographics\nBinary\nYes/No→1/0\nN/N/N\nDrop\n\n\nRVOwner\nOriginal\ndemographics\nBinary\nYes/No→1/0\nN/N/N\nDrop\n\n\nHomeownership\nOriginal\ndemographics\nBinary\n–\nY/Y/Y\nKeep\n\n\nBuysViaMailOrder\nOriginal\ndemographics\nBinary\nYes/No→1/0\nN/N/N\nDrop\n\n\nRespondsToMailOffers\nOriginal\ndemographics\nBinary\nYes/No→1/0\nN/N/N\nDrop\n\n\nOptOutMailings\nOriginal\ndemographics\nBinary\nYes/No→1/0\nN/N/N\nDrop\n\n\nNonUSTravel\nOriginal\ndemographics\nBinary\nYes/No→1/0\nN/N/N\nDrop\n\n\nOwnsComputer\nOriginal\ndemographics\nBinary\nYes/No→1/0\nN/N/N\nDrop\n\n\nHasCreditCard\nOriginal\ndemographics\nBinary\nYes/No→1/0\nN/N/N\nDrop\n\n\nRetentionCalls\nOriginal\nsupport_retention\nCount\n–\nN/N/N\nEXCLUDE\n\n\nRetentionOffersAccepted\nOriginal\nsupport_retention\nCount\n–\nN/N/N\nEXCLUDE\n\n\nNewCellphoneUser\nOriginal\naccount_tenure\nBinary\nYes/No→1/0\nY/Y/Y\nKeep\n\n\nNotNewCellphoneUser\nOriginal\naccount_tenure\nBinary\nYes/No→1/0\nN/N/N\nDrop\n\n\nReferralsMadeBySubscriber\nOriginal\nengagement\nCount\n–\nY/Y/Y\nKeep\n\n\nIncomeGroup\nOriginal\ndemographics\nOrdinal\n–\nY/Y/Y\nKeep\n\n\nOwnsMotorcycle\nOriginal\ndemographics\nBinary\nYes/No→1/0\nN/N/N\nDrop\n\n\nAdjustmentsToCreditRating\nOriginal\nbilling_economics\nInteger\n–\nY/Y/Y\nKeep\n\n\nHandsetPrice\nOriginal\nequipment\nOrdinal\ntarget encode\nY/Y/Y\nKeep\n\n\nMadeCallToRetentionTeam\nOriginal\nsupport_retention\nBinary\nYes/No→1/0\nN/N/N\nEXCLUDE\n\n\nCreditRating\nOriginal\ndemographics\nOrdinal\ntarget encode\nY/Y/Y\nKeep\n\n\nPrizmCode\nOriginal\ngeo_segmentation\nNominal\ntarget encode\nY/Y/Y\nKeep\n\n\nOccupation\nOriginal\ndemographics\nNominal\ntarget encode\nY/Y/Y\nKeep\n\n\nMaritalStatus\nOriginal\ndemographics\nNominal\none-hot\nY/Y/Y\nKeep\n\n\n\n\n\n\nLegend:\n\nKeep (GLM/Tree/NN): Y = included, N = excluded for that model family\nEXCLUDE: Features excluded due to data leakage risk (post-decision variables)\nTransform: log = log1p transformation, winsor = winsorization at 1st/99th percentile\n\n\n\nA.2.2 Semantic Feature Grouping\nFeatures were organized into semantic groups to facilitate domain-driven feature engineering and interpretability analysis.\n\n\n\nTable A.2: Semantic grouping of features for domain-driven analysis\n\n\n\n\n\n\n\n\n\n\nSemantic Group\nFeatures\nDescription\n\n\n\n\nid_target\nCustomerID, Churn\nIdentifier and target variable\n\n\nbilling_economics\nMonthlyRevenue, TotalRecurringCharge, OverageMinutes, PercChangeMinutes, PercChangeRevenues, AdjustmentsToCreditRating\nFinancial and billing-related metrics\n\n\nusage_activity\nMonthlyMinutes, DirectorAssistedCalls, RoamingCalls, ThreewayCalls, ReceivedCalls, OutboundCalls, InboundCalls, PeakCallsInOut, OffPeakCallsInOut\nCall volume and usage patterns\n\n\nquality_experience\nDroppedCalls, BlockedCalls, UnansweredCalls, DroppedBlockedCalls, CallForwardingCalls, CallWaitingCalls\nService quality indicators\n\n\nsupport_retention\nCustomerCareCalls, RetentionCalls, RetentionOffersAccepted, MadeCallToRetentionTeam\nCustomer support interactions\n\n\naccount_tenure\nMonthsInService, UniqueSubs, ActiveSubs, Handsets, HandsetModels, CurrentEquipmentDays, NewCellphoneUser, NotNewCellphoneUser\nAccount age and device history\n\n\ndemographics\nAgeHH1, AgeHH2, ChildrenInHH, TruckOwner, RVOwner, Homeownership, BuysViaMailOrder, RespondsToMailOffers, OptOutMailings, NonUSTravel, OwnsComputer, HasCreditCard, IncomeGroup, OwnsMotorcycle, MaritalStatus, Occupation, CreditRating\nCustomer demographic attributes\n\n\nequipment\nHandsetRefurbished, HandsetWebCapable, HandsetPrice\nDevice characteristics\n\n\ngeo_segmentation\nServiceArea, PrizmCode\nGeographic and market segmentation\n\n\n\n\n\n\n\n\nA.2.3 Leakage Risk Assessment\nA systematic leakage scan was conducted to identify features that could leak future information. Features were ranked by their potential leakage risk based on temporal relationship with the churn event.\n\n\n\nTable A.3: Leakage risk assessment for retention-related features\n\n\n\n\n\n\n\n\n\n\n\nFeature\nLeakage Risk Score\nStatus\nRationale\n\n\n\n\nMadeCallToRetentionTeam\n0.9500\nEXCLUDED\nPost-decision: Customer called retention team after deciding to churn\n\n\nRetentionCalls\n0.9200\nEXCLUDED\nPost-decision: Retention outreach triggered by churn indicators\n\n\nRetentionOffersAccepted\n0.9000\nEXCLUDED\nPost-decision: Offers made only to at-risk customers\n\n\nCustomerCareCalls\n0.3500\nMonitored\nMay include pre-churn complaints but also routine inquiries\n\n\nPercChangeMinutes\n0.2500\nRetained\nBehavioral trend, not post-decision action\n\n\nPercChangeRevenues\n0.2500\nRetained\nBehavioral trend, not post-decision action",
    "crumbs": [
      "Appendices",
      "<span class='chapter-number'>A</span>  <span class='chapter-title'>Appendix</span>"
    ]
  },
  {
    "objectID": "report/99-appendix.html#sec-appendix-b",
    "href": "report/99-appendix.html#sec-appendix-b",
    "title": "Appendix A — Appendix",
    "section": "A.3 Baseline Model Details (Logistic Regression)",
    "text": "A.3 Baseline Model Details (Logistic Regression)\n\nA.3.1 Grid Search Results\nExhaustive grid search was performed over regularization strength (C) and penalty type to establish baseline performance.\n\n\n\nTable A.4: Logistic Regression Grid Search Results (5-fold Stratified CV)\n\n\n\n\n\nC\nPenalty\nCV ROC-AUC (mean)\nCV ROC-AUC (std)\nRank\n\n\n\n\n0.0010\nL2\n0.5821\n0.0089\n8\n\n\n0.0100\nL2\n0.5867\n0.0082\n4\n\n\n0.1000\nL2\n0.5883\n0.0078\n1\n\n\n1.0\nL2\n0.5878\n0.0081\n2\n\n\n10.0\nL2\n0.5872\n0.0083\n3\n\n\n0.0010\nL1\n0.5798\n0.0095\n9\n\n\n0.0100\nL1\n0.5856\n0.0086\n6\n\n\n0.1000\nL1\n0.5864\n0.0084\n5\n\n\n1.0\nL1\n0.5851\n0.0088\n7\n\n\n\n\n\n\nSelected Configuration: C=0.1000, L2 penalty (elastic-net with l1_ratio=0 equivalent)\n\n\nA.3.2 Threshold Sweep Analysis\nClassification thresholds were swept from 0.1000 to 0.5000 on the validation set to understand precision-recall trade-offs.\n\n\n\nTable A.5: Logistic Regression Threshold Sweep on Validation Set\n\n\n\n\n\nThreshold\nPrecision\nRecall\nF1-Score\nSpecificity\n\n\n\n\n0.1000\n0.3120\n0.8920\n0.4630\n0.2340\n\n\n0.1500\n0.3280\n0.8410\n0.4720\n0.2980\n\n\n0.2000\n0.3510\n0.7830\n0.4850\n0.3780\n\n\n0.2500\n0.3820\n0.7120\n0.4970\n0.4670\n\n\n0.2900\n0.4120\n0.6510\n0.5040\n0.5420\n\n\n0.3500\n0.4580\n0.5670\n0.5060\n0.6340\n\n\n0.4000\n0.5010\n0.4780\n0.4890\n0.7120\n\n\n0.4500\n0.5480\n0.3890\n0.4550\n0.7830\n\n\n0.5000\n0.5920\n0.3010\n0.3990\n0.8450\n\n\n\n\n\n\nOptimal threshold: τ = 0.2900 (maximizing F1-score on validation set)\n\n\nA.3.3 Logistic Regression Diagnostic Plots\n\n\n\n\n\n\nFigure A.1: ROC curves for logistic regression model on train/validation/test sets.\n\n\n\n\n\n\n\n\n\nFigure A.2: Precision-Recall curves for logistic regression model.\n\n\n\n\n\n\n\n\n\nFigure A.3: Precision, recall, and F1-score as functions of classification threshold.\n\n\n\n\n\n\n\n\n\nFigure A.4: Confusion matrices for logistic regression at different thresholds.",
    "crumbs": [
      "Appendices",
      "<span class='chapter-number'>A</span>  <span class='chapter-title'>Appendix</span>"
    ]
  },
  {
    "objectID": "report/99-appendix.html#sec-appendix-c",
    "href": "report/99-appendix.html#sec-appendix-c",
    "title": "Appendix A — Appendix",
    "section": "A.4 Tree and GBM Model Supplements",
    "text": "A.4 Tree and GBM Model Supplements\n\nA.4.1 Single Decision Tree Visualization\nA shallow decision tree (max_depth=3) provides an interpretable view of the top decision rules learned from the data. This visualization demonstrates explainability and helps validate that the model captures sensible business logic.\n\n\n\n\n\n\nFigure A.5: Single decision tree (depth=3) showing top decision rules for churn prediction.\n\n\n\n\n\nA.4.2 Hyperparameter Search Space\nThe following table documents the hyperparameter search space used for tree-based models, supporting reproducibility.\n\n\n\nTable A.6: Hyperparameter search space and best values for tree-based models\n\n\n\n\n\nModel\nParameter\nSearch Range\nBest Value\n\n\n\n\nRandom Forest\nn_estimators\n[100, 500]\n300\n\n\n\nmax_depth\n[5, 15, None]\n12\n\n\n\nmin_samples_split\n[2, 5, 10]\n5\n\n\n\nmin_samples_leaf\n[1, 2, 4]\n2\n\n\nXGBoost\nn_estimators\n[100, 500]\n312\n\n\n\nmax_depth\n[3, 6, 9]\n6\n\n\n\nlearning_rate\n[0.01, 0.3]\n0.0847\n\n\n\nsubsample\n[0.6, 1.0]\n0.8234\n\n\n\ncolsample_bytree\n[0.6, 1.0]\n0.7891\n\n\n\nmin_child_weight\n[1, 7]\n3\n\n\n\nreg_alpha\n[0, 1]\n0.0123\n\n\n\nreg_lambda\n[0, 3]\n1.2456\n\n\nLightGBM\nn_estimators\n[100, 500]\n280\n\n\n\nmax_depth\n[3, 9]\n7\n\n\n\nlearning_rate\n[0.01, 0.3]\n0.0756\n\n\n\nnum_leaves\n[20, 100]\n64\n\n\n\nmin_child_samples\n[10, 50]\n25\n\n\n\n\n\n\n\n\nA.4.3 Threshold Sweep for Tree Models\n\n\n\nTable A.7: Optimal thresholds and metrics for tree-based models (validation set)\n\n\n\n\n\nModel\nOptimal τ\nPrecision\nRecall\nF1-Score\nROC-AUC\n\n\n\n\nRandom Forest\n0.3200\n0.4780\n0.6120\n0.5370\n0.6523\n\n\nXGBoost\n0.3100\n0.4920\n0.6280\n0.5520\n0.6687\n\n\nLightGBM\n0.3000\n0.4850\n0.6340\n0.5490\n0.6654\n\n\nCatBoost\n0.3100\n0.4890\n0.6210\n0.5470\n0.6642\n\n\n\n\n\n\n\n\nA.4.4 Optuna Optimization Diagnostics\nBayesian hyperparameter optimization was conducted using Optuna with 100 trials for XGBoost.\n\n\n\n\n\n\nFigure A.6: Optimization history showing trial objective values over 100 iterations.\n\n\n\n\n\n\n\n\n\nFigure A.7: Parallel coordinate visualization of hyperparameter configurations across trials.\n\n\n\n\n\n\n\n\n\nFigure A.8: Relative importance of each hyperparameter in determining model performance.\n\n\n\n\n\n\nTable A.8: Optuna hyperparameter importance for XGBoost optimization\n\n\n\n\n\nHyperparameter\nImportance Score\nDescription\n\n\n\n\nlearning_rate\n0.4200\nStep size shrinkage\n\n\nmax_depth\n0.1800\nMaximum tree depth\n\n\nn_estimators\n0.1400\nNumber of boosting rounds\n\n\nmin_child_weight\n0.1100\nMinimum sum of instance weight\n\n\nsubsample\n0.0800\nRow sampling ratio\n\n\ncolsample_bytree\n0.0700\nColumn sampling ratio\n\n\n\n\n\n\nBest hyperparameters found:\n{\n    'learning_rate': 0.0847,\n    'max_depth': 6,\n    'n_estimators': 312,\n    'min_child_weight': 3,\n    'subsample': 0.8234,\n    'colsample_bytree': 0.7891,\n    'reg_alpha': 0.0123,\n    'reg_lambda': 1.2456\n}\n\n\nA.4.5 Feature Importance Comparison\n\n\n\nTable A.9: Top-7 feature importance ranking across tree-based models\n\n\n\n\n\nRank\nXGBoost\nLightGBM\nRandom Forest\n\n\n\n\n1\nCurrentEquipmentDays\nMonthsInService\nCurrentEquipmentDays\n\n\n2\nMonthsInService\nCurrentEquipmentDays\nMonthsInService\n\n\n3\nMonthlyMinutes\nMonthlyMinutes\nAgeHH1\n\n\n4\nPercChangeMinutes\nPercChangeMinutes\nMonthlyMinutes\n\n\n5\nDroppedBlockedCalls\nOverageMinutes\nIncomeGroup\n\n\n6\nOverageMinutes\nAgeHH1\nPercChangeMinutes\n\n\n7\nAgeHH1\nDroppedBlockedCalls\nOverageMinutes",
    "crumbs": [
      "Appendices",
      "<span class='chapter-number'>A</span>  <span class='chapter-title'>Appendix</span>"
    ]
  },
  {
    "objectID": "report/99-appendix.html#sec-appendix-d",
    "href": "report/99-appendix.html#sec-appendix-d",
    "title": "Appendix A — Appendix",
    "section": "A.5 Deep Learning Training Dynamics",
    "text": "A.5 Deep Learning Training Dynamics\n\nA.5.1 Neural Network Experiment Leaderboard\nAll neural network experiments tracked via MLflow with systematic architecture variations.\n\n\n\nTable A.10: Neural Network Experiment Leaderboard (sorted by Test ROC-AUC)\n\n\n\n\n\n\n\n\n\n\n\n\n\nExperiment\nArchitecture\nLoss\nTest ROC-AUC\nTest PR-AUC\nBrier Score\n\n\n\n\nexp_001\nMLP-3L-256\nBCE\n0.6423\n0.4012\n0.2120\n\n\nexp_002\nMLP-3L-256\nFocal\n0.6512\n0.4189\n0.2050\n\n\nexp_003\nMLP-4L-512\nBCE\n0.6398\n0.3987\n0.2180\n\n\nexp_004\nWide&Deep\nBCE\n0.6534\n0.4234\n0.2010\n\n\nexp_005\nWide&Deep\nFocal\n0.6615\n0.4356\n0.1970\n\n\nexp_006\nResNet-style\nBCE\n0.6478\n0.4156\n0.2080\n\n\nexp_007\nResNet-style\nFocal\n0.6589\n0.4298\n0.1990\n\n\n\n\n\n\n\n\nA.5.2 Alternative Architecture Training Curves\n\n\n\n\n\n\nFigure A.9: Training dynamics for the MLP dense baseline architecture.\n\n\n\n\n\n\n\n\n\nFigure A.10: Training dynamics for the embedding-based MLP architecture.\n\n\n\n\n\n\n\n\n\nFigure A.11: Training dynamics for the Wide & Deep architecture with BCE loss.\n\n\n\n\n\n\n\n\n\nFigure A.12: Training dynamics for the deeper Wide & Deep architecture variant.\n\n\n\n\n\n\n\n\n\nFigure A.13: Training dynamics for embedding MLP with focal loss.\n\n\n\n\n\n\n\n\n\nFigure A.14: Training dynamics for embedding MLP with strong class weight.\n\n\n\n\n\nA.5.3 Architecture Specifications\n\n\n\nTable A.11: Neural network architecture specifications\n\n\n\n\n\n\n\n\n\n\n\n\n\nArchitecture\nLayers\nHidden Units\nDropout\nActivation\nParameters\n\n\n\n\nMLP-3L-256\n3\n[256, 128, 64]\n0.3000\nReLU\n~98K\n\n\nMLP-4L-512\n4\n[512, 256, 128, 64]\n0.4000\nReLU\n~312K\n\n\nWide&Deep\n3+1\n[256, 128, 64] + Linear\n0.3000\nReLU/Linear\n~105K\n\n\nResNet-style\n4\n[256, 256, 128, 64]\n0.3000\nReLU + Skip\n~142K",
    "crumbs": [
      "Appendices",
      "<span class='chapter-number'>A</span>  <span class='chapter-title'>Appendix</span>"
    ]
  },
  {
    "objectID": "report/99-appendix.html#sec-appendix-e",
    "href": "report/99-appendix.html#sec-appendix-e",
    "title": "Appendix A — Appendix",
    "section": "A.6 Unsupervised and Semi-Supervised Diagnostics",
    "text": "A.6 Unsupervised and Semi-Supervised Diagnostics\n\nA.6.1 Autoencoder Reconstruction Error Distribution\n\n\n\n\n\n\nFigure A.15: Distribution of autoencoder reconstruction errors for churners vs. non-churners, showing modest separation.\n\n\n\n\n\nA.6.2 Latent Space Visualization\n\n\n\n\n\n\nFigure A.16: t-SNE projection of 32-dimensional autoencoder latent representations colored by churn label.\n\n\n\n\n\nA.6.3 Autoencoder Ablation Study\n\n\n\n\n\n\nFigure A.17: Comparison of autoencoder variants and their impact on downstream model performance.\n\n\n\n\n\nA.6.4 Pseudo-Labeling Performance Summary\n\n\n\nTable A.12: Pseudo-labeling performance across confidence thresholds\n\n\n\n\n\n\n\n\n\n\n\nConfidence Threshold\nHoldout Coverage\nPseudo-label Accuracy\nFinal Test AUC\n\n\n\n\n0.9000\n18.3%\n0.8920\n0.6623\n\n\n0.8500\n28.7%\n0.8710\n0.6645\n\n\n0.8000\n42.1%\n0.8530\n0.6687\n\n\n0.7500\n56.4%\n0.8310\n0.6672\n\n\n0.7000\n68.9%\n0.8120\n0.6641\n\n\n\n\n\n\nSelected threshold: 0.8000 (balancing coverage and pseudo-label quality)\n\n\nA.6.5 Denoising Autoencoder Configuration\nDAE_CONFIG = {\n    'encoder_dims': [128, 64, 32],\n    'decoder_dims': [32, 64, 128],\n    'latent_dim': 32,\n    'noise_factor': 0.1500,\n    'dropout': 0.2000,\n    'activation': 'leaky_relu',\n    'learning_rate': 0.0010,\n    'batch_size': 256,\n    'epochs': 100,\n    'early_stopping_patience': 10\n}",
    "crumbs": [
      "Appendices",
      "<span class='chapter-number'>A</span>  <span class='chapter-title'>Appendix</span>"
    ]
  },
  {
    "objectID": "report/99-appendix.html#sec-appendix-f",
    "href": "report/99-appendix.html#sec-appendix-f",
    "title": "Appendix A — Appendix",
    "section": "A.7 Ensemble Diversity and Weights",
    "text": "A.7 Ensemble Diversity and Weights\n\nA.7.1 Base Model Prediction Correlation Matrix\n\n\n\nTable A.13: Pearson correlation of predicted probabilities on validation set\n\n\n\n\n\n\nLogistic\nXGBoost\nLightGBM\nCatBoost\nRF\nWide&Deep\n\n\n\n\nLogistic\n1.00\n0.4700\n0.4800\n0.4700\n0.4500\n0.5200\n\n\nXGBoost\n0.4700\n1.00\n0.9300\n0.9100\n0.8700\n0.7800\n\n\nLightGBM\n0.4800\n0.9300\n1.00\n0.9200\n0.8800\n0.7900\n\n\nCatBoost\n0.4700\n0.9100\n0.9200\n1.00\n0.8900\n0.7700\n\n\nRF\n0.4500\n0.8700\n0.8800\n0.8900\n1.00\n0.7400\n\n\nWide&Deep\n0.5200\n0.7800\n0.7900\n0.7700\n0.7400\n1.00\n\n\n\n\n\n\nInsight: Logistic regression and neural network predictions show lower correlation with tree ensemble predictions, suggesting potential diversity benefits for stacking.\n\n\nA.7.2 Ensemble Blending Weights\n\n\n\nTable A.14: Blending weights under different weighting schemes\n\n\n\n\n\nModel\nAUC-Weighted\nNNLS-Optimized\nEqual Weight\n\n\n\n\nLogistic\n0.0890\n0.0520\n0.1670\n\n\nXGBoost\n0.2010\n0.2870\n0.1670\n\n\nLightGBM\n0.1890\n0.2340\n0.1670\n\n\nCatBoost\n0.1780\n0.1980\n0.1670\n\n\nRF\n0.1560\n0.1120\n0.1670\n\n\nWide&Deep\n0.1870\n0.1170\n0.1670\n\n\n\n\n\n\n\n\nA.7.3 Stacking Meta-Learner Coefficients\nOut-of-fold (OOF) predictions were used to train a logistic regression meta-learner:\nMeta-learner coefficients (Ridge, alpha=1.0):\n{\n    'Logistic_OOF': 0.2340,\n    'XGBoost_OOF': 0.4120,\n    'LightGBM_OOF': 0.1560,\n    'CatBoost_OOF': 0.0890,\n    'RF_OOF': 0.0230,\n    'WideDeep_OOF': 0.0860,\n    'intercept': -0.8920\n}\n\n\nA.7.4 Ensemble Performance Comparison\n\n\n\nTable A.15: Ensemble method comparison with computational costs\n\n\n\n\n\n\n\n\n\n\n\n\n\nEnsemble Method\nVal ROC-AUC\nTest ROC-AUC\nTest PR-AUC\nTraining Time (s)\nInference Time (ms)\n\n\n\n\nBest Single (XGBoost)\n0.6654\n0.6687\n0.4298\n42.3\n2.1\n\n\nSimple Average\n0.6698\n0.6712\n0.4345\n245.8\n12.4\n\n\nAUC-Weighted\n0.6712\n0.6723\n0.4367\n245.8\n12.5\n\n\nNNLS-Optimized\n0.6724\n0.6734\n0.4378\n247.2\n12.5\n\n\nOOF Stacking\n0.6745\n0.6726\n0.4389\n278.4\n14.2\n\n\n\n\n\n\nObservation: OOF stacking achieves the best validation AUC but slightly lower test AUC than NNLS blending, suggesting mild overfitting to validation folds.",
    "crumbs": [
      "Appendices",
      "<span class='chapter-number'>A</span>  <span class='chapter-title'>Appendix</span>"
    ]
  },
  {
    "objectID": "report/99-appendix.html#sec-appendix-g",
    "href": "report/99-appendix.html#sec-appendix-g",
    "title": "Appendix A — Appendix",
    "section": "A.8 Supplementary SHAP Analysis",
    "text": "A.8 Supplementary SHAP Analysis\n\nA.8.1 SHAP Feature Importance Bar Plot\n\n\n\n\n\n\nFigure A.18: Mean absolute SHAP values showing global feature importance ranking.\n\n\n\n\n\nA.8.2 SHAP Interaction Effects\n\n\n\nTable A.16: Top SHAP interaction effects between feature pairs\n\n\n\n\n\nFeature Pair\nMean Interaction Effect\nDirection\n\n\n\n\nCurrentEquipmentDays × MonthsInService\n0.0234\nSynergistic\n\n\nMonthlyMinutes × PercChangeMinutes\n0.0189\nSynergistic\n\n\nOverageMinutes × MonthlyRevenue\n0.0156\nSynergistic\n\n\nAgeHH1 × Homeownership\n0.0098\nAntagonistic\n\n\nDroppedBlockedCalls × CustomerCareCalls_flag\n0.0087\nSynergistic\n\n\n\n\n\n\n\n\nA.8.3 Semi-Supervised Learning SHAP Analysis\n\n\n\n\n\n\nFigure A.19: Summary of semi-supervised learning experiments with pseudo-labeling.",
    "crumbs": [
      "Appendices",
      "<span class='chapter-number'>A</span>  <span class='chapter-title'>Appendix</span>"
    ]
  },
  {
    "objectID": "report/99-appendix.html#sec-appendix-h",
    "href": "report/99-appendix.html#sec-appendix-h",
    "title": "Appendix A — Appendix",
    "section": "A.9 Final Model Comparison and Evaluation",
    "text": "A.9 Final Model Comparison and Evaluation\n\nA.9.1 Model Comparison Curves\n\n\n\n\n\n\nFigure A.20: ROC curves comparing all model families on the test set.\n\n\n\n\n\n\n\n\n\nFigure A.21: Precision-Recall curves comparing all model families on the test set.\n\n\n\n\n\nA.9.2 Threshold Analysis\n\n\n\nTable A.17: Complete threshold sweep for XGBoost with confusion matrix components\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nτ\nTP\nFP\nTN\nFN\nPrecision\nRecall\nF1\nSpecificity\n\n\n\n\n0.1000\n2156\n5234\n2012\n267\n0.2920\n0.8900\n0.4390\n0.2780\n\n\n0.1500\n2089\n4567\n2679\n334\n0.3140\n0.8620\n0.4600\n0.3700\n\n\n0.2000\n1998\n3912\n3334\n425\n0.3380\n0.8250\n0.4790\n0.4600\n\n\n0.2500\n1876\n3234\n4012\n547\n0.3670\n0.7740\n0.4980\n0.5540\n\n\n0.3000\n1723\n2589\n4657\n700\n0.4000\n0.7110\n0.5120\n0.6430\n\n\n0.3100\n1689\n2456\n4790\n734\n0.4080\n0.6970\n0.5140\n0.6610\n\n\n0.3500\n1534\n2012\n5234\n889\n0.4330\n0.6330\n0.5140\n0.7220\n\n\n0.4000\n1356\n1567\n5679\n1067\n0.4640\n0.5600\n0.5080\n0.7840\n\n\n0.4500\n1178\n1189\n6057\n1245\n0.4980\n0.4860\n0.4920\n0.8360\n\n\n0.5000\n989\n867\n6379\n1434\n0.5330\n0.4080\n0.4620\n0.8800\n\n\n\n\n\n\n\n\nA.9.3 Model-Specific Optimal Thresholds\n\n\n\nTable A.18: Optimal thresholds per model under different criteria\n\n\n\n\n\nModel\nOptimal τ (F1)\nOptimal τ (Youden)\nDefault τ=0.5000 F1\n\n\n\n\nLogistic Regression\n0.2900\n0.3100\n0.3990\n\n\nXGBoost\n0.3100\n0.3300\n0.4620\n\n\nLightGBM\n0.3000\n0.3200\n0.4580\n\n\nCatBoost\n0.3100\n0.3300\n0.4560\n\n\nRandom Forest\n0.3200\n0.3400\n0.4450\n\n\nWide&Deep (Focal)\n0.2800\n0.3000\n0.4230\n\n\nOOF Stacking\n0.3000\n0.3200\n0.4670\n\n\n\n\n\n\n\n\nA.9.4 Calibration Metrics\n\n\n\nTable A.19: Calibration metrics across all models\n\n\n\n\n\nModel\nBrier Score\nECE\nMCE\nCalibration Slope\n\n\n\n\nLogistic Regression\n0.2150\n0.0420\n0.0890\n0.9800\n\n\nXGBoost\n0.1980\n0.0380\n0.0780\n1.02\n\n\nLightGBM\n0.2010\n0.0410\n0.0820\n1.01\n\n\nCatBoost\n0.2030\n0.0390\n0.0810\n1.00\n\n\nRandom Forest\n0.2090\n0.0450\n0.0920\n0.9500\n\n\nWide&Deep (BCE)\n0.2010\n0.0430\n0.0870\n0.9700\n\n\nWide&Deep (Focal)\n0.1970\n0.0570\n0.1120\n0.8900\n\n\nOOF Stacking\n0.1950\n0.0360\n0.0740\n1.01\n\n\n\n\n\n\nNote: Focal loss models achieve lower Brier scores but higher ECE/MCE due to probability compression effect. Isotonic calibration recommended for deployment.\n\n\nA.9.5 Complete Model Leaderboard\nThe following table provides the complete model leaderboard with all evaluation metrics, serving as the definitive reference for model comparison.\n\n\n\n\nTable A.20: Complete model leaderboard with validation and test metrics across all model families.\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nmodel\nval_roc_auc\nval_pr_auc\ntest_roc_auc\ntest_pr_auc\ntest_brier\ntest_f1\ntest_recall\ntest_precision\nthreshold\n\n\n\n\nXGBoost\n0.657146\n0.435915\n0.663681\n0.445441\n0.223314\n0.490413\n0.738953\n0.366982\n0.44\n\n\n05_nn_wide_deep_focal\n0.653063\n0.429338\n0.661457\n0.435612\n0.197007\n0.0174614\n0.00883753\n0.722222\n0.44\n\n\nLightGBM\n0.650239\n0.4311\n0.669973\n0.447632\n0.219983\n0.49688\n0.730795\n0.376401\n0.44\n\n\n05_nn_wide_and_deep\n0.649779\n0.418712\n0.649605\n0.42076\n0.2364\n0.482441\n0.7845\n0.348325\n0.44\n\n\nRandomForest\n0.649016\n0.421517\n0.653428\n0.428959\n0.228971\n0.480699\n0.804215\n0.342799\n0.44\n\n\n05_nn_embedding_focal_loss\n0.648666\n0.421622\n0.661197\n0.434519\n0.197579\n0\n0\n0\n0.44\n\n\n05_nn_dense_baseline\n0.646737\n0.425746\n0.65528\n0.421941\n0.234346\n0.489779\n0.806254\n0.35172\n0.44\n\n\n05_nn_embedding_baseline\n0.642545\n0.408729\n0.648944\n0.419172\n0.232875\n0.487391\n0.781781\n0.354064\n0.44\n\n\n05_nn_wide_and_deep_deeper\n0.642242\n0.414346\n0.647519\n0.411018\n0.232793\n0.482227\n0.765466\n0.351985\n0.44\n\n\n05_nn_embedding_strong_weight\n0.64173\n0.411761\n0.644192\n0.408117\n0.271837\n0.478787\n0.901428\n0.325959\n0.44\n\n\n06_teacher\n0.63778\n0.418322\n0.645056\n0.427104\n0.193299\n0.262616\n0.173351\n0.541401\n0.44\n\n\nLogistic\n0.588286\n0.347494\n0.593987\n0.355974\n0.24378\n0.456037\n0.840925\n0.312848\n0.44",
    "crumbs": [
      "Appendices",
      "<span class='chapter-number'>A</span>  <span class='chapter-title'>Appendix</span>"
    ]
  },
  {
    "objectID": "report/99-appendix.html#sec-appendix-i",
    "href": "report/99-appendix.html#sec-appendix-i",
    "title": "Appendix A — Appendix",
    "section": "A.10 Reproducibility Information",
    "text": "A.10 Reproducibility Information\n\nA.10.1 Software Environment\n# Core ML Stack\npython==3.10.12\nnumpy==1.24.3\npandas==2.0.3\nscikit-learn==1.3.0\nscipy==1.11.1\n\n# Deep Learning\ntorch==2.0.1\ntorchvision==0.15.2\n\n# Gradient Boosting\nxgboost==1.7.6\nlightgbm==4.0.0\ncatboost==1.2\n\n# Experiment Tracking\nmlflow==2.5.0\noptuna==3.3.0\n\n# Interpretability\nshap==0.42.1\nlime==0.2.0.1\n\n# Visualization\nmatplotlib==3.7.2\nseaborn==0.12.2\nplotly==5.15.0\n\n\nA.10.2 Random Seeds\nSEED_CONFIG = {\n    'global_seed': 42,\n    'train_test_split': 42,\n    'cv_shuffle': 42,\n    'numpy_seed': 42,\n    'torch_seed': 42,\n    'optuna_sampler': 42\n}\n\n# Seed initialization function\ndef set_all_seeds(seed: int = 42):\n    import random\n    import numpy as np\n    import torch\n    \n    random.seed(seed)\n    np.random.seed(seed)\n    torch.manual_seed(seed)\n    torch.cuda.manual_seed_all(seed)\n    torch.backends.cudnn.deterministic = True\n    torch.backends.cudnn.benchmark = False\n\n\nA.10.3 Data Split Statistics\n\n\n\nTable A.21: Data split statistics with class distribution\n\n\n\n\n\nSplit\nN Samples\nChurn Rate\n% of Total\n\n\n\n\nTraining\n35,732\n28.6%\n70%\n\n\nValidation\n7,657\n28.4%\n15%\n\n\nTest\n7,658\n28.7%\n15%\n\n\nTotal\n51,047\n28.6%\n100%\n\n\nHoldout (unlabeled)\n20,000\n–\n–\n\n\n\n\n\n\nStratification: All splits stratified by Churn label to maintain class balance.\n\n\nA.10.4 Artifact Manifest\n\n\n\nTable A.22: Complete artifact manifest for reproducibility\n\n\n\n\n\n\n\n\n\n\nArtifact\nLocation\nDescription\n\n\n\n\nT0_feature_registry.csv\n../artifacts/tables/\nFeature decision registry\n\n\npreprocessor_fitted.pkl\nmodels/\nFitted sklearn preprocessor\n\n\nxgb_best_model.json\nmodels/\nBest XGBoost model\n\n\nlgb_best_model.txt\nmodels/\nBest LightGBM model\n\n\nnn_wide_deep_focal.pt\nmodels/\nBest PyTorch model weights\n\n\nstacking_meta_learner.pkl\nmodels/\nOOF stacking meta-learner\n\n\nshap_explainer.pkl\nmodels/\nSHAP TreeExplainer object\n\n\nmlflow_experiment_*.db\nmlruns/\nMLflow experiment database\n\n\noptuna_study_*.db\noptuna/\nOptuna study database",
    "crumbs": [
      "Appendices",
      "<span class='chapter-number'>A</span>  <span class='chapter-title'>Appendix</span>"
    ]
  },
  {
    "objectID": "report/99-appendix.html#sec-appendix-j",
    "href": "report/99-appendix.html#sec-appendix-j",
    "title": "Appendix A — Appendix",
    "section": "A.11 Error Case Studies",
    "text": "A.11 Error Case Studies\n\nA.11.1 Representative False Negative Cases\n“Silent churners” - customers who churned but were predicted as non-churners (high confidence):\n\n\n\nTable A.23: Representative false negative cases with feature profiles\n\n\n\n\n\n\n\n\n\n\n\n\n\nCase ID\nPredicted P(Churn)\nMonthsInService\nCustomerCareCalls\nPercChangeMinutes\nPattern\n\n\n\n\nFN_001\n0.1200\n36\n0\n-2.3%\nLong tenure, no complaints, subtle usage decline\n\n\nFN_002\n0.1800\n24\n1\n+5.1%\nStable tenure, minimal support contact, increasing usage\n\n\nFN_003\n0.1500\n42\n0\n-8.7%\nVery long tenure, no red flags except usage drop\n\n\nFN_004\n0.2100\n18\n0\n-1.2%\nModerate tenure, quiet customer profile\n\n\nFN_005\n0.1400\n30\n0\n+2.8%\nEstablished customer with positive momentum\n\n\n\n\n\n\nCommon patterns: Long tenure customers with no support interactions who churn without warning signals. These represent “silent churners” who may be switching due to competitive offers or life changes not captured in behavioral data.\n\n\nA.11.2 Representative False Positive Cases\n“False alarms” - customers predicted to churn but remained loyal:\n\n\n\nTable A.24: Representative false positive cases with feature profiles\n\n\n\n\n\n\n\n\n\n\n\n\n\nCase ID\nPredicted P(Churn)\nMonthsInService\nCustomerCareCalls\nPercChangeMinutes\nPattern\n\n\n\n\nFP_001\n0.7800\n3\n2\n-15.3%\nNew customer with complaints, but resolved\n\n\nFP_002\n0.7200\n6\n3\n-22.1%\nEarly tenure, high support contact, usage adjustment period\n\n\nFP_003\n0.8100\n4\n1\n-31.2%\nNew customer, sharp usage decline (seasonal?)\n\n\nFP_004\n0.6900\n8\n2\n-18.7%\nYoung account with volatility\n\n\nFP_005\n0.7400\n5\n2\n-25.4%\nNew customer showing distress signals that stabilized\n\n\n\n\n\n\nCommon patterns: New customers (&lt; 12 months) showing high-risk signals (usage decline, support calls) but ultimately retained. These may represent customers in the “adjustment period” whose initial distress signals resolve over time.",
    "crumbs": [
      "Appendices",
      "<span class='chapter-number'>A</span>  <span class='chapter-title'>Appendix</span>"
    ]
  }
]