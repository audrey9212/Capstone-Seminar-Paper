---
title: ""
numbered: false
author: ""
date: ""
lang: en
---

# Abstract {.unnumbered .unlisted}

**Abstract**
Customer churn prediction is a critical objective for telecom operators because retention efforts are typically more cost-effective than acquiring new customers. This thesis develops a leakage-safe and reproducible machine learning pipeline for churn prediction and benchmarks interpretable and high-performing models under realistic deployment constraints. The workflow covers feature cleaning and engineering, stratified holdout evaluation, and comparison across logistic regression, gradient-boosted tree models, deep neural networks (including focal-loss variants), and ensemble methods (weighted blending and out-of-fold stacking) to address class imbalance and improve ranking performance. Models are assessed using threshold-free metrics (ROC-AUC and PR-AUC) alongside a fixed operating point that reflects limited retention outreach capacity. The logistic regression baseline achieved a test ROC-AUC of **0.594** and PR-AUC of **0.356**. Using validation ROC-AUC as the selection criterion, **XGBoost** was chosen as the champion model (val ROC-AUC ≈ **0.657**, val PR-AUC ≈ **0.436**), achieving test ROC-AUC of **0.664** and PR-AUC of **0.445**. At the chosen operating threshold (**τ = 0.44**), the XGBoost model reached **0.367 precision**, **0.739 recall**, and **0.490 F1**, prioritizing churn capture while maintaining actionable precision. While **LightGBM** achieved the highest test ROC-AUC (≈ **0.670**), ensemble methods provided a modest additional lift, reaching approximately **0.669 ROC-AUC** and **0.449 PR-AUC** on the test set. Finally, SHAP-based interpretability is used to validate model behavior and translate predictions into retention insights by highlighting actionable patterns in customer tenure, usage dynamics, and service quality signals. Overall, the results show that explainable gradient-boosting models can deliver robust predictive performance while remaining operationally useful for targeted retention strategies.

\noindent\textbf{Keywords:} Telecom churn; supervised learning; gradient boosting; stacking ensemble; imbalanced classification; explainable machine learning; SHAP

\tableofcontents
\newpage
